{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自然策略梯度 (A Natural Policy Gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 标签 (Labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvard引用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kakade, S.M., 2002. A natural policy gradient. In Advances in neural information processing systems (pp. 1531-1538)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BibTex引用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@inproceedings{kakade2002natural,\n",
    "  title={A natural policy gradient},\n",
    "  author={Kakade, Sham M},\n",
    "  booktitle={Advances in neural information processing systems},\n",
    "  pages={1531--1538},\n",
    "  year={2002}\n",
    "}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 摘要 (Abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, *compatible* value functions, as defined by Sutton et al. (2000). We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris.\n",
    "\n",
    "我们提供了一种基于参数空间结构来表示最陡下降方向的自然梯度方法。虽然梯度方法不能在参数值上做出很大改变，我们展示了自然梯度会偏向选择一个贪心最优动作而不是一个更好的动作。这些贪心最优动作是在单步策略迭代时基于近似且兼容的价值函数选择的。我们接下来展示了在简单MDP中和更有挑战性的俄罗斯方块MDP中显著的性能提升。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初次编辑日期 (Initial Edit Date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年5月17日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献 (References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Kakade, S.M., 2002. A natural policy gradient. In Advances in neural information processing systems (pp. 1531-1538)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 摘要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将会描述一个能保证性能单调提升的策略优化迭代流程。通过对理论成立的流程进行近似处理，我们开发了一种实用的算法，这种算法叫作信赖域策略优化（Trust Region Policy Optimization, or TRPO）。这种算法与自然策略梯度方法类似并且对优化像神经网络这样的大型非线性策略非常有效。我们的实验展示了TRPO在一系列任务上可靠的性能：让模拟的机器人学习游泳、跳跃和行走；用屏幕图像作为输入玩Atari游戏。尽管TRPO所做的近似与理论并不完全相符，但是TRPO往往能在仅仅微调超参数的情况下在性能上取得单调改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 引言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大多数策略优化算法都可以被分到这三类中：\n",
    "\n",
    "1. 策略迭代方法。交替进行当前策略的价值函数的估值和策略改进。\n",
    "2. 策略梯度方法。用从样本轨迹中得到的期望回报（总奖励）的梯度估计值。\n",
    "3. 无导数优化方法。比如交叉熵方法（cross-entropy method, or CEM）和协方差矩阵自适应方法（covariance matrix adaptation, CMA。将回报当作一个待优化的黑盒函数来优化策略参数。\n",
    "\n",
    "现在在许多问题上都更倾向于使用像CEM和CMA这种无导数的随机优化方法，这是因为它们不仅可以取得很好的结果，而且易于理解和实现。比如说，俄罗斯方块作为近似动态规划（approximate dynamic programming, or ADP）的一个经典问题，随机优化方法就难以胜任这个任务。对于连续控制问题，像CMA这样的方法已经可以在人工设计的低维策略的基础上成功学习用于像运动这类有挑战性的任务的控制策略了。因为基于梯度的优化算法比无梯度方法拥有更好的样本复杂性保证，所以ADP和基于梯度的方法无法一直优于无梯度随机搜索的结果让人十分不满。对于参数量很大的监督学习任务来说，连续的基于梯度的优化已经在学习函数近似器上非常成功了。如果我们可以将在监督学习任务上取得的成功扩展到强化学习中，那么在复杂策略上的有效训练也会成为可能。\n",
    "\n",
    "在这篇文章中，我们会首先证明通过最小化一个特定的代理目标函数我们可以保证在策略中取得一定的改进。接下来我们会对理论上成立的算法进行一系列的近似来得到一个实用的算法，这个实用的算法就是信赖域策略优化（trust region policy optimization, or TRPO）。我们会描述这种算法的两个变体：第一个是单路径方法（the single-path method），这种方法可以被用在无模型的情况下；第二种是藤方法（the vine method），这种方法需要将系统恢复到某些特定的状态，所以只在模拟中可以用。这些算法都是可扩展的，并且可以优化拥有上万参数的非线性策略，这也是过去无模型策略搜索方法所不能达到的。在我们的实验中，我们证明了相同的TRPO方法可以被用来学习游泳、跳跃和行走的复杂策略，还有用原始图片输入学习玩Atari游戏。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预备知识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an infinite-horizon discounted Markov decision process (MDP), defined by the tuple $(\\mathcal{S}, \\mathcal{A}, P, r, \\rho_0, \\gamma)$, where $\\mathcal{S}$ is a finite set of states, $\\mathcal{A}$ is a finite set of actions, $P:\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S}\\rightarrow\\mathbb{R}$ is the transition probability distribution, $r: \\mathcal{S}\\rightarrow\\mathbb{R}$ is the reward function, $\\rho_0 :\\mathcal{S}\\rightarrow\\mathbb{R}$ is the distribution of the initial state $s_0$, and $\\gamma \\in (0, 1)$ is the discount factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\pi$ denote a stochastic policy $\\pi : \\mathcal{S}\\times\\mathcal{A}\\rightarrow [0, 1]$, and let $\\eta (\\pi)$ denote its expected discounted reward:\n",
    "\n",
    "$\\quad \\eta (\\pi)=\\mathbb{E}_{s_0,a_0,\\ldots}\\left[ \\sum_{t=0}^{\\infty}\\gamma^t r(s_t) \\right]$, where\n",
    "\n",
    "$\\quad s_0 \\sim \\rho_0 (s_0)$, $a_t \\sim \\pi (a_t \\mid s_t)$, $s_{t+1} \\sim P(s_{t+1}\\mid s_t , a_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following standard definitions of the state-action value function $Q_{\\pi}$, the value function $V_{\\pi}$, and the advatage function $A_{\\pi}$:\n",
    "\n",
    "$\\quad Q_{\\pi}(s_t, a_t)=\\mathbb{E}_{s_{t+1},a_{t+1},\\ldots}\\left[ \\sum_{l=0}^{\\infty} \\gamma^l r(s_{t+l}) \\right]$,\n",
    "\n",
    "$\\quad V_{\\pi}(s_t)=\\mathbb{E}_{a_t,s_{t+1},\\ldots}\\left[ \\sum_{l=0}^{\\infty} \\gamma^l r(s_{t+l}) \\right]$,\n",
    "\n",
    "$\\quad A_{\\pi}(s,a)=Q_{\\pi}(s,a)-V_{\\pi}(s)$, where\n",
    "\n",
    "$\\quad a_t \\sim \\pi(a_t\\mid s_t)$, $s_{t+1}\\sim P(s_{t+1}\\mid s_t, a_t)$ for $t \\geq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following useful identity expresses the expected return of another policy $\\tilde{\\pi}$ in terms of the advantage over $\\pi$, accumulated over timesteps:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:eq}\n",
    "\\eta (\\tilde{\\pi}) = \\eta (\\pi) + \\mathbb{E}_{s_0,a_0,\\ldots \\sim \\tilde{\\pi}} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t A_{\\pi}(s_t, a_t) \\right]\n",
    "\\end{equation}\n",
    "\n",
    "where the notation $\\mathbb{E}_{s_0,a_0,\\ldots \\sim \\tilde{\\pi}}\\left[ \\ldots \\right]$ indicates that actions are sampled $a_t \\sim \\tilde{\\pi}(\\cdot \\mid s_t)$. Let $\\rho_{\\pi}$ be the (unnormalized) discounted visitation frequencies\n",
    "\n",
    "$\\rho_{\\pi}(s)=P(s_0 =s)+\\gamma P(s_1 =s)+\\gamma^2 P(s_2 =s)+\\ldots$,\n",
    "\n",
    "where $s_0 \\sim \\rho_0$ and the actions are chosen according to $\\pi$. We can rewrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初次编辑时间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年5月9日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Schulman, J., Levine, S., Abbeel, P., Jordan, M. and Moritz, P., 2015, June. Trust region policy optimization. In International Conference on Machine Learning (pp. 1889-1897)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

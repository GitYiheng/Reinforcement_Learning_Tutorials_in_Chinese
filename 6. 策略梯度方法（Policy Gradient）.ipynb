{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于策略的强化学习（Policy-Based Reinforcement Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 之前我们用参数$\\theta$来近似价值或者动作价值函数\n",
    "\\begin{split}\n",
    "V_{\\theta}(s) & \\approx V^{\\pi}(s) \\\\\n",
    "Q_{\\theta}(s,a) & \\approx Q^{\\pi}(s,a)\n",
    "\\end{split}\n",
    "\n",
    "- 我们直接基于价值函数生成了一个策略\n",
    "    - 也就是用$\\epsilon$-贪心\n",
    "\n",
    "- 接下来我们将会直接对策略进行参数化（directly parameterise the policy）\n",
    "\\begin{equation}\n",
    "\\pi_{\\theta}(s,a) = \\mathbb{P}[a \\mid s, \\theta]\n",
    "\\end{equation}\n",
    "\n",
    "- 无模型强化学习（model-free reinforcement learning）同样是我们的重点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于价值的和基于策略的强化学习（Value-Based and Policy-Based RL）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/value-based_and_policy-based_RL.png\" style=\"width: 300px;\" />\n",
    "\n",
    "- 基于价值的（value-based）\n",
    "    - 学习到的价值函数（learnt value function）\n",
    "    - 隐式策略（implicit policy），比如说$\\epsilon$-贪心\n",
    "- 基于策略的（policy-based）\n",
    "    - 没有价值函数\n",
    "    - 学习到的策略（learnt policy）\n",
    "- 行动者-批评者方法（Actor-Critic）\n",
    "    - 学习到的价值函数（learnt value function）\n",
    "    - 学习到的策略（learnt policy）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于策略强化学习方法的优势（Advantages of Policy-Based RL）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优势：\n",
    "\n",
    "- 更好的收敛特性\n",
    "- 在高维（high-dimensional）或连续（continuous）动作空间中更高效\n",
    "- 可以学习随机策略（stochastic policy）\n",
    "\n",
    "劣势：\n",
    "\n",
    "- 一般收敛到局部最优值而不是全局最优值（local rather than global optimum）\n",
    "- 一般来说评估一个策略效率不高并且偏差很高（inefficient and high variance）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 范例-石头剪刀布（Example: Rock-Paper-Scissors）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/example_rock-paper-scissors.png\" style=\"width: 300px;\" />\n",
    "\n",
    "- 两名玩家参与的石头剪刀\n",
    "    - 剪刀赢布\n",
    "    - 石头赢剪刀\n",
    "    - 布赢石头\n",
    "- 让我们来考虑迭代的（iterated）石头剪刀布的策略\n",
    "    - 一个确定性的策略非常容易被利用\n",
    "    - 一个均匀随机的策略是最优的（也就是纳什均衡，Nash equilibrium）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 混淆的Gridworld（Aliased Gridworld）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/aliased_gridworld_1.png\" style=\"width: 300px;\" />\n",
    "\n",
    "- 这里的智能体不能区分灰色的状态\n",
    "- 让我们考虑下面这个形式的特征（对所有N, E, S, W，也就是北、东、南、西）\n",
    "\\begin{equation}\n",
    "\\phi(s, a) = \\textbf{1}(wall\\enspace to\\enspace N,\\enspace a=move \\enspace E)\n",
    "\\end{equation}\n",
    "\n",
    "- 我们用一个近似的价值函数（approximate value function）来比较基于价值的强化学习（value-based RL）\n",
    "\\begin{equation}\n",
    "Q_{\\theta}(s, a) = f(\\phi (s, a), \\theta)\n",
    "\\end{equation}\n",
    "\n",
    "- 对于基于策略的强化学习（policy-based RL），我们使用一个参数化的策略（a parameterised policy）\n",
    "\\begin{equation}\n",
    "\\pi_{\\theta}(s,a) = g(\\phi (s, a), \\theta)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/aliased_gridworld_2.png\" style=\"width: 300px;\" />\n",
    "\n",
    "- 在混淆的情况下，一个最优的确定性策略（an optimal deterministic policy）会在下面两种策略中选择其一\n",
    "    - 在两个灰色的状态都会向W移动（用红色箭头标出来的）\n",
    "    - 在两个灰色的状态都会向E移动\n",
    "- 无论选择了哪种确定性策略，智能体都会卡住而永远到达不了钱袋的状态\n",
    "- 基于价值的强化学习方法会学习到接近于确定性的策略\n",
    "    - 也就是贪心或者是$\\epsilon$-贪心\n",
    "- 所以智能体会在走廊中徘徊很久"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/aliased_gridworld_3.png\" style=\"width: 300px;\" />\n",
    "\n",
    "- 一个最优的随机策略（an optimal stochastic policy）会在灰色的状态上随机选择向E或是W移动\n",
    "\\begin{split}\n",
    "\\pi_{\\theta}(wall\\enspace to\\enspace N\\enspace and\\enspace S,\\enspace move\\enspace E) & = 0.5 \\\\\n",
    "\\pi_{\\theta}(wall\\enspace to\\enspace N\\enspace and\\enspace S,\\enspace move\\enspace W) & = 0.5\n",
    "\\end{split}\n",
    "\n",
    "- 智能体有很高的概率在几步后就到达目标状态（goal state）\n",
    "- 基于策略的强化学习（policy-based RL）可以学习到最优的随机策略（optimal stochastic policy）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 策略目标函数（Policy Objective Function）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 目标：给定参数是$\\theta$的策略$\\pi_{\\theta}(s, a)$，我们需要找到最优的$\\theta$\n",
    "- 但我们应该怎样衡量策略$\\pi_{\\theta}$的好坏呢？\n",
    "- 在回合制的环境中（episodic environments），我们可以使用起始价值（start value）\n",
    "\\begin{equation}\n",
    "J_1 (\\theta) = V^{\\pi_{\\theta}}(s_1) = \\mathbb{E}_{\\pi_{\\theta}}[v_1]\n",
    "\\end{equation}\n",
    "\n",
    "- 在连续的环境中（continuing environments），我们可以使用平均价值（average value）\n",
    "\n",
    "\\begin{equation}\n",
    "J_{avV}(\\theta) = \\sum_s d^{\\pi_{\\theta}}(s)V^{\\pi_{\\theta}}(s)\n",
    "\\end{equation}\n",
    "\n",
    "- 或是每步的平均奖励（average reward per time-step）\n",
    "\\begin{equation}\n",
    "J_{avR}(\\theta) = \\sum_s d^{\\pi_{\\theta}}(s) \\sum_a \\pi_{\\theta}(s,a)\\mathcal{R}_s^a\n",
    "\\end{equation}\n",
    "\n",
    "- 这里的$d^{\\pi_{\\theta}}(s)$是$\\pi_{\\theta}$的马尔科夫链的静态分布（stationary distribution）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 策略优化（Policy Optimisation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 基于策略的强化学习问题（policy-based RL）就是一个优化问题（an optimisation problem）\n",
    "- 搜寻最大化$J(\\theta)$的$\\theta$\n",
    "- 有一些方法并不需要用到梯度信息\n",
    "    - 爬坡法（hill climbing）\n",
    "    - Simplex/amoeba/Nelder-Mead法\n",
    "    - 基因算法（Genetic Algorithms）\n",
    "- 在使用梯度信息时效率可能会更高\n",
    "    - 梯度下降（gradient descent）\n",
    "    - 共轭梯度法（conjugate gradient）\n",
    "    - 拟牛顿法（Quasi-Newton）\n",
    "- 我们会主要使用梯度下降法（gradient descent），当然它也有很多延伸的方法\n",
    "- 也有利用顺序结构的方法（methods that exploit sequential structure）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 策略梯度法（Policy Gradient）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $J(\\theta)$可以是任何策略目标函数（policy objective function）\n",
    "- 策略梯度算法通过沿着策略的梯度上升方向在$J(\\theta)$中搜索一个局部最大值（a local maximum）\n",
    "\\begin{equation}\n",
    "\\Delta \\theta = \\alpha \\nabla_{\\theta} J(\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "- 这个公式中的$\\nabla_{\\theta} J(\\theta)$就是策略梯度（policy gradient）\n",
    "\\begin{equation}\n",
    "\\nabla_{\\theta}J(\\theta)\n",
    "=\\left(\n",
    "\\begin{split}\n",
    "& \\frac{\\partial J(\\theta)}{\\partial \\theta_1} \\\\\n",
    "& \\quad \\vdots \\\\\n",
    "& \\frac{\\partial J(\\theta)}{\\partial \\theta_n}\n",
    "\\end{split}\n",
    "\\right)\n",
    "\\end{equation}\n",
    "\n",
    "- 这里的$\\alpha$是步长参数（a step-size parameter）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 通过有限差分法来计算梯度（Computing Gradients by Finite Differences）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们下面来评估$\\pi_{\\theta}(s,a)$的策略梯度（policy gradient）\n",
    "- 对于每个维度 $k \\in [1, n]$\n",
    "    - 估计目标函数关于$\\theta$第$k^{th}$维的偏导数\n",
    "    - 通过在$\\theta$的第$k^{th}$维改变一个很小的量$\\epsilon$\n",
    "    \\begin{equation}\n",
    "    \\frac{\\partial J(\\theta)}{\\partial \\theta_k} \\approx \\frac{J(\\theta + \\epsilon u_k) - J(\\theta)}{\\epsilon}\n",
    "    \\end{equation}\n",
    "    - 这里的$u_k$是一个在第$k^{th}$维上长度为1，其它维度上长度为0的单位向量\n",
    "- 我们会做$n$个评估（$n$ evaluations）来计算$n$维的策略梯度\n",
    "- 简单、有噪音、效率不高，但有时有效（simple, noisy, inefficient - but sometimes effective）\n",
    "- 就算策略是不可微分的，有限差分法也可以用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用有限差分策略梯度法来训练AIBO步行（Training AIBO to Walk by Finite Difference Policy Gradient）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/aibo.png\" style=\"width: 500px;\" />\n",
    "\n",
    "- 目标：让AIBO学习快速步行（在Robocup中是有用的）\n",
    "- AIBO的步行策略是由12个数字来控制的（椭圆轨迹）\n",
    "- 用有限差分策略梯度法来调整这些参数\n",
    "- 通过给穿过场地计时来评估策略的表现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIBO的步行策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 训练前\n",
    "- 训练中\n",
    "- 训练后"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评分函数（Score Function）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们现在用分析法来计算策略梯度（compute the policy gradient analytically）\n",
    "- 让我们假设策略$\\pi_{\\theta}$在非零时是可微分的\n",
    "- 并且我们知道梯度信息$\\nabla_{\\theta}\\pi_{\\theta}(s,a)$\n",
    "- 似然比（likelihood ratios）利用了下面这个恒等式\n",
    "\\begin{split}\n",
    "\\nabla_{\\theta}\\pi_{\\theta}(s,a) & = \\pi_{\\theta}(s,a)\\frac{\\nabla_{\\theta}\\pi_{\\theta}(s, a)}{\\pi_{\\theta}(s,a)} \\\\\n",
    "& = \\pi_{\\theta}(s,a)\\nabla_{\\theta} \\log \\pi_{\\theta}(s,a)\n",
    "\\end{split}\n",
    "\n",
    "- 所以评分函数（score function）就是$\\nabla_{\\theta} \\log \\pi_{\\theta}(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax策略（Softmax Policy）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们会用softmax策略作为一个运行示例\n",
    "- 我们用特征$\\phi(s,a)^{\\intercal}\\theta$的线性组合给动作加权\n",
    "- 动作的概率和指数权重（exponentiated weight）成正比\n",
    "\\begin{equation}\n",
    "\\pi_{\\theta}(s,a)\\propto e^{\\phi(s,a)^{\\intercal}\\theta}\n",
    "\\end{equation}\n",
    "\n",
    "- 评分函数是\n",
    "\\begin{equation}\n",
    "\\nabla_{\\theta} \\log \\pi_{\\theta}(s,a)=\\phi (s, a) - \\mathbb{E}_{\\pi_{\\theta}}[\\phi(s,\\cdot)]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 高斯策略（Gaussian Policy）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在连续的动作空间中，使用高斯策略是一个自然的选择\n",
    "- 均值是状态特征的线性组合（mean is a linear combination of state features）$\\mu(s)=\\phi(s)^{\\intercal}\\theta$\n",
    "- 方差可以是固定的（fixed）$\\sigma^2$，或者也可以被参数化（parameterised）\n",
    "- 策略是高斯的（policy is Gaussian），$a \\sim \\mathcal{N}(\\mu(s),\\sigma^2)$\n",
    "- 评价函数是\n",
    "\\begin{equation}\n",
    "\\nabla_{\\theta} \\log \\pi_{\\theta}(s, a) = \\frac{(a-\\mu(s))\\phi(s)}{\\sigma^2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一步MDP（One-Step MDPs）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 让我们考虑一种简单的一步MDP\n",
    "    - 起始状态$s\\sim d(s)$\n",
    "    - 在一步后停止并伴有奖励$r=\\mathcal{R}_{s, a}$\n",
    "- 用似然比（likelihood ratios）来计算策略梯度\n",
    "\\begin{split}\n",
    "J(\\theta) & = \\mathbb{E}_{\\pi_{\\theta}}[r] \\\\\n",
    "& = \\sum_{s\\in S} d(s) \\sum_{a \\in \\mathcal{A}}\\pi_{\\theta}(s,a)\\mathcal{R}_{s,a} \\\\\n",
    "\\nabla_{\\theta} J(\\theta) & = \\sum_{s\\in \\mathcal{S}}d(s) \\sum_{a \\in \\mathcal{A}}\\pi_{\\theta}(s,a)\\nabla_{\\theta}\\log \\pi_{\\theta}(s,a)\\mathcal{R}_{s,a} \\\\\n",
    "& = \\mathbb{E}_{\\pi_{\\theta}}[\\nabla_{\\theta}\\log \\pi_{\\theta}(s,a)r]\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 策略梯度定理（Policy Gradient Theorem）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 策略梯度定理把似然性方法一般化（generalise）到了多步MDP中\n",
    "- 用长期价值（long-term value）$Q^{\\pi}(s,a)$替换瞬时奖励（instantaneous reward）$r$\n",
    "- 策略梯度定理可以用到初始状态目标、平均回报和平均价值目标上（policy gradient theorem applies to start state objective, average reward and average value objective）\n",
    "\n",
    "对任何一个可微分的策略（any differentiable policy）$\\pi_{\\theta}(s,a)$来说，对任何的策略目标函数（policy objective functions）$J=J_1$、 $J_{avR}$或是$\\frac{1}{1-\\gamma}J_{avV}$我们都有策略梯度\n",
    "\\begin{equation}\n",
    "\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\pi_{\\theta}}[\\nabla_{\\theta}\\log \\pi_{\\theta}(s,a)Q^{\\pi_{\\theta}}(s,a)]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

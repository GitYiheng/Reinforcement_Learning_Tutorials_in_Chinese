{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 探索与应用困境（Exploration vs Exploitation Dilemma）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 实时决策都涉及一个根本性的选择（online decision-making involves a fundamental choice）：\n",
    "    - 应用（exploitation）：在现有信息的基础上做出最佳决策（make the best decision given current information）\n",
    "    - 探索（exploration）：收集更多的信息（gather more information）\n",
    "- 最佳的长期策略可能涉及牺牲短期利益（the best long-term strategy may involve short-term sacrifices）\n",
    "- 收集足够的信息来做出最好的全局决策（gather enough information to make the best overall decisions）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 范例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 餐厅的选择（restaurant selection）\n",
    "    - 应用：去你最喜欢的饭店\n",
    "    - 探索：尝试新的饭店\n",
    "- 实时横幅广告（online banner advertisements）\n",
    "    - 应用：展示最成功的广告\n",
    "    - 探索：展示不同的广告\n",
    "- 石油钻探（oil drilling）\n",
    "    - 应用：在最佳已知位置钻探\n",
    "    - 探索：在新的位置钻探\n",
    "- 玩游戏（game playing）\n",
    "    - 应用：用你相信最好的策略\n",
    "    - 探索：采用试验性的策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 原理（Principles）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 朴素探索策略（naive exploration）\n",
    "    - 向贪心策略中加入噪音项，比如$\\epsilon$-贪心策略（add noise to greedy policy, like $\\epsilon - greedy$）\n",
    "- 乐观初始化策略（optimistic initialisation）\n",
    "    - 用优于真实值的估计值作为初始值，直到用真实样本来更新它们（assume the best until proven otherwise）\n",
    "- 面对不确定性时的乐观策略（optimism in the face of uncertainty）\n",
    "    - 倾向选择不确定性大的动作（prefer actions with uncertain values）\n",
    "- 概率匹配策略（probability matching）\n",
    "    - 根据每个动作可能成为最优选择的概率来进行选择（select actions according to probability they are best）\n",
    "- 信息状态搜索策略（information state search）\n",
    "    - 基于信息价值的前瞻预测搜索（lookahead search incorporating value of information）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多臂老虎机问题（The Multi-Armed Bandit）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/the_multi-armed_bandit.png\" style=\"width: 300px;\" />\n",
    "\n",
    "- 一个多臂老虎机是一个元组（a tuple）$<\\mathcal{A}, \\mathcal{R}>$\n",
    "- $\\mathcal{A}$是一个含$m$个动作的已知集合（a known set）\n",
    "- $\\mathcal{R}^a (r) = \\mathbb{P}[r \\mid a]$是一个奖励的未知概率分布（an unknown probability distribution over rewards）\n",
    "- 在每一时刻$t$智能体都会选择一个动作$a_t \\in \\mathcal{A}$\n",
    "- 环境会生成一个奖励（a reward）$r_t \\sim \\mathcal{R}^{a_t}$\n",
    "- 我们的目标就是最大化累计奖励（maximise cumulative reward）$\\sum_{\\tau =1}^t r_{\\tau}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 遗憾（Regret）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 动作价值是动作$a$的平均奖励\n",
    "\\begin{equation}\n",
    "Q(a)=\\mathbb{E}[r\\mid a]\n",
    "\\end{equation}\n",
    "\n",
    "- 最优价值$V^{\\ast}$是\n",
    "\\begin{equation}\n",
    "V^{\\ast}=Q(a^{\\ast})=\\max_{a\\in\\mathcal{A}}Q(a)\n",
    "\\end{equation}\n",
    "\n",
    "- 遗憾（regret）是单步的机会损失（opportunity loss for one step）\n",
    "\\begin{equation}\n",
    "I_t = \\mathbb{E}[V^{\\ast}-Q(a_t)]\n",
    "\\end{equation}\n",
    "\n",
    "- 总遗憾（total regret）是总机会损失（total opportunity loss）\n",
    "\\begin{equation}\n",
    "L_t = \\mathbb{E}\\left[\\sum_{\\tau = 1}^t V^{\\ast} - Q(a_{\\tau})\\right]\n",
    "\\end{equation}\n",
    "\n",
    "- 最大化累计奖励（maximise cumulative reward）$\\equiv$最小化总遗憾（minimise total regret）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 给遗憾计数（Counting Regret）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 计数（count）$N_t (a)$是选择动作$a$的期望次数（expected number of selections for action $a$）\n",
    "- 间隙（gap）$\\Delta_a$是动作$a$和最优动作$a^{\\ast}$之间的价值差，$\\Delta_a = V^{\\ast} - Q(a)$\n",
    "- 遗憾是一个关于间隙和计数的函数（regret is a function of gaps and the counts）\n",
    "\\begin{align}\n",
    "L_t & = \\mathbb{E}\\left[ \\sum_{\\tau =1}^t V^{\\ast}-Q(a_{\\tau}) \\right] \\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}}\\mathbb{E}[N_t (a)](V^{\\ast}-Q(a)) \\\\\n",
    "& = \\sum_{a \\in \\mathcal{A}}\\mathbb{E}[N_t (a)]\\Delta_a\n",
    "\\end{align}\n",
    "\n",
    "- 一个好的算法会确保大间隙的计数很少（small counts for large gaps）\n",
    "- 问题：间隙是未知的！（gaps are not known!）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性或次线性的遗憾（Linear or Sublinear Regret）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/linear_or_sublinear_regret.png\" style=\"width: 500px;\" />\n",
    "\n",
    "- 如果一个算法持续地探索（forever explores），它会有一个线性的总遗憾（linear total regret）\n",
    "- 如果一个算法从不探索（never explores），它会有一个线性的总遗憾（linear total regret）\n",
    "- 那么我们可不可能取得一个次线性的总遗憾（sublinear total regret）呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 贪心算法（Greedy Algorithm）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 让我们来考虑估值$\\hat{Q}_t (a) \\approx Q(a)$的算法\n",
    "- 用蒙特卡洛评估法来估计每个动作的价值（estimate the value of each action by Monte-Carlo evaluation）\n",
    "\\begin{equation}\n",
    "\\hat{Q}_t (a) = \\frac{1}{N_t (a)}\\sum_{t=1}^T r_t \\textbf{1} (a_t = a)\n",
    "\\end{equation}\n",
    "\n",
    "- 贪心算法选择价值最高的动作（the greedy algorithm selects action with highest value）\n",
    "\\begin{equation}\n",
    "a_t^{\\ast} = \\operatorname*{argmax}_{a \\in \\mathcal{A}}\\hat{Q}_t (a)\n",
    "\\end{equation}\n",
    "\n",
    "- 贪心策略会一直停留在次优动作的选项上（greedy can lock onto a suboptimal action forever）\n",
    "- $\\Rightarrow$ 贪心策略的总遗憾是线性的（greedy has linear total regret）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\epsilon$-贪心算法（$\\epsilon$-Greedy Algorithm）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\epsilon$-贪心算法会持续探索（continues to explore forever）\n",
    "    - 选择$a=\\operatorname*{argmax}_{a\\in\\mathcal{A}}\\hat{Q}(a)$的概率为$1-\\epsilon$\n",
    "    - 选择随机动作的概率为$\\epsilon$\n",
    "- 恒定的$\\epsilon$保证了最小遗憾（constant $\\epsilon$ ensures minimum regret）\n",
    "\\begin{equation}\n",
    "I_t \\geq \\frac{\\epsilon}{\\mathcal{A}}\\sum_{a\\in\\mathcal{A}}\\Delta_a\n",
    "\\end{equation}\n",
    "\n",
    "- $\\Rightarrow$ $\\epsilon$-贪心策略的总遗憾是线性的（$\\epsilon$-greedy has linear total regret）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 乐观初始化策略（Optimistic Initialisation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这是一个简单并且实用的思想（simple and practical idea）：用高于真实值的初始值来对$Q(a)$进行初始化（initialise $Q(a)$ to high value）\n",
    "- 用渐进式蒙特卡洛评估来更新动作值（update action value by incremental Monte-Carlo evaluation）\n",
    "- 用$N(a) > 0$作为开始\n",
    "\\begin{equation}\n",
    "\\hat{Q}_t(a_t) = \\hat{Q}_{t-1}+\\frac{1}{N_t(a_t)}(r_t - \\hat{Q}_{t-1})\n",
    "\\end{equation}\n",
    "\n",
    "- 促使早期的系统性探索（encourages systematic exploration early on）\n",
    "- 但仍旧可能停留在次优动作上（lock onto suboptimal action）\n",
    "- $\\Rightarrow$ 贪心策略加上乐观初始化的总遗憾是线性的（linear total regret）\n",
    "- $\\Rightarrow$ $\\epsilon$-贪心策略加上乐观初始化的总遗憾是线性的（linear total regret）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 衰减$\\epsilon_t$-贪心算法（Decaying $\\epsilon_t$-Greedy Algorithm）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 选择衰减的进程 $\\epsilon_1, \\epsilon_2, \\ldots$（pick a decay schedule for $\\epsilon_1, \\epsilon_2, \\ldots$）\n",
    "- 让我们来考虑下面这个进程（schedule）\n",
    "\\begin{align}\n",
    "c & > 0 \\\\\n",
    "d & = \\min_{a \\mid \\Delta_a > 0} \\Delta_i \\\\\n",
    "\\epsilon_t & = \\min \\left\\{ 1, \\frac{c \\lvert \\mathcal{A} \\rvert}{d^2 t} \\right\\}\n",
    "\\end{align}\n",
    "\n",
    "- 衰减$\\epsilon_t$-贪心策略的总遗憾是对数渐进的（logarithmic asymptotic total regret）！\n",
    "- 坏消息是，进程的设置需要更多关于间隙的信息（schedule requires advance knowledge of gaps）\n",
    "- 目标：（在没有关于$\\mathcal{R}$的信息的情况下）寻找一种对任何多臂老虎机来说遗憾都是次线性的算法（find an algorithm with sublinear regret for any multi-armed bandit without knowledge of $\\mathcal{R}$）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下限（Lower Bound）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 任何算法的性能都是由最优手柄和其他手柄之间的的相似性决定的（the performance of any algorithm is determined by similarity between optimal arm and other arms）\n",
    "- 难题会有奖励均值不同但看上去相似的手柄（hard problems have similar-looking arms with different means）\n",
    "- 这可以用间隙（gap）$\\Delta_a$和分布相似性（similarity in distributions）$KL(\\mathcal{R}^a \\| \\mathcal{R}^{a^{\\ast}})$进行形式化的描述\n",
    "\n",
    "渐进总遗憾与步数的关系至少是高于其对数的（asymptotic total regret is at least logarithmic in number of steps）\n",
    "\\begin{equation}\n",
    "\\lim_{t \\rightarrow \\infty} L_t \\geq \\log t \\sum_{a \\mid \\Delta_a > 0} \\frac{\\Delta_a}{KL(\\mathcal{R}^a \\| \\mathcal{R}^{a^{\\ast}})}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 面对不确定性时的乐观策略（Optimism in the Face of Uncertainty）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/optimism_in_the_face_of_uncertainty.png\" style=\"width: 500px;\" />\n",
    "\n",
    "- 那我们应该选择哪个动作呢？\n",
    "- 一个动作价值的不确定性越高（more uncertain we are about an action-value）\n",
    "- 我们探索这个动作的重要性就越高（more important it is to explore that action）\n",
    "- 这个动作可能就是最优动作（it could turn out to be the best action）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在选择蓝色动作后\n",
    "- 我们对它的价值的不确定性就会减少\n",
    "- 也更有可能选择另一个动作\n",
    "- 直到我们锁定在最佳动作上（home in on best action）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 置信上界（Upper Confidence Bounds）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对每个动作价值我们都对它的置信上界$\\hat{U}_t (a)$进行估值\n",
    "- 从而使得$Q(a)\\leq \\hat{Q}_t (a) + \\hat{U}_t (a)$有较高的概率\n",
    "- 这取决于被选择的次数$N(a)$\n",
    "    - 当$N_t (a)$较小时 $\\Rightarrow$ $\\hat{U}_t (a)$会较大（估值的不确定性较大）\n",
    "    - 当$N_t (a)$较大时 $\\Rightarrow$ $\\hat{U}_t (a)$会较小（估值会比较准确）\n",
    "- 选择置信上界（Upper Confidence Bound, or UCB）最大的动作\n",
    "\\begin{equation}\n",
    "a_t = \\operatorname*{argmax}_{a \\in \\mathcal{A}} \\hat{Q}_t (a) + \\hat{U}_t (a)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hoeffding不等式（Hoeffding's Inequality）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们假设 $X_1, \\ldots, X_t$ 都是在$[0,1]$区间中iid的随机变量（independent and identically distributed random variables），样本均值（sample mean）为$\\overline{X}_t = \\frac{1}{\\tau}\\sum_{\\tau = 1}^t X_{\\tau}$。这样的话\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{P}\\left[ \\mathbb{E}[X] > \\overline{X}_t + u \\right] \\leq e^{-2t u^2}\n",
    "\\end{equation}\n",
    "\n",
    "- 我们会在老虎机的奖励上使用Hoeffding不等式（apply Hoeffding's Inequality to rewards of the bandit）\n",
    "- 以选择动作$a$为条件\n",
    "\\begin{equation}\n",
    "\\mathbb{P}\\left[ Q(a) > \\hat{Q}_t (a) + U_t (a) \\right] \\leq e^{-2 N_t (a) U_t (a)^2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算置信上界（Calculating Upper Confidence Bounds）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 选择真实价值超过UCB的一个概率$p$\n",
    "- 解$U_t (a)$\n",
    "\\begin{align}\n",
    "e^{-2 N_t (a) U_t (a)^2} & = p \\\\\n",
    "U_t (a) & = \\sqrt{\\frac{-\\log p}{2 N_t (a)}}\n",
    "\\end{align}\n",
    "\n",
    "- 当我们观察到更多的奖励时减少$p$，比如说 $p=t^{-4}$\n",
    "- 确保当$t \\rightarrow \\infty$时我们选择最优动作的表达式是\n",
    "\\begin{equation}\n",
    "U_t (a) = \\sqrt{\\frac{2\\log t}{N_t (a)}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCB1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这也导出了UCB1算法\n",
    "\\begin{equation}\n",
    "a_t = \\operatorname*{argmax}_{a\\in \\mathcal{A}} Q(a) + \\sqrt{\\frac{2\\log t}{N_t (a)}}\n",
    "\\end{equation}\n",
    "\n",
    "UCB算法的总遗憾是对数渐进的（the UCB algorithm achieves logarithmic asymptotic total regret）\n",
    "\n",
    "\\begin{equation}\n",
    "\\lim_{t \\rightarrow \\infty} L_t \\leq 8 \\log t \\sum_{a \\mid \\Delta_a > 0} \\Delta_a\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 范例-UCB和$\\epsilon$-贪心策略在10臂老虎机上的比较（Example: UCB vs $\\epsilon$-Greedy on 10-Armed Bandit）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/example_ucb_vs_epsilon-greedy_on_10-armed_bandit.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 贝叶斯老虎机策略（Bayesian Bandits）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 到现在为止我们都没有对奖励分布$\\mathcal{R}$做任何假设（no assumptions about the reward distribution $\\mathcal{R}$）\n",
    "    - 除了在奖励范围上的假设（except bounds on rewards）\n",
    "- 贝叶斯老虎机策略就应用了关于奖励的先验知识（prior knowledge of rewards），$p[\\mathcal{R}]$\n",
    "- 贝叶斯老虎机策略在这个基础上计算了奖励的后验分布（posterior distribution of rewards）$p[\\mathcal{R}\\mid h_t]$\n",
    "    - 这里的历史记录（history）被记作$h_t = a_1, r_1,\\ldots,a_{t-1},r_{t-1}$\n",
    "- 利用得到的后验信息来指导接下来的探索（use posterior to guide exploration）\n",
    "    - 贝叶斯置信上界策略（upper confidence bounds, or namely Bayesian UCB）\n",
    "    - 概率匹配策略，或者叫汤普森取样策略（probability matching, or namely Thompson sampling）\n",
    "- 先验信息越准确，策略的性能就越好（better performance if prior knowledge is accurate）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 贝叶斯UCB的范例：独立高斯分布（Bayesian UCB Example: Independent Gaussians）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 让我们假设奖励是遵循高斯分布的，$\\mathcal{R}_a (r) = \\mathcal{N} (r;\\mu_a \\sigma_a^2)$\n",
    "<img src=\"files/figures/bayesian_ucb_example_independent_gaussians.png\" style=\"width: 500px;\" />\n",
    "\n",
    "- 我们接下来（用贝叶斯法则）计算用$\\mu_a$和$\\sigma_a^2$来描述的高斯后验\n",
    "\\begin{equation}\n",
    "p[\\mu_a,\\sigma_a^2\\mid h_t] \\propto p[\\mu_a,\\sigma_a^2] \\prod_{t\\mid a_t=a}\\mathcal{N}(r_t;\\mu_a,\\sigma_a^2)\n",
    "\\end{equation}\n",
    "\n",
    "- 选择使$Q(a)$的标准差最大的动作\n",
    "\\begin{equation}\n",
    "a_t = \\operatorname*{argmax}_{a\\in\\mathcal{A}}\\mu_a +\\frac{c\\sigma_a}{\\sqrt{N(a)}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概率匹配策略（Probability Matching）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 概率匹配策略会根据动作$a$是最优动作的概率来对动作进行选择\n",
    "\\begin{equation}\n",
    "\\pi (a\\mid h_t) = \\mathbb{P}[Q(a) > Q(a'),\\forall a' \\neq a \\mid h_t]\n",
    "\\end{equation}\n",
    "\n",
    "- 概率匹配策略是一个在面对不确定性时的乐观策略（probability matching is optimistic in the face of uncertainty）\n",
    "    - 不确定性大的动作取值最大的概率更高（uncertain actions have higher probability of being max）\n",
    "- 用后验信息来计算出解析解的难度较大（can be difficult to compute analytically from posterior）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 汤普森取样策略（Thompson Sampling）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 汤普森取样策略实现了概率匹配\n",
    "\\begin{align}\n",
    "\\pi (a\\mid h_t) & = \\mathbb{P}[Q(a) > Q(a'),\\forall a' \\neq a \\mid h_t] \\\\\n",
    "& = \\mathbb{E}_{\\mathcal{R}\\mid h_t}\\left[ \\textbf{1}(a=\\operatorname*{argmax}_{a\\in\\mathcal{A}}Q(a)) \\right]\n",
    "\\end{align}\n",
    "\n",
    "- 用贝叶斯法则来计算后验分布$p[\\mathcal{R}\\mid h_t]$\n",
    "- 从后验信息中取样（sample）一个奖励分布$\\mathcal{R}$\n",
    "- 计算动作价值函数$Q(a)=\\mathbb{E}[\\mathcal{R}_a]$\n",
    "- 选择最大化样本价值的动作（select action maximising value on sample），$a_t = \\operatorname*{argmax}$_{a\\in \\mathcal{A}}Q(a)\n",
    "\n",
    "- 汤普森取样策略达到了Lai和Robbins的下限！（Thompson sampling achieves Lai and Robbins lower bound!）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 信息的价值（Value of Information）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 因为探索能获取更多的信息所以是非常有价值的（exploration is useful because it gains information）\n",
    "- 我们能够定量地描述信息的价值吗（quantify the value of information）？\n",
    "    - 那么决策者在决策前会用多少奖励来换取信息呢\n",
    "    - 在获取信息后的长期奖励 - 即时奖励\n",
    "- 不确定情况下的信息增益会更大（information gain is higher in uncertain situations）\n",
    "- 因此更多地探索不确定的情况是在理的\n",
    "- 如果我们知道了怎样定义信息的价值，那么我们就可以用最好的方式在探索和应用中作出权衡（if we know value of information, we can trade-off exploration and exploitation optimally）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 信息状态空间（Information State Space）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们把老虎机看作是一个单步决策问题（view bandits as one-step decision-making problems）\n",
    "- 也可以看作是一个序列决策问题（sequential decision-making problems）\n",
    "- 每一步时我们都有一个信息状态（information state）$\\tilde{s}$\n",
    "    - $\\tilde{s}$是一个关于历史信息的统计量（statistic of the history），$\\tilde{s}_t = f(h_t)$\n",
    "    - 概述了到现在为止累计的所有信息（summarise all information accumulated so far）\n",
    "- 每个动作$a$都有$\\tilde{\\mathcal{P}}_{\\tilde{s},\\tilde{s}'}^a$的概率会（通过增加信息量）导致向新的信息状态$\\tilde{s}'$的转移（each action $a$ causes a transition to a new information state $\\tilde{s}'$ by adding information, with probability $\\tilde{\\mathcal{P}}_{\\tilde{s},\\tilde{s}'}^a$）\n",
    "- 这定义了一个在增广信息状态空间（augmented information state space）上的MDP$\\tilde{M}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最初编辑日期"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年4月25日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/XX.pdf\n",
    "\n",
    "[2] https://www.youtube.com/watch?v=sGuiWX07sKw"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于深度强化学习的连续控制 (Continuous Control with Deep Reinforcement Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 标签 (Labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reinforcement Learning\n",
    "- Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvard引用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D. and Wierstra, D., 2015. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BibTex引用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "@article{lillicrap2015continuous,\n",
    "  title={Continuous control with deep reinforcement learning},\n",
    "  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},\n",
    "  journal={arXiv preprint arXiv:1509.02971},\n",
    "  year={2015}\n",
    "}\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 摘要 (Abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We adapt the idea underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policy \"end-to-end\": directly from raw pixel inputs.\n",
    "\n",
    "基于Deep Q-Learning的基本思想，我们把它应用在了连续动作领域（continuous action domain）。我们会介绍一个基于确定性梯度策略（deterministic policy gradient）的actor-critic、无模型算法，这个算法可以被运用在连续动作空间上。在学习算法、网络结构和超参数（hyper-parameters）不改变的情况下，我们的算法可靠地求解了包括cartpole swing-up、dexterous manipulation、legged locomotion和car driving等20多个物理模拟任务。我们的算法能够找到性能与有模型规划算法相匹敌的策略。我们也进一步展示了对于许多任务来说，我们的算法能够对策略进行“端到端”的学习：也就是直接从原始像素输入中学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主要贡献 (Contributions)\n",
    "\n",
    "(1). DDPG is proposed to solve problems with continuous action domain which is difficult and unstable with DQN.\n",
    "\n",
    "- DDPG的提出主要是为了解决DQN无法处理动作空间是连续的问题。\n",
    "\n",
    "(2). DDPG adopts the use of a replay buffer to perform off-policy learning to minimize correlations between samples.\n",
    "\n",
    "- DDPG沿用了DQN的replay buffer进行off-policy学习来减弱样本间的关联性。\n",
    "\n",
    "(3). DDPG adopts the use of a separate target Q network to give consistent targets during temporal difference backups\n",
    "\n",
    "- DDPG沿用了DQN中单独的target Q network，这样在进行temporal difference backups时就有了稳定的target。\n",
    "\n",
    "(4). DDPG makes use of the batch normalization technique from deep learning.\n",
    "\n",
    "- DDPG引入了深度学习中的batch normalization。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概要 (Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While DQN solves problems with high-dimensional observation spaces, it can only handle discrete and low-dimensional action spaces. Many tasks of interest, most notably physical control tasks, have continuous (real valued) and high dimensional action spaces. DQN cannot be straightforward applied to continuous domains since it relies on a finding the action that maximizes the action-value function, which in the continuous valued case requires an iterative optimization process at every step.\n",
    "\n",
    "尽管DQN可以解决高维度观测空间的问题，但它只能处理离散和低维度的动作空间。但许多比如物理控制任务都是基于连续和高维动作空间的。因为DQN是基于最大化动作价值函数来选择动作的，而在连续值的情况下每一步都需要迭代优化，所以DQN不能被直接应用在连续动作空间上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN is not suitable for problems with continuous action space. An obvious approach to adapting deep reinforcement learning methods such as DQN to continuous domains is to simply discretize the action space. This would lead to 2 problems:\n",
    "\n",
    "- The curse of dimensionality: the number of actions increases exponentially with the number of degrees of freedom.\n",
    "\n",
    "- Naive discretization of action spaces needlessly throws away information about the structure of the action domain, which may be essential for solving many problems.\n",
    "\n",
    "DQN并不适合有连续动作空间的问题。对于有连续动作空间的问题，一个明显的方法就是在像DQN这样的深度强化学习算法上稍作修改，对动作空间进行离散化。\n",
    "\n",
    "- 维度诅咒：动作的数量会根据自由度的增加呈指数型增长。\n",
    "\n",
    "- 对动作空间过度简化的离散操作同时也无谓地舍弃了动作空间的结构信息，这些信息对于许多问题来说可能都是很重要的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDPG combines the actor-critic approach with insights from the recent success of Deep Q Network (DQN). Prior to DQN, it was generally believed that learning value functions using large, non-linear function approximators was difficult and unstable. DQN is able to learn value functions using such function approximators in a stable and robust way due to 2 innovations:\n",
    "\n",
    "- The network is trained off-policy with samples from a replay buffer to minimize correlations between samples.\n",
    "\n",
    "- The network is trained with a target Q network to give consistent targets during temporal difference backups.\n",
    "\n",
    "DDPG结合了actor-critic方法和近期在DQN上取得的新的见解。在DQN之前，用大型非线性的函数近似器来学习价值函数被普遍认为是非常困难而且不稳定的。DQN能够用这样的函数近似器来稳定且可靠地学习价值函数归功于2项创新：\n",
    "\n",
    "- 网络的训练是脱离策略的，样本是从重播缓冲器中提取的，这样就减弱了样本间的关联性。\n",
    "\n",
    "- 网络是用目标Q network来训练的，这样就能在时差备份时用相同的目标了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the aforementioned 2 ideas from DQN, DDPG also makes use of a technique from deep learning.\n",
    "\n",
    "- Batch normalization\n",
    "\n",
    "除了上面提到的2个从DQN中沿用过来思想，DDPG还从深度学习学习了最新的思想：\n",
    "\n",
    "- 批标准化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 之前的研究 (Previous Work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-learning:\n",
    "\n",
    "- Watkins, C.J. and Dayan, P., 1992. Q-learning. Machine learning, 8(3-4), pp.279-292.\n",
    "\n",
    "#### Deep Q network (DQN):\n",
    "\n",
    "- Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D. and Riedmiller, M., 2013. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602.\n",
    "\n",
    "- Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G. and Petersen, S., 2015. Human-level control through deep reinforcement learning. Nature, 518(7540), p.529.\n",
    "\n",
    "#### Deterministic policy gradient (DPG):\n",
    "\n",
    "- Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D. and Riedmiller, M., 2014, June. Deterministic policy gradient algorithms. In ICML.\n",
    "\n",
    "- Hafner, R. and Riedmiller, M., 2011. Reinforcement learning in feedback control. Machine learning, 84(1-2), pp.137-169.\n",
    "\n",
    "- Prokhorov, D.V. and Wunsch, D.C., 1997. Adaptive critic designs. IEEE transactions on Neural Networks, 8(5), pp.997-1007.\n",
    "\n",
    "#### Batch normalization:\n",
    "\n",
    "- Ioffe, S. and Szegedy, C., 2015. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 假设 (Assumptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors consider reinforcement learning setup consisting of an agent interacting with an environment $E$ in discrete timesteps. At each timestep $t$ the agent receives an observation $x_t$, takes an action $a_t$ and receives a scalar reward $r_t$. In all the environments considered here the actions are real-valued $a_t \\in \\mathbb{R}^N$. In general, the **environment may be partially observed** so that the entire history of the observation, action pair $s_t = (x_1, a_1, \\ldots, a_{t-1}, x_t)$ may be required to describe the state. Here, authors **assumed the environment is fully-observed** so $s_t = x_t$.\n",
    "\n",
    "An agent's behaviour is defined by a policy, $\\pi$, which maps states to a probability distribution over the actions $\\pi: \\mathcal{S}\\rightarrow\\mathcal{P}(\\mathcal{A})$. The environment, $E$, may also be **stochastic**. Authors model it as a Markov decision process (MDP) with a state space $\\mathcal{S}$, action space $\\mathcal{A}=\\mathbb{R}^N$, an initial state distribution $p(s_1)$, transition dynamics $p(s_{t+1} \\mid s_t, a_t)$, and reward function $r(s_t, a_t)$.\n",
    "\n",
    "The return from a state is defined as the sum of discounted future reward $R_t = \\sum_{i=t}^{T} \\gamma^{(i-t)} r(s_i, a_i)$ with a discounting factor $\\gamma \\in [0, 1]$. Note that the return depends on the actions chosen, and therefore on the policy $\\pi$, and may be stochastic. The goal in reinforcement learning is to learn a policy which maximizes the expected return from the start distribution $J=\\mathbb{E}_{r_i, s_i \\sim E, a_i \\sim \\pi} [R_1]$. Authors denote the discounted state visitation distribution for a policy $\\pi$ as $\\rho^\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初次编辑日期 (Initial Edit Date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年5月13日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献 (References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D. and Wierstra, D., 2015. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

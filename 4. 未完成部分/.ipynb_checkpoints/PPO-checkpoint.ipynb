{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目录"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 强化学习简介\n",
    "\n",
    "* Policy gradient\n",
    "\n",
    "* Actor-critic\n",
    "\n",
    "* PPO (proximal policy optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 什么是强化学习？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try - Error - Learning\n",
    "\n",
    "行动 - 评估 - 学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 强化学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Agents) learn to make good sequences of decisions.\n",
    "\n",
    "（智能体）学习最优的决策序列。\n",
    "\n",
    "* \"Learn to make\": don't know in advance how world works.\n",
    "\n",
    "    * “学习做出”：事先并不知道环境的信息。\n",
    "\n",
    "* \"Good\": reward for sequence of decisions.\n",
    "\n",
    "    * “最优的”：一系列决策的奖励。\n",
    "\n",
    "* \"Sequences of decisions\": repeated interactions with world.\n",
    "\n",
    "    * “决策序列”：与环境重复的交互。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习是基于什么的呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从智能体周围的环境和奖励中学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP (Markov Decision Process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/deep_reinforcement_learning.png\" style=\"width: 500px;\" />\n",
    "\n",
    "不断重复这一流程\n",
    "\n",
    "$$s_0, a_0, r_1, s_1, a_1, r_2, \\ldots, s_T$$\n",
    "\n",
    "* 状态 (state): 状态就是环境的输入\n",
    "\n",
    "* 动作 (action): 动作可以是离散的或连续的 (discrete or continuous)\n",
    "\n",
    "* 奖励 (reward): 一个标量值\n",
    "\n",
    "* 状态迁移概率 (state transition probability) $P_{s s'}^{a}$: 迁移到下一个状态的概率\n",
    "\n",
    "* 折扣系数 (discount factor) $\\gamma$: 未来价值的权重\n",
    "\n",
    "* 策略 (policy): 在特定状态执行特定动作的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q函数和V函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状态价值函数 (value function, V): 在当前状态下预期的未来累计奖励之和\n",
    "\n",
    "$$v(s) = \\mathbb{E}\\left[ R_{t+1} + \\gamma R_{t+2} + \\cdots \\mid S_t = s \\right]$$\n",
    "\n",
    "动作状态价值函数 (action-value function, Q): 在当前状态下执行特定动作预期的未来累计奖励之和\n",
    "\n",
    "$$q(s, a) = \\mathbb{E} \\left[ R_{t+1} + \\gamma R_{t+2} + \\cdots \\mid S_t = s, A_t = a \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 范例: Grid World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/grid_world_example_1.png\" style=\"width: 300px;\" />\n",
    "\n",
    "* 目标：\n",
    "\n",
    "    * 智能体（红色正方形）规避障碍物（绿色三角形）到达目标点（蓝色圆形）。\n",
    "\n",
    "* 环境:\n",
    "\n",
    "    * $5 \\times 5$ 的 grid world\n",
    "    \n",
    "* 信息：\n",
    "    \n",
    "    * 包括了智能体、环境、状态、动作、奖励\n",
    "\n",
    "* 奖励：\n",
    "\n",
    "    * 智能体到达障碍物位置时，奖励为 (-1)\n",
    "    * 智能体到达目标位置时，奖励为 (+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid World作为DMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 状态 S = 在grid world中的位置坐标\n",
    "\n",
    "* 动作 A = { up, down, right, left }\n",
    "\n",
    "* 奖励 R = { 障碍物 (-1), 目标 (+1) }\n",
    "\n",
    "* 状态迁移概率 $P_{s s'}^{a}$ = 到达下一个状态 ($s'$) 的概率\n",
    "\n",
    "* 折扣系数 $\\gamma$ = 未来价值的权重\n",
    "\n",
    "* 策略 $\\pi (a \\mid s)$ = 在特定状态执行特定动作的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深度强化学习 (Deep Reinforcement Learning)\n",
    "\n",
    "强化学习：基于策略 (policy) 的或基于动作价值函数 (Q-value)\n",
    "\n",
    "深度学习：深度神经网络用作函数近似 (function approximation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 函数近似 (Function Approximation)\n",
    "\n",
    "* **Value-based RL - Q-function approximation**\n",
    "\n",
    "<img src=\"files/figures/value-based_function_approximation.png\" style=\"height: 300px;\" />\n",
    "\n",
    "* **Policy-based RL - policy approximation:**\n",
    "\n",
    "<img src=\"files/figures/policy-based_function_approximation.png\" style=\"height: 300px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning Landscape\n",
    "\n",
    "* Policy optimization\n",
    "    * (DFO) devirative-free optimization/evolution\n",
    "    * Policy gradient\n",
    "* Dynamic programming\n",
    "    * Policy iteration\n",
    "    * Value iteration\n",
    "        * Q-learning\n",
    "    * Modified policy iteration (between policy iteration and value iteration)\n",
    "* Policy gradient + value iteration = actor-critic methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO: policy optimization中性能较好的算法\n",
    "\n",
    "Policy optimization: 用深度学习来学习最优策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network\n",
    "\n",
    "<img src=\"files/figures/policy-based_function_approximation.png\" style=\"height: 300px;\" />\n",
    "\n",
    "Input: State $S$\n",
    "\n",
    "Output: Policy $\\pi_{\\theta} (a \\mid s) = \\mathbb{P} \\left[ A_t = a \\mid S_t = s, \\theta \\right]$\n",
    "\n",
    "Function approximator: 深度神经网络近似的最优策略\n",
    "\n",
    "Optimal policy: 最大化奖励之和的策略，也就是最大化目标函数的策略 $J(\\theta)=\\mathbb{E}\\left[ \\sum_{t=1}^{T} r_t \\mid \\pi_{\\theta} \\right] = \\mathbb{E} \\left[ r_1 + r_2 + r_3 + \\cdots + r_T \\mid \\pi_{\\theta} \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 寻找最大化目标函数的$\\theta$\n",
    "\n",
    "\\begin{split}\n",
    "\\operatorname*{argmax}_{\\theta} J(\\theta) & = \\operatorname*{argmax}_{\\theta} \\mathbb{E} \\left[ \\sum_{t=0}^T r_t \\mid \\pi_{\\theta} \\right] \\\\\n",
    "& = \\operatorname*{argmax}_{\\theta} \\sum_{t=0}^{T-1} P(s_t, a_t \\mid \\tau) r_{t+1}\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用Deep Neural Network来近似Policy\n",
    "\n",
    "用gradient ascent来向最大化目标函数的方向进行更新\n",
    "\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\nabla_{\\theta} J(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline算法：理解Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初次编辑日期 (Initial Edit Date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年5月21日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献 (References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] https://github.com/wooridle/DeepRL-PPO-tutorial\n",
    "\n",
    "[2] http://web.stanford.edu/class/cs234/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

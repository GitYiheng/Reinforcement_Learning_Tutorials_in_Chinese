{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 什么是动态规划(Dynamic Programming)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "动态（Dynamic）：问题的序列或是时序元素\n",
    "\n",
    "规划（Programming）：优化一个方案，或是说优化一个策略。例如线性规划（linear programming）\n",
    "\n",
    "- 一种解决复杂问题的方法\n",
    "\n",
    "- 把大问题化解程子问题\n",
    "\n",
    "    - 解决子问题\n",
    "    \n",
    "    - 把子问题的解决方案结合起来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 动态规划的前提"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "动态规划对满足以下两点性质的问题是非常普适的方案：\n",
    "\n",
    "- 最优子结构（optimal substructure）\n",
    "\n",
    "    - 适用优化原则\n",
    "    \n",
    "    - 最优解可以被分解成子问题\n",
    "\n",
    "- 子问题有重叠部分（overlapping subproblems）\n",
    "\n",
    "    - 子问题会重复出现多次\n",
    "    \n",
    "    - 解能够被缓存并再次利用\n",
    "\n",
    "- 马尔科夫决策过程满足了以上两点性质\n",
    "\n",
    "    - 贝尔曼方程给出了递归分解\n",
    "    \n",
    "    - 价值函数储存并再次使用这些解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用动态规划来规划"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 动态规划的前提假设是我们有MDP的完全信息\n",
    "\n",
    "- 它被用来给一个MDP做规划\n",
    "\n",
    "- 对于预测问题：\n",
    "\n",
    "    - 输入： MDP$<S,A,P,R,\\gamma>$和策略$\\pi$（或是MRP$<S,P^{\\pi},R^{\\pi},\\gamma>$）\n",
    "    \n",
    "    - 输出： 价值函数$v_{\\pi}$\n",
    "\n",
    "- 对于控制问题：\n",
    "\n",
    "    - 输入： MDP $<S,A,P,R,\\gamma>$\n",
    "    \n",
    "    - 输出： 最优价值函数$v_{\\ast}$和最优策略$\\pi_{\\ast}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 迭代策略评估（Iterative Policy Evaluation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 问题： 评估一个给定的策略$\\pi$\n",
    "\n",
    "- 方案： 贝尔曼期望备份的迭代应用\n",
    "\n",
    "- $v_1 \\rightarrow v_2 \\rightarrow \\ldots \\rightarrow v_{\\pi}$\n",
    "\n",
    "- 使用同步备份（synchronous backups），\n",
    "\n",
    "    - 在每次$（k+1）$迭代\n",
    "    \n",
    "    - 对所有状态$s \\in S$\n",
    "    \n",
    "    - 根据$v_{k}(s')$来更新$v_{k+1}(s)$\n",
    "    \n",
    "    - $s'$是$s$的后继状态\n",
    "\n",
    "- 在后面我们会提到异步备份（asynchronous backups）\n",
    "\n",
    "- 后面会证明向$v_{\\pi}$的收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/iterative_policy_evaluation.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "\\begin{equation}\n",
    "v_{k+1}(s) = \\sum_{a \\in A} \\pi(a|s)(R_{s}^{a} + \\gamma \\sum_{s' \\in S} P_{s s'}^{a} v_{k}(s')) \\\\\n",
    "v^{k+1} = R^{\\pi} + \\gamma P^{\\pi} v^{k}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评估在小型Gridworld中的一个随机策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/small_gridworld_1.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "- 无折扣的片段MDP（undiscounted episodic MDP），也就是$\\gamma=1$\n",
    "\n",
    "- 非终止状态 $1,\\ldots,14$\n",
    "\n",
    "- 一个终止状态（用两个带阴影的格子来表示）\n",
    "\n",
    "- 越出格子的动作并不会改变状态\n",
    "\n",
    "- 在没有达到终点前的奖励都是$-1$\n",
    "\n",
    "- 智能体遵循均匀随机策略\n",
    "\n",
    "\\begin{equation}\n",
    "\\pi(n|\\cdot) = \\pi(e|\\cdot) = \\pi(s|\\cdot) = \\pi(w|\\cdot) = 0.25\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/small_gridworld_2.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/small_gridworld_3.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 怎样改进一个策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 给定一个策略$\\pi$\n",
    "\n",
    "    - 评估策略$\\pi$\n",
    "    \n",
    "    \\begin{equation}\n",
    "    v_{\\pi}(s) = E[R_{t+1} + \\gamma R_{t+2} + \\ldots | S_{t}=s]\n",
    "    \\end{equation}\n",
    "\n",
    "    - 以$v_{\\pi}$为基础通过贪心的动作选取来改进策略\n",
    "    \n",
    "    \\begin{equation}\n",
    "    \\pi' = greedy(v_{\\pi})\n",
    "    \\end{equation}\n",
    "\n",
    "- 在小型Gridworld中改进过的策略是最优的，$\\pi'=\\pi^{\\ast}$\n",
    "\n",
    "- 在一般情况下，我们需要更多的迭代来做改进和评估\n",
    "\n",
    "- 但这个策略迭代过程终究会收敛到$\\pi_{\\ast}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 策略迭代（Policy Iteration）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "策略评估（policy evaluation）： 用迭代策略评估来估算$v_{\\pi}$\n",
    "\n",
    "策略改进（policy improvement）： 用贪心策略改进来生成$\\pi' \\geq \\pi$\n",
    "\n",
    "<img src=\"files/figures/policy_iteration_1.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "<img src=\"files/figures/policy_iteration_2.png\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 杰克的汽车租赁问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 状态： 两个地点，每个地点最多容纳20辆汽车\n",
    "\n",
    "- 动作： 一个晚上可以在两个地点之间转移5辆汽车\n",
    "\n",
    "- 奖励： 每辆出租的汽车都会带来$10的收益（汽车必须可用）\n",
    "\n",
    "- 状态转移： 汽车的返还和申请都是随机的\n",
    "\n",
    "    - 泊松分布，$n$返还/申请的概率是$\\frac{\\lambda^{n}}{n !}e^{-\\lambda}$\n",
    "    \n",
    "    - 第一个地点： 平均申请=3，平均返还=3\n",
    "    - 第二个地点： 平均申请=4，平均返还=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 杰克的汽车租赁问题中的策略迭代"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/policy_iteration_in_jacks_car_rental.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 策略改进（Policy Improvement）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 考虑一个确定性的策略，$a=\\pi(s)$\n",
    "\n",
    "- 我们可以通过贪心的动作选择（acting greedily）来改进这个策略\n",
    "\n",
    "\\begin{equation}\n",
    "\\pi'(s) = argmax_{a \\in A} q_{\\pi} (s, a)\n",
    "\\end{equation}\n",
    "\n",
    "- 我们像这样在所有状态$s$上通过单步更新来改进价值函数，\n",
    "\n",
    "\\begin{equation}\n",
    "q_{\\pi}(s, \\pi'(s)) = max_{a \\in A} q_{\\pi}(s, a) \\geq q_{\\pi} (s, \\pi(s)) = v_{\\pi}(s)\n",
    "\\end{equation}\n",
    "\n",
    "- 我们用这种方法来改进价值函数，$v_{\\pi'}(s) \\geq v_{\\pi}(s)$\n",
    "\n",
    "\\begin{align}\n",
    "v_{\\pi}(s) & \\leq q_{\\pi}(s, \\pi'(s)) = E_{\\pi'}[R_{t+1}+\\gamma v_{\\pi}(S_{t+1})|S_t=s] \\\\\n",
    "& \\leq E_{\\pi'}[R_{t+1}+\\gamma q_{\\pi}(S_{t+1},\\pi'(S_{t+1}))|S_t=s] \\\\\n",
    "& \\leq E_{\\pi'}[R_{t+1}+\\gamma R_{t+2}+\\gamma^2 q_{\\pi}(S_{t+2},\\pi'(S_{t+2}))|S_t=s] \\\\\n",
    "& \\leq E_{\\pi'}[R_{t+1}+\\gamma R_{t+2}+\\ldots|S_t=s]=v_{\\pi'}(s)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 当改进停止时，\n",
    "\n",
    "\\begin{equation}\n",
    "q_{\\pi}(s, \\pi'(s)) = max_{a \\in A} q_{\\pi}(s, a) = q_{\\pi}(s, \\pi(s)) = v_{\\pi}(s)\n",
    "\\end{equation}\n",
    "\n",
    "- 这时贝尔曼最优方程满足以下条件\n",
    "\n",
    "\\begin{equation}\n",
    "v_{\\pi}(s) = max_{a \\in A} q_{\\pi} (s, a)\n",
    "\\end{equation}\n",
    "\n",
    "- 因此$v_{\\pi}(s) = v_{\\ast}(s), \\forall s \\in S$\n",
    "\n",
    "- 所以$\\pi$就是一个最优策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 策略迭代的变体"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 那么我们要问策略迭代真的需要价值函数收敛到$v_{\\pi}$吗？\n",
    "\n",
    "- 或者说我们需要引入停止条件吗\n",
    "\n",
    "    - 比如价值函数的$\\epsilon$-收敛\n",
    "\n",
    "- 或者说我们只是简单地在$k$次策略迭代评估后停止？\n",
    "\n",
    "- 比如说在小型Gridworld中，当$k=3$时就已经足够我们取得最优策略了\n",
    "\n",
    "- 那我们为什么不在每次策略评估后更新策略呢？也就是说我们在$k=1$时就停止\n",
    "\n",
    "    - 这和价值迭代（value iteration）是等价的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 广义策略迭代（Generalised Policy Iteration）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "策略评估： 用任何策略评估算法来估计$v_{\\pi}$\n",
    "\n",
    "策略改进： 用任何策略改进算法来生成$\\pi' \\geq \\pi$\n",
    "\n",
    "<img src=\"files/figures/generalised_policy_iteration_1.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "<img src=\"files/figures/generalised_policy_iteration_2.png\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最优原则（Principle of Optimality）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任何的最优策略都可以被分解成两个元素：\n",
    "\n",
    "- 一个最优动作$A_{\\ast}$\n",
    "\n",
    "- 遵循基于后继状态$S'$的一个最优策略\n",
    "\n",
    "我们说策略$\\pi(a|s)$在状态$s$上取得了最优价值$v_{\\pi}(s) = v_{\\ast}(s)$当且仅当\n",
    "\n",
    "- 对于任何从$s$可及的状态$s'$来说\n",
    "\n",
    "- 如果$v_{\\pi}(s')=v_{\\ast}(s')$，我们可以说$\\pi$取得了最优价值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 确定性的价值迭代"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如果我们知道了子问题的解$v_{\\ast}(s')$\n",
    "\n",
    "- 在这样的情况下$v_{\\ast}(s)$的解就可以通过前瞻一步（one-step lookahead）来找到\n",
    "\n",
    "\\begin{equation}\n",
    "v_{\\ast}(s) \\leftarrow max_{a \\in A} R_s^a + \\gamma \\sum_{s' \\in S} P_{s s'}^a v_{\\ast}(s')\n",
    "\\end{equation}\n",
    "\n",
    "- 价值迭代的思想就是迭代更新\n",
    "\n",
    "- 直观的解释就是我们从最终的奖励开始反向推导\n",
    "\n",
    "- 我们仍然需要使用循环随机的MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最短路径"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/shortest_path.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 价值迭代（Value Iteration）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 问题：寻找最优策略$\\pi$\n",
    "\n",
    "- 解：迭代应用贝尔曼最优备份\n",
    "\n",
    "- $v_1 \\rightarrow v_2 \\rightarrow \\ldots \\rightarrow v_{\\ast}$\n",
    "\n",
    "- 使用同步备份（synchronous backups）\n",
    "\n",
    "    - 在每次$k+1$迭代\n",
    "    \n",
    "    - 对所有状态$s \\in S$\n",
    "    \n",
    "    - 根据$v_k(s')$来更新$v_{k+1}(s)$\n",
    "\n",
    "- 向$v_{\\ast}$收敛的证明会在后面给出\n",
    "\n",
    "- 不同于策略迭代，价值迭代没有明确的策略\n",
    "\n",
    "- 价值函数的中间结果也许并不对应任何策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/value_iteration.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "\\begin{equation}\n",
    "v_{k+1}(s) = max_{a \\in A} (R_s^a + \\gamma \\sum_{s' \\in S} P_{s s'}^a v_k(s')) \\\\\n",
    "v_{k+1} = max_{a \\in A} R^a + \\gamma P^a v_k\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 同步动态规划算法（Synchronous Dynamic Programming Algorithms）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 问题 | 贝尔曼方程 | 算法         \n",
    "| :---: | :---: | :---:\n",
    "| 预测 | 贝尔曼期望方程 | 迭代策略评估\n",
    "| 控制 | 贝尔曼期望方程 + 贪心策略改进 | 策略迭代\n",
    "| 控制 | 贝尔曼最优方程 | 价值迭代"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 算法是基于状态价值函数$v_{\\pi}(s)$或$v_{\\ast}(s)$\n",
    "\n",
    "- 在$m$个动作和$n$个状态的情况下，每次迭代的计算复杂度是$O(m n^2)$\n",
    "\n",
    "- 可以被用到动作价值函数$q_{\\pi}(s, a)$和$q_{\\ast}(s, a)$上\n",
    "\n",
    "- 每次迭代的计算复杂度是$O(m^2 n^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 异步动态规划（Asynchronous Dynamic Programming）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 到现在为止用到的动态规划方法都用了同步备份（synchronous backups）\n",
    "\n",
    "- 也就是所有的状态备份都是平行的\n",
    "\n",
    "- 异步动态规划在备份状态时，每个状态都是独立进行并且顺序是不定的\n",
    "\n",
    "- 对每个选中的状态，我们都加以合适的备份\n",
    "\n",
    "- 这样可以显著地减少计算量\n",
    "\n",
    "- 如果所有状态都持续被选中就一定会收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "异步动态规划中三个简单的思想：\n",
    "\n",
    "- 原位动态规划（in-place dynamic programming）\n",
    "\n",
    "- 优先扫描（prioritised sweeping）\n",
    "\n",
    "- 实时动态规划（real-time dynamic programming）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 原位动态规划（In-Place Dynamic Programming）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同步价值迭代储存了价值函数的两份备份\n",
    "\n",
    "\\begin{equation}\n",
    "for \\ \\ all \\ \\ s \\ \\ in \\ \\ S \\\\\n",
    "v_{new}(s) \\leftarrow max_{a \\in A} (R_s^a + \\gamma \\sum_{s' \\in S} P_{s s'}^a v_{old}(s')) \\\\\n",
    "v_{old} \\leftarrow v_{new}\n",
    "\\end{equation}\n",
    "\n",
    "- 原位价值迭代只储存价值函数的一份备份\n",
    "\n",
    "\\begin{equation}\n",
    "for \\ \\ all \\ \\ s \\ \\ in \\ \\ S \\\\\n",
    "v(s) \\leftarrow max_{a \\in A}(R_s^a + \\gamma \\sum_{s' \\in S} P_{s s'}^a v(s'))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优先扫描（Prioritised Sweeping）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用贝尔曼误差的大小引导状态选择，也就是\n",
    "\n",
    "\\begin{equation}\n",
    "\\lvert max_{a \\in A}(R_s^a + \\gamma \\sum_{s' \\in S} P_{s s'}^a v(s')) - v(s) \\rvert\n",
    "\\end{equation}\n",
    "\n",
    "- 用最大的剩余贝尔曼误差来备份状态\n",
    "\n",
    "- 在每次备份后用贝尔曼误差来更新相关状态\n",
    "\n",
    "- 需要反向动力学的知识（之前的状态）\n",
    "\n",
    "- 可以用一个优先级序列来提高实施效率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实时动态规划（Real-Time Dynamic Programming）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 思想： 只更新与智能体有关的状态\n",
    "\n",
    "- 用智能体的经验来引导状态的选择\n",
    "\n",
    "- 在每个步长后$S_t, A_t, R_{t+1}$\n",
    "\n",
    "- 备份状态$S_t$\n",
    "\n",
    "\\begin{equation}\n",
    "v(S_t) \\leftarrow max_{a \\in A}(R_{S_t}^a + \\gamma \\sum_{s' \\in S} P_{S_t s'}^a v(s'))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全宽备份（Full-Width Backups）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/full-width_backups.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "- 动态规划使用的是全宽备份（full-width backups）\n",
    "\n",
    "- 对每次备份来说（同步或异步）\n",
    "\n",
    "    - 每一个后继状态和动作都被考虑在内\n",
    "    \n",
    "    - 使用了MDP状态转移和奖励函数的知识\n",
    "\n",
    "- 动态规划对中等规模的问题是有效的（百万级状态）\n",
    "\n",
    "- 对于大规模的问题，动态规划受到贝尔曼维度诅咒（Bellman's curse of dimensionality）的影响\n",
    "\n",
    "    - 状态数$n=\\lvert S \\rvert$根据状态变量数呈指数型增长\n",
    "\n",
    "- 就算是一次备份也会非常昂贵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 样本备份（Sample Backups）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/sample_backups.png\" style=\"width: 100px;\"/>\n",
    "\n",
    "- 使用样本奖励和样本状态转移$<S,A,R,S'>$\n",
    "\n",
    "- 而不是奖励函数$R$和状态转移动力方程$P$\n",
    "\n",
    "- 优势：\n",
    "\n",
    "    - 无模型（model-free）：不需要MDP的知识\n",
    "    \n",
    "    - 通过取样来克服维度诅咒（the curse of dimensionality）\n",
    "    \n",
    "    - 备份的代价是独立于$n=\\lvert S \\rvert$的常量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 近似动态规划（Approximate Dynamic Programming）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 近似价值函数\n",
    "\n",
    "- 用一个函数逼近器（function approximator）$\\hat{v}(s, \\textbf{w})$\n",
    "\n",
    "- 在$\\hat{v}(\\cdot, \\textbf{w})$上应用动态规划\n",
    "\n",
    "- 例如在每次迭代$k$都进行拟合值迭代（Fitted Value Iteration），\n",
    "\n",
    "    - 样本状态$\\tilde{S} \\subseteq S$\n",
    "    \n",
    "    - 对每个状态$s \\in \\tilde{S}$我们都用贝尔曼最优方程来估计目标价值，\n",
    "    \n",
    "    \\begin{equation}\n",
    "    \\tilde{v}_{k}(s) = max_{a \\in A} (R_s^a + \\gamma \\sum_{s' \\in S} P_{s s'}^a \\hat{v}(s', \\textbf{w}_k))\n",
    "    \\end{equation}\n",
    "\n",
    "    - 用目标$\\{<s, \\tilde{v}_k(s)>\\}$来训练下一个价值函数$\\hat{v}(\\cdot, \\textbf{w}_{k+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一些技术问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们怎样得知价值迭代收敛到了$v_{\\ast}$？\n",
    "\n",
    "- 或者说迭代策略评估收敛到了$v_{\\pi}$？\n",
    "\n",
    "- 还有就是策略迭代收敛到了$v_{\\ast}$？\n",
    "\n",
    "- 这个解是唯一的吗？\n",
    "\n",
    "- 那这些算法的收敛速度又怎样呢？\n",
    "\n",
    "- 这些问题都能被收缩映射理论（contraction mapping theorem）解决"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 价值函数空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 考虑价值函数的向量空间$\\mathcal{V}$\n",
    "\n",
    "- 这里有$\\lvert \\mathcal{S} \\rvert$个维度\n",
    "\n",
    "- 这个空间中的每个点都完全描述了一个价值函数$v(s)$\n",
    "\n",
    "- 那么贝尔曼备份对空间中的这些点做了什么呢？\n",
    "\n",
    "- 我们将会展示这使得价值函数更接近了\n",
    "\n",
    "- 因此备份必须收敛到一个唯一解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 价值函数$\\infty$-范数（$\\infty$-Norm）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们将会用$\\infty$-范数来测量状态价值函数$u$和$v$之间的距离\n",
    "\n",
    "- 也就是状态价值间的最大差\n",
    "\n",
    "\\begin{equation}\n",
    "\\left\\| u - v \\right\\|_{\\infty} = max_{s \\in S} \\lvert u(s) - v(s) \\rvert\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 贝尔曼期望备份是一个收缩（Bellman Expectation Backup is a Contraction）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义贝尔曼期望备份算子（Bellman expectation backup operator）$T^{\\pi}$，\n",
    "\n",
    "\\begin{equation}\n",
    "T^{\\pi}(v) = \\mathcal{R}^{\\pi} + \\gamma \\mathcal{P}^{\\pi} v\n",
    "\\end{equation}\n",
    "\n",
    "- 这个算子是一个$\\gamma$-收缩，也就是说它使得价值函数至少接近了$\\gamma$，\n",
    "\n",
    "\\begin{align}\n",
    "\\left\\| T^{\\pi}(u) - T^{\\pi}(v) \\right\\|_{\\infty} & = \\left\\| (\\mathcal{R}^{\\pi}+\\gamma \\mathcal{P}^{\\pi}u) - (\\mathcal{R}^{\\pi}+\\gamma \\mathcal{P}^{\\pi} v) \\right\\|_{\\infty} \\\\\n",
    "& = \\left\\| \\gamma \\mathcal{P}^{\\pi}(u-v) \\right\\|_{\\infty} \\\\\n",
    "& \\leq \\left\\| \\gamma \\mathcal{P}^{\\pi} \\left\\| u - v \\right\\|_{\\infty} \\right\\|_{\\infty} \\\\\n",
    "& \\leq \\gamma \\left\\| u - v \\right\\|_{\\infty}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 收缩映射理论（Contraction Mapping Theorem）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对任何在一个算子$T(v)$下完整的（也就是闭合的）度量空间$\\mathcal{V}$，在这里$T$是一个$\\gamma$-收缩\n",
    "\n",
    "- $T$收敛到唯一固定点\n",
    "\n",
    "- 以一个线性的收敛速率（linear convergence rate）$\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 策略评估和策略迭代的迭代收敛（Convergence of Iter. Policy Evaluation and Policy Iteration）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 贝尔曼期望算子$T^{\\pi}$有唯一固定点\n",
    "\n",
    "- $v_{\\pi}$是$T^{\\pi}$（用贝尔曼期望方程得出）的一个固定点\n",
    "\n",
    "- 通过收缩映射理论（contraction mapping theorem）\n",
    "\n",
    "- 策略迭代评估收敛到$v_{\\pi}$\n",
    "\n",
    "- 策略迭代收敛到$v_{\\ast}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 贝尔曼最优备份是一个收缩（Bellman Optimality Backup is a Contraction）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们定义贝尔曼最优备份算子（Bellman optimality backup operator）$T^{\\ast}$为,\n",
    "\n",
    "\\begin{equation}\n",
    "T^{\\ast}(v) = max_{a \\in A} \\mathcal{R}^a + \\gamma \\mathcal{P}^a v\n",
    "\\end{equation}\n",
    "\n",
    "- 这个算子是一个$\\gamma$-收缩，也就是说它使得价值函数至少接近了$\\gamma$（证明与之前相似）\n",
    "\n",
    "\\begin{equation}\n",
    "\\left\\| T^{\\ast}(u) - T^{\\ast}(v) \\right\\|_{\\infty} \\leq \\gamma \\left\\| u - v \\right\\|_{\\infty}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 价值迭代的收敛（Convergence of Value Iteration）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 贝尔曼最优算子$T^{\\ast}$有唯一固定点\n",
    "\n",
    "- $v_{\\ast}$是$T^{\\ast}$（用贝尔曼最优方程）的一个固定点\n",
    "\n",
    "- 以收缩映射理论（contraction mapping theorem）\n",
    "\n",
    "- 价值迭代收敛到$v_{\\ast}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初次编辑时间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年4月18日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/dyna.pdf\n",
    "\n",
    "[2] https://www.youtube.com/watch?v=Nd1-UUMVfz4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

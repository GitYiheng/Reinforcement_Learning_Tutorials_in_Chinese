{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 马尔科夫决策过程（Markov Decision Process）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "马尔科夫决策过程描述了强化学习中的环境。在这里我们认为环境是完全可观测的（fully observable），也就是说当前时刻的状态（state）可以完全定义整个过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 马尔科夫性质（Markov Property）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The future is independent of the past given the present.\"\n",
    "\n",
    "“给定现在的状态，未来的状态独立于过去的状态。”\n",
    "\n",
    "当且仅当\n",
    "\n",
    "$P[S_{t+1} | S_{t}] = P[S_{t+1} | S_{1},\\ldots,S_{t}]$\n",
    "\n",
    "时，\n",
    "\n",
    "我们说这个状态$S_t$是具备马尔科夫性质的。\n",
    "\n",
    "- 这个状态应该包含了过去的所有相关信息\n",
    "- 当这个状态是已知的时候，我们可以舍弃之前的信息\n",
    "- 这个状态为决策提供了足够的统计信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 状态转移矩阵（State Transition Matrix）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对一个马尔科夫状态$s$和下一帧的状态$s'$来说，我们把状态转移概率定义为\n",
    "\n",
    "$P_{s s'} = P[S_{t+1} = s' | S_{t} = s]$\n",
    "\n",
    "状态转移矩阵$P$定义了所有状态$s$到下一帧状态$s'$的转移概率，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "P = \n",
    "\\begin{bmatrix}\n",
    "P_{11} & \\cdots & P_{1n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "P_{n1} & \\cdots & P_{nn}\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状态转移矩阵$P$中每行概率之和为1。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 马尔科夫过程（Markov Process）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个马尔科夫过程是一个无记忆的随机过程，也就是一序列具有马尔科夫性质的随机状态$S_{1},S_{2},\\ldots$\n",
    "\n",
    "马尔科夫过程（或者叫作马尔科夫链，Markov Chain）是一个元组$<S,P>$\n",
    "\n",
    "- $S$是状态的一个（有限）集合\n",
    "\n",
    "- $P$是一个状态转移概率矩阵，其中$P_{s s'} = P[S_{t+1} = s' | S_t = s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 马尔科夫奖励过程（Markov Reward Process）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个马尔科夫奖励过程（MRP）是一个伴有价值反馈的马尔科夫链。\n",
    "\n",
    "一个马尔科夫奖励过程是一个元组$<S,P,R,\\gamma>$\n",
    "\n",
    "- $S$是状态的一个有限集合\n",
    "\n",
    "- $P$是一个状态转移概率矩阵，$P_{s s'} = P[S_{t+1} = s' | S_{t} = s]$\n",
    "\n",
    "- $R$是一个奖励函数（reward function），$R_{s} = E[R_{t+1} | S_{t} = s]$\n",
    "\n",
    "- $\\gamma$是取值范围$\\gamma \\in [0, 1]$的一个折扣系数（discount factor）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 回报（Return）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回报$G_{t}$的定义是$t$时刻之后的折扣奖励之和，\n",
    "\n",
    "$G_{t} = R_{t+1} + \\gamma R_{t+2} + \\ldots = \\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1}$\n",
    "\n",
    "- 折扣系数$\\gamma \\in [0, 1]$是未来回报的当前价值\n",
    "\n",
    "- 在$k+1$帧后接收到奖励$R$的价值是$\\gamma^{k}R$\n",
    "\n",
    "- 即时奖励的价值是高于延迟奖励的\n",
    "    - 接近于0的$\\gamma$值会使评估更加“目光短浅”\n",
    "    - 接近于1的$\\gamma$值会使评估更加“目光长远”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 为什么要使用折扣系数呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大多数的马尔科夫回报和决策过程都使用了折扣系数。那其中的原因是什么呢？\n",
    "\n",
    "- 折扣奖励在数学性质上的便利性\n",
    "\n",
    "- 可以避免在循环马尔科夫过程的无限回报\n",
    "\n",
    "- 未来的不确定性可能并没有充分体现\n",
    "\n",
    "- 在奖励是金融方面的，即时奖励较延迟奖励来说可能享有更高的利息\n",
    "\n",
    "- 人和动物的行为显示了对即时奖励的倾向性\n",
    "\n",
    "- 在确定所有过程都会终止的前提下，我们也可以用无折扣的马尔科夫奖励过程（$\\gamma=1$，undiscounted Markov reward processes）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 价值函数（Value Function）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "价值函数$v(s)$给出了状态$s$的长期价值\n",
    "\n",
    "一个MRP的状态价值函数（state value function）$v(s)$是从状态$s$开始的期望回报\n",
    "\n",
    "$v(s) = E[G_{t} | S_{t} = s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRPs的贝尔曼方程（Bellman Equation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "价值函数可以被拆分成两部分:\n",
    "\n",
    "- 即时奖励$R_{t+1}$\n",
    "\n",
    "- 后继状态的折扣价值$\\gamma v(S_{t+1})$\n",
    "\n",
    "\\begin{split}\n",
    "v(s) & = E[G_t | S_t = s] \\\\\n",
    "& = E[R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+\\ldots | S_{t} = s] \\\\\n",
    "& = E[R_{t+1}+\\gamma(R_{t+2}+\\gamma R_{t+3}+\\ldots) | S_{t} = s] \\\\\n",
    "& = E[R_{t+1}+\\gamma G_{t+1} | S_{t} = s] \\\\\n",
    "& = E[R_{t+1}+\\gamma v(S_{t+1}) | S_{t} = s]\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$v(s) = E[R_{t+1}+\\gamma v(S_{t+1}) | S_t = s]$\n",
    "\n",
    "<img src=\"files/figures/Bellman_equation_for_MRPs_1.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "$v(s) = R_{s} + \\gamma \\sum_{s' \\in S} P_{s s'}v(s')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 矩阵形式的贝尔曼方程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们有一个列向量$v$，每个对应的状态都有一个输入，那么贝尔曼方程可以用矩阵形式被简明地被表示为\n",
    "\n",
    "$v = R + \\gamma P v$\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "v(1) \\\\\n",
    "\\vdots \\\\\n",
    "v(n)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "R_1 \\\\\n",
    "\\vdots \\\\\n",
    "R_n\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\gamma\n",
    "\\begin{bmatrix}\n",
    "P_{11} & \\cdots & P_{1n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "P_{n1} & \\cdots & P_{nn}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "v(1) \\\\\n",
    "\\vdots \\\\\n",
    "v(n)\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 解贝尔曼方程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 贝尔曼方程是一个线性方程\n",
    "\n",
    "- 它可以被直接解出来：\n",
    "\n",
    "\\begin{split}\n",
    "v = R + \\gamma P v \\\\\n",
    "(I - \\gamma P) v = R \\\\\n",
    "v = (I - \\gamma P)^{-1} R\n",
    "\\end{split}\n",
    "\n",
    "- $n$个状态的计算复杂度是$O(n^3)$\n",
    "\n",
    "- 直接解只在较小规模的MRPs中可能存在\n",
    "\n",
    "- 对于规模较大的MRPs，我们有一些迭代的方法：\n",
    "    - 动态规划（Dynamic programming）\n",
    "    - 蒙特卡洛评估（Monte-Carlo evaluation）\n",
    "    - 时差学习法（Temporal-Difference learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf\n",
    "\n",
    "[2] https://www.youtube.com/watch?v=lfHX2hHRMVQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

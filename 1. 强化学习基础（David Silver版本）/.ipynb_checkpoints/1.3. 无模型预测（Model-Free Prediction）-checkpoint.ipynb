{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 无模型强化学习（Model-Free Reinforcement Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用动态规划来进行规划\n",
    "    - 解一个已知的MDP\n",
    "\n",
    "- 无模型预测（model-free prediction）\n",
    "    - 估计一个未知MDP的价值函数\n",
    "\n",
    "- 无模型控制（model-free control）\n",
    "    - 优化一个未知MDP的价值函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蒙特卡洛强化学习（Monte-Carlo Reinforcement Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MC方法直接从经验集（episodes of experience）上学习\n",
    "\n",
    "- MC是无模型的（model-free）：没有MDP状态转移和奖励的知识\n",
    "\n",
    "- MC从完整的回合（complete episodes）中学习：没有用到自举（bootstrapping）\n",
    "\n",
    "- MC用了最简单的思想：价值等于平均回报（value=mean return）\n",
    "\n",
    "- 警告：MC只能用在回合制的MDP上（episodic MDPs）\n",
    "    - 所有的回合必须终止"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蒙特卡洛策略评估（Monte-Carlo Policy Evaluation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 目标：从使用策略$\\pi$的回合经验中学习$v_{\\pi}$\n",
    "\n",
    "\\begin{equation}\n",
    "S_1, A_1, R_2, \\ldots, S_k \\sim \\pi\n",
    "\\end{equation}\n",
    "\n",
    "- 回报（return）是总折扣奖励（total discounted reward）：\n",
    "\n",
    "\\begin{equation}\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\ldots + \\gamma^{T-1} R_T\n",
    "\\end{equation}\n",
    "\n",
    "- 价值函数（value function）就是期望回报（expected return）：\n",
    "\n",
    "\\begin{equation}\n",
    "v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t | S_t = s]\n",
    "\\end{equation}\n",
    "\n",
    "- 蒙特卡洛策略评估使用了经验平均回报（empirical mean return）而不是期望回报（expected return）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 首访蒙特卡洛策略评估（First-Visit Monte-Carlo Policy Evaluation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这是用来评估状态$s$\n",
    "\n",
    "- 状态$s$在一个回合中被访问的首个时间步长（first time-step）$t$\n",
    "\n",
    "- 增量计数器（increment counter） $N(s) \\leftarrow N(s)+1$\n",
    "\n",
    "- 增量总回报（increment total return） $S(s) \\leftarrow S(s) + G_t$\n",
    "\n",
    "- 用平均回报来估计价值 $V(s) = S(s)/N(s)$\n",
    "\n",
    "- 根据大数定理（law of large numbers），在$N(s) \\rightarrow \\infty$时$V(s) \\rightarrow v_{\\pi}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 每访蒙特卡洛策略评估（Every-Visit Monte-Carlo Policy Evaluation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这是用来评估状态$s$\n",
    "\n",
    "- 状态$s$在一个回合中每个被访问的步长（every time-step）$t$\n",
    "\n",
    "- 增量计数器（increment counter） $N(s) \\leftarrow N(s)+1$\n",
    "\n",
    "- 增量总回报（increment total return） $S(s) \\leftarrow S(s) + G_t$\n",
    "\n",
    "- 用平均回报来估计价值 $V(s) = S(s)/N(s)$\n",
    "\n",
    "- 同样的，在$N(s) \\rightarrow \\infty$时$V(s) \\rightarrow v_{\\pi}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 范例-二十一点（Blackjack Example）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 状态（200个）\n",
    "    - 当前总和（12-21）\n",
    "    - 庄家的明牌（A-10）\n",
    "    - 我有没有一张“可用的”A（yes-no）\n",
    "- 停牌（stick）：停止要牌（结束）\n",
    "- 要牌（twist）：再加一张牌（不换牌）\n",
    "- 停牌的奖励：\n",
    "    - 如果玩家的牌面点数之和$>$庄家的牌面点数之和，奖励为+1\n",
    "    - 如果玩家的牌面点数之和$=$庄家的牌面点数之和，奖励为0\n",
    "    - 如果玩家的牌面点数之和$<$庄家的牌面点数之和，奖励为-1\n",
    "- 要牌的奖励：\n",
    "    - 如果玩家的牌面点数之和$>$21（结束），奖励为-1\n",
    "    - 其它情况下，奖励为0\n",
    "- 状态转移：如果牌面点数之和$<$12，自动要牌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21点在蒙特卡洛学习后的价值函数（Blackjack Value Function after Monte-Carlo Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "策略：如果牌面点数之和$\\geq$20的话选择停牌，否则选择要牌\n",
    "\n",
    "<img src=\"files/figures/blackjack.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 平均增量（Incremental Mean）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "序列$x_1, x_2, \\ldots$的平均值$\\mu_1, \\mu_2, \\ldots$可以被逐步计算，\n",
    "\n",
    "\\begin{align}\n",
    "\\mu_k & = \\frac{1}{k}\\sum_{j=1}^k x_j \\\\\n",
    "& = \\frac{1}{k}(x_k + \\sum_{j=1}^{k-1} x_j) \\\\\n",
    "& = \\frac{1}{k}(x_k + (k-1)\\mu_{k-1}) \\\\\n",
    "& = \\mu_{k-1} + \\frac{1}{k}(x_k - \\mu_{k-1})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蒙特卡洛逐步更新（Incremental Monte-Carlo Updates）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在每个回合$S_1, A_1, R_2, \\ldots, S_T$后逐步更新$V(s)$\n",
    "\n",
    "- 对每个回报为$G_t$的状态$S_t$\n",
    "\n",
    "\\begin{align}\n",
    "& N(S_t) \\leftarrow N(S_t)+1 \\\\\n",
    "& V(S_t) \\leftarrow V(S_t) + \\frac{1}{N(S_t)}(G_t - V(S_t))\n",
    "\\end{align}\n",
    "\n",
    "- 在非静态问题上（non-stationary problems），跟踪移动平均值（running mean）是非常有用的，也就是忘掉旧的回合（old episodes）\n",
    "\n",
    "\\begin{equation}\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha (G_t - V(S_t))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 时差学习（Temporal-Difference Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TD方法直接从经验集（episodes of experience）上学习\n",
    "- TD是无模型的（model-free）：没有MDP状态转移和奖励的知识\n",
    "- TD用自举（bootstrapping）从不完整的回合（incomplete episodes）中学习\n",
    "- TD从一个估计值向一个估计值的方向上更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蒙特卡洛和时差法（MC and TD）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 目标：在用策略$\\pi$的前提下从经验中在线（online）学习$v_{\\pi}$\n",
    "\n",
    "- 增量每访蒙特卡洛（incremental every-visit Monte-Carlo）\n",
    "    - 在价值$V(S_t)$的基础上朝实际回报（actual return）$G_t$的方向更新\n",
    "\n",
    "\\begin{equation}\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha (G_t - V(S_t))\n",
    "\\end{equation}\n",
    "\n",
    "- 最简单的时差学习算法：TD(0)\n",
    "    - 在价值$V(S_t)$的基础上朝回报估计值（estimated return）的方向更新$R_{t+1} + \\gamma V(S_{t+1})$\n",
    "\n",
    "    \\begin{equation}\n",
    "    V(S_t) \\leftarrow V(S_t) + \\alpha (R_{t+1} + \\gamma V(S_{t+1}) - V(S_t))\n",
    "    \\end{equation}\n",
    "    \n",
    "    - $R_{t+1} + \\gamma V(S_{t+1})$被叫作TD目标（TD target）\n",
    "    - $\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$被叫作TD误差（TD error）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 范例-开车回家（Driving Home Example）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/driving_home_1.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/driving_home_2.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蒙特卡洛和时差法的优劣（1）（Advantages and Disadvantages of MC vs TD）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TD可以在知道最终结果前就学习\n",
    "    - TD可以在每一步后在线学习\n",
    "    - MC必须等到回合结束也就是回报是已知的时候\n",
    "- TD可以在没有最终结果的时候进行学习\n",
    "    - TD可以从未完成的序列中学习\n",
    "    - MC只能从完成的序列中学习\n",
    "    - TD在连续的（无终止的）环境中也可以使用\n",
    "    - MC只能在回合制的（会终止的）环境中使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在偏差和方差中的权衡（Bias/Variance Trade-Off）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 回报$G_t = R_{t+1} + \\gamma R_{t+2} + \\ldots + \\gamma^{T-1} R_T$是$v_{\\pi}(S_t)$的无偏估计（unbiased estimate）\n",
    "- 真实的TD目标$R_{t+1}+\\gamma v_{\\pi}(S_{t+1})$是$v_{\\pi}(S_t)$的无偏估计（unbiased estimate）\n",
    "- TD目标$R_{t+1}+\\gamma V(S_{t+1})$是$v_{\\pi}(S_t)$的有偏估计（biased estimate）\n",
    "- 与回报相比，TD目标的方差要小得多：\n",
    "    - 回报取决于许多随机的动作、状态转移和奖励\n",
    "    - TD目标取决于一个随机的动作、状态转移和奖励"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蒙特卡洛和时差法的优劣（2）（Advantages and Disadvantages of MC vs TD）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MC的特点是高方差和零偏差（high variance, zero bias）\n",
    "    - 有非常好的收敛特性\n",
    "    - （就算用函数逼近也有非常好的收敛特性）\n",
    "    - 对初始价值不是非常敏感\n",
    "    - 理解和使用起来非常简单\n",
    "- TD的特点是低方差和有一些偏差（low variance, some bias）\n",
    "    - 通常比MC要高效许多\n",
    "    - TD(0)收敛到$v_{\\pi}(s)$\n",
    "    - （但在使用函数逼近时不总是收敛）\n",
    "    - 对初始值更加敏感"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 范例-随机行走（Random Walk Example）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/random_walk_1.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/random_walk_2.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 批量蒙特卡洛和时差法（Batch MC and TD）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MC和TD的收敛情况：当经验$\\rightarrow \\infty$时$V(s) \\rightarrow v_{\\pi}(s)$\n",
    "- 但有限经验的批量解的情况是怎么样的呢？\n",
    "\\begin{equation}\n",
    "s_1^1, a_1^1, r_2^1, \\ldots, s_{T_1}^1 \\\\\n",
    "\\vdots \\\\\n",
    "s_1^K, a_1^K, r_2^K, \\ldots, s_{T_K}^K\n",
    "\\end{equation}\n",
    "    - 也就是重复样本回合$k \\in [1, K]$\n",
    "    - 在回合$k$上用MC或者TD(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 范例-AB状态（AB Example）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有两个状态A和B；没有折扣（no discounting）；8个回合的经验\n",
    "\n",
    "<img src=\"files/figures/AB.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "那么$V(A)$和$V(B)$是多少呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 确定等值（Certainty Equivalence）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MC收敛到有最小均方误差（minimum mean-squared error）的解\n",
    "    - 与观察到的回报拟合得最好\n",
    "    \\begin{equation}\n",
    "    \\sum_{k=1}^{K}\\sum_{t=1}^{T_k}(G_t^k - V(s_t^k))^2\n",
    "    \\end{equation}\n",
    "    - 在AB状态的范例中，$V(A)=0$\n",
    "- TD(0)收敛到有最大似然的马尔科夫模型的解\n",
    "    - MDP$<\\mathcal{S},\\mathcal{A},\\hat{\\mathcal{P}},\\hat{\\mathcal{R}},\\gamma>$的解与数据拟合得最好\n",
    "    \\begin{equation}\n",
    "    \\hat{\\mathcal{P}}_{s, s'}^a = \\frac{1}{N(s ,a)}\\sum_{k=1}^{K}\\sum_{t=1}^{T_k}\\textbf{1}(s_t^k, a_t^k, s_{t+1}^k = s, a, s') \\\\\n",
    "    \\hat{\\mathcal{R}}_s^a = \\frac{1}{N(s, a)}\\sum_{k=1}^{K}\\sum_{t=1}^{T_k}\\textbf{1}(s_t^k, a_t^k = s, a)r_t^k\n",
    "    \\end{equation}\n",
    "    - 在AB状态的范例中，$V(A)=0.75$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蒙特卡洛和时差法的优劣（3）（Advantages and Disadvantages of MC vs TD）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TD利用了马尔科夫性质\n",
    "    - 通常在马尔科夫环境中效率更高\n",
    "- MC没有利用马尔科夫性质\n",
    "    - 通常在非马尔科夫环境中效率更高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蒙特卡洛备份（Monte-Carlo Backup）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha (G_t - V(S_t))\n",
    "\\end{equation}\n",
    "\n",
    "<img src=\"files/figures/Monte-Carlo_backup.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 时差法备份（Temporal-Difference Backup）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha (R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)))\n",
    "\\end{equation}\n",
    "\n",
    "<img src=\"files/figures/Temporal-Difference_backup.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 动态规划备份（Dynamic Programming Backup）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "V(S_t) \\leftarrow \\mathbb{E}_{\\pi}[R_{t+1}+\\gamma V(S_{t+1})]\n",
    "\\end{equation}\n",
    "\n",
    "<img src=\"files/figures/Dynamic_Programming_backup.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自举和取样（Bootstrapping and Sampling）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 自举（bootstrapping）：更新用到一个估计值\n",
    "    - MC没有用到自举\n",
    "    - DP用到了自举\n",
    "    - TD用到了自举\n",
    "- 取样（sampling）：更新用到了样本的期望值\n",
    "    - MC用到了样本\n",
    "    - DP没有用到样本\n",
    "    - TD用到了样本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 纵观强化学习（Unified View of Reinforcement Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/unified_view_of_RL.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $n$-步预测（$n$-Step Prediction）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 让TD目标向未来看$n$步\n",
    "\n",
    "<img src=\"files/figures/n-step_prediction.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $n$-步回报（$n$-Step Return）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 让我们来考虑在$n=1,2,\\ldots,\\infty$时的$n$-步回报：\n",
    "\\begin{align}\n",
    "& n=1 & (TD) \\quad & G_t^{(1)}=R_{t+1}+\\gamma V(S_{t+1}) \\\\\n",
    "& n=2 & & G_t^{(2)}=R_{t+1}+\\gamma R_{t+2} + \\gamma^2 V(S_{t+2}) \\\\\n",
    "& \\quad \\vdots & & \\vdots \\\\\n",
    "& n=\\infty & (MD) \\quad & G_t^{(\\infty)}=R_{t+1}+\\gamma R_{t+2} + \\ldots + \\gamma^{T-1}R_T\n",
    "\\end{align}\n",
    "\n",
    "- 我们把$n$-步回报定义为\n",
    "\\begin{equation}\n",
    "G_t^{(n)}=R_{t+1}+\\gamma R_{t+2}+\\ldots+\\gamma^{n-1}R_{t+n}+\\gamma^n V(S_{t+n})\n",
    "\\end{equation}\n",
    "\n",
    "- $n$-步时差学习\n",
    "\\begin{equation}\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha (G_t^{(n)} - V(S_t))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 范例-大型随机行走（Large Random Walk Example）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/large_random_walk_example.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $n$-步平均回报"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们可以在$n$取不同值时计算这些$n$-步回报的平均值\n",
    "- 比如说我们可以计算$2$-步和$4$-步的平均回报\n",
    "\\begin{equation}\n",
    "\\frac{1}{2}G^{(2)}+\\frac{1}{2}G^{(4)}\n",
    "\\end{equation}\n",
    "\n",
    "<img src=\"files/figures/average_n-step_returns.png\" style=\"width: 100px;\"/>\n",
    "\n",
    "- 我们可以结合两个不同时间步长的信息\n",
    "- 那么我们可不可以高效地结合所有时间步长的信息呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\lambda$-回报（$\\lambda$-Return）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/lambda-return.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "- $\\lambda$-回报$G_t^{\\lambda}$结合了所有$n$-步的回报$G_t^{(n)}$\n",
    "- 用权重$(1-\\lambda)\\lambda^{n-1}$\n",
    "\\begin{equation}\n",
    "G_t^{\\lambda} = (1 - \\lambda) \\sum_{n=1}^{\\infty}\\lambda^{n-1} G_t^{(n)}\n",
    "\\end{equation}\n",
    "- 前视（forward-view）$TD(\\lambda)$\n",
    "\\begin{equation}\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha (G_t^{\\lambda} - V(S_t))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD($\\lambda$)权重函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/td_lambda_weighting_function.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "\\begin{equation}\n",
    "G_t^{\\lambda} = (1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1}G_t^{(n)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前视TD($\\lambda$)（Forward-View TD($\\lambda$)）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/forward-view_td_lambda.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "- 我们向$\\lambda$-回报的方向更新价值函数\n",
    "- 前视通过向未来的方向预测来计算$G_t^{\\lambda}$\n",
    "- 跟MC一样，前视TD($\\lambda$)只能用完整的回合来计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在大型随机行走问题上使用前视TD($\\lambda$)法（Forward-View TD($\\lambda$) on Large Random Walk）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/forward-view_td_lambda_on_large_random_walk.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 后视TD($\\lambda$)（Backward-View TD($\\lambda$)）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 前视法提供了理论\n",
    "- 后视法提供了实现机理（mechanism）\n",
    "- 在线更新（update online）、单步更新（every step）、从不完整的序列信息中更新（from incomplete sequences）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 有效性标记（Eligibility Traces）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/eligibility_traces_1.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "- 功劳分配问题（credit assignment problem）：休克是因为铃铛的声响还是灯泡的闪烁呢？（did bell or light cause shock?）\n",
    "- 频率作为参考（frequency heuristic）：把功劳分配给出现频率最高的状态\n",
    "- 就近性作为参考（recency heuristic）：把功劳分配给最近出现的状态\n",
    "- 有效性标记（eligibility traces）结合了两种参考指标（heuristics）\n",
    "\\begin{align}\n",
    "& E_0 (s) = 0 \\\\\n",
    "& E_t (s) = \\gamma \\lambda E_{t-1}(s) + \\textbf{1}(S_t = s)\n",
    "\\end{align}\n",
    "\n",
    "<img src=\"files/figures/eligibility_traces_2.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 后视TD($\\lambda$)（Backward-View TD($\\lambda$)）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 给每一个状态$s$都做一个有效性标记（eligibility trace）\n",
    "- 对每一个状态$s$的$V(s)$都进行更新\n",
    "- 更新的幅度与TD-误差$\\delta_t$和有效性标记$E_t (s)$成正比\n",
    "\\begin{equation}\n",
    "\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\\\\n",
    "V(s) \\leftarrow V(s) + \\alpha \\delta_t E_t (s)\n",
    "\\end{equation}\n",
    "\n",
    "<img src=\"files/figures/backward-view_td_lambda.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD($\\lambda$)和TD(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 当$\\lambda = 0$时，只有当前的状态会被更新\n",
    "\\begin{align}\n",
    "& E_t (s) = \\textbf{1}(S_t = s) \\\\\n",
    "& V(s) \\leftarrow V(s) + \\alpha \\delta_t E_t (s)\n",
    "\\end{align}\n",
    "- 这和TD(0)更新时等价的\n",
    "\\begin{equation}\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha \\delta_t\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD($\\lambda$)和MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 当$\\lambda = 1$时，整个回合从开始到结束的每一步都会被分配到功劳（credit）\n",
    "- 让我们来考虑一下在回合制的环境（episodic environments）下离线更新（offline update）的情况\n",
    "- 在整个回合的历程中，TD(1)的整体更新是和MC等价的\n",
    "\n",
    "在离线更新的总和上（sum of offline updates），前视和后视TD($\\lambda$)是等价的\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{t=1}^{T}\\alpha \\delta_t E_t (s) = \\sum_{t+1}^{T} \\alpha (G_t^{\\lambda} - V(S_t))\\textbf{1}(S_t = s)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC和TD(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 让我们来考虑一个状态$s$在$k$时刻时被访问的回合\n",
    "- TD(1)的有效性标记从被访问时就开始衰减，\n",
    "\\begin{align}\n",
    "E_t(s) & =\\gamma E_{t-1}(s)+\\textbf{1}(S_t=s) \\\\\n",
    "& =\\left\\{\n",
    "  \\begin{array}{lr}\n",
    "  \\begin{split}\n",
    "    & 0 \\quad & if \\quad t<k\\\\\n",
    "    & \\gamma^{t-k} \\quad & if \\quad t \\geq k \n",
    "  \\end{split}\n",
    "  \\end{array}\n",
    "\\right.\n",
    "\\end{align}\n",
    "\n",
    "- TD(1)在线更新累计误差（accumulate error）\n",
    "\\begin{align}\n",
    "\\sum_{t=1}^{T-1}\\alpha \\delta_t E_t(s) & =\\alpha \\sum_{t=k}^{T-1} \\gamma^{t-k}\\delta_t \\\\ & = \\alpha (G_k - V(S_k))\n",
    "\\end{align}\n",
    "\n",
    "- 在回合结束时我们会累加总误差（accumulates total error）\n",
    "\\begin{equation}\n",
    "\\delta_k+\\gamma\\delta_{k+1}+\\gamma^2\\delta_{k+2}+\\ldots+\\gamma^{T-1-k}\\delta_{T-1}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD(1)的伸缩特性（Telescoping in TD(1)）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当$\\lambda = 1$时，TD的误差之和伸长成了与MC等价的误差，\n",
    "\\begin{align}\n",
    "& \\quad \\delta_t+\\gamma\\delta_{t+1}+\\gamma^2\\delta_{t+2}+\\ldots+\\gamma^{T-1-t}\\delta_{T-1} \\\\\n",
    "& = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\\\\n",
    "& + \\gamma R_{t+2} + \\gamma^2 V(S_{t+2}) - \\gamma V(S_{t+1}) \\\\\n",
    "& + \\gamma^2 R_{t+3} + \\gamma^3V(S_{t+3})-\\gamma^2V(S_{t+2}) \\\\\n",
    "& \\quad \\vdots \\\\\n",
    "& + \\gamma^{T-1-t}R_T + \\gamma^{T-t}V(S_T) - \\gamma^{T-1-t}V(S_{T-1}) \\\\\n",
    "& = R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+\\ldots+\\gamma^{T-1-t}R_T-V(S_t) \\\\\n",
    "& = G_t - V(S_t)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD($\\lambda$)和TD(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TD(1)大致上与每访蒙特卡洛（every-visit Monte Carlo）等价\n",
    "- 误差的累计是在线（online）和逐步的（step-by-step）\n",
    "- 如果价值函数只在回合结束时被离线更新\n",
    "- 那么总更新就和MC是完全等价的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD($\\lambda$)的伸缩特性（Telescoping in TD($\\lambda$)）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对一般的$\\lambda$来说，TD误差也可以伸长到$\\lambda$-误差，$G_t^{\\lambda} - V(S_t)$\n",
    "\\begin{split}\n",
    "& G_t^{\\lambda}-V(S_t) & = -V(S_t) \\quad & + \\quad & (1-\\lambda)\\lambda^0 (R_{t+1}+\\gamma V(S_{t+1})) \\\\\n",
    "& & & + \\quad & (1-\\lambda)\\lambda^1 (R_{t+1}+\\gamma R_{t+2}+\\gamma^2 V(S_{t+2})) \\\\\n",
    "& & & + \\quad & (1-\\lambda)\\lambda^2 (R_{t+1}+\\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 V(S_{t+3})) \\\\\n",
    "& & & + \\quad & \\ldots \\\\\n",
    "& & = -V(S_t) \\quad & + \\quad & (\\gamma\\lambda)^0(R_{t+1}+\\gamma V(S_{t+1})-\\gamma \\lambda V(S_{t+1})) \\\\\n",
    "& & & + \\quad & (\\gamma\\lambda)^1 (R_{t+2}+\\gamma V(S_{t+2})-\\gamma\\lambda V(S_{t+2})) \\\\\n",
    "& & & + \\quad & (\\gamma\\lambda)^2 (R_{t+3} + \\gamma V(S_{t+3}) - \\gamma\\lambda V(S_{t+3})) \\\\\n",
    "& & & + \\quad & \\ldots \\\\\n",
    "& & = & & (\\gamma\\lambda)^0 (R_{t+1}+\\gamma V(S_{t+1})-V(S_{t})) \\\\\n",
    "& & & + & (\\gamma\\lambda)^1 (R_{t+2}+\\gamma V(S_{t+2})-V(S_{t+1})) \\\\\n",
    "& & & + & (\\gamma\\lambda)^2 (R_{t+3}+\\gamma V(S_{t+3})-V(S_{t+2})) \\\\\n",
    "& & & + & \\ldots \\\\\n",
    "& & = \\delta_t + \\gamma \\lambda \\delta_{t+1} + (\\gamma\\lambda)^2 \\delta_{t+2} + \\ldots\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前视和后视TD($\\lambda$)（Forwards and Backwards TD($\\lambda$)）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 让我们来考虑一个状态$s$在$k$时刻被访问的回合\n",
    "- TD($\\lambda$)的有效性标记从访问的时刻就开始衰减了，\n",
    "\\begin{align}\n",
    "& E_t (s) & = \\quad & \\gamma\\lambda E_{t-1} (s) + \\textbf{1} (S_t = s) \\\\\n",
    "& & = \\quad & \\left\\{\\begin{array}{lr}\n",
    "\\begin{split}\n",
    "& 0 \\quad & if \\quad t<k \\\\\n",
    "& (\\gamma\\lambda)^{t-k} \\quad & if \\quad t \\geq k\n",
    "\\end{split}\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{align}\n",
    "\n",
    "- 后视TD($\\lambda$)在线更新累计误差（updates accumulated error online）\n",
    "\\begin{split}\n",
    "& \\sum_{t=1}^{T}\\alpha\\delta_t E_t (s) & = \\alpha \\sum_{t=k}^{T}(\\gamma\\lambda)^{t-k}\\delta_t \\\\\n",
    "& & = \\alpha (G_k^{\\lambda}-V(S_k))\n",
    "\\end{split}\n",
    "\n",
    "- 在回合结束时我们会为$\\lambda$-回报累加总误差\n",
    "- 对于多次访问$s$的情况，$E_t (s)$累积了许多误差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前视和后视TD在离线情况下的等价性（Offline Equivalence of Forward and Backward TD）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "离线更新（Offline updates）\n",
    "\n",
    "- 在回合内更新是累加的\n",
    "- 只不过是在回合结束时用批量的方式更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前视和后视TD在在线情况下的等价性（Online Equivalence of Forward and Backward TD） "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在线更新（Online updates）\n",
    "\n",
    "- TD($\\lambda$)在回合中的每步都会进行在线更新\n",
    "- 前视和后视TD($\\lambda$)在在线更新的情况下是有些许差别的\n",
    "- 完美的在线TD($\\lambda$)所达到的效果是等价的\n",
    "- 通过使用在形式上有些许不同的有效性标记版本\n",
    "- Sutton and von Seijen, ICML 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前视和后视TD($\\lambda$)小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/summary_of_forward_and_backward_td_lambda.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "\"=\"表示的是在回合结束时整体更新的等价性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初次编辑时间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年4月20日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MC-TD.pdf\n",
    "\n",
    "[2] https://www.youtube.com/watch?v=PnHCvfgC_ZA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization is an important tool in decision science and in the analysis of physical systems. To use it, we must first identify some *objective*, a quantitative measure of the performance of the system under study. This objective could be profit, time, potential energy, or any quantity or combination of quantities that can be represented by a single number. The objective depends on certain characteristics of the system, called *variables* or *unknowns*. Our goal is to find values of the variables that optimize the objective. Often the variables are restricted, or *constrained*, in some way. For instance, quantities such as electron density in the molecule and the interest rate on a loan cannot be negative.\n",
    "\n",
    "**The process of identifying objective, variables, and constraints for a given problem is known as *modeling*.** Construction of an appropriate model is the first step - sometimes the most important step - in the optimization process. If the model is too simplistic, it will not give useful insights into the practical problem, but if it is too complex, it may become too difficult to solve.\n",
    "\n",
    "Once the model has been formulated, an optimization algorithm can be used to find its solution. Usually, the algorithm and model are complicated enough that a computer is needed to implement this process. There is no universal optimization algorithm. Rather, there are numerous algorithms, each of which is tailored to a particular type of optimization problem. It is often the user's responsibility to choose an algorithm that is appropriate for their specific application. This choice is an important one; it may determine whether the problem is solved rapidly or slowly and, indeed, whether the solution is found at all.\n",
    "\n",
    "After an optimization algorithm has been applied to the model, we must be able to recognize whether it has succeeded in its task of finding a solution. In many cases, there are elegantt mathematical expressions known as *optimality conditions* for checking that the current set of variables is indeed the solution of the problem. If the optimality conditions are not satisfied, they may give useful information on how the current estimate of the solution can be improved. Finally, the model may be improved by applying techniques such as *sensitivity analysis*, which reveals the sensitivity of the solution to changes in the model and data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically speaking, optimization is the minimization or maximization of function subject to constraints on its variables. We use the following notation:\n",
    "\n",
    "$x$ is the vector of *variables*, also called *unknowns* or *parameters*;\n",
    "\n",
    "$f$ is the *objective function*, a function of $x$ that we want to maximize or minimize;\n",
    "\n",
    "$c$ is the vector of *constraints* that the unknowns must satisfy. This is a vector function of the variables $x$. The number of componenets in $c$ is the number of individual restrictions that we place on the variables.\n",
    "\n",
    "The optimization problem can then be written as\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{x \\in R^n} f(x) \\qquad \\text{subject to } \n",
    "\\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        c_i (x)=0, \\qquad i \\in \\mathcal{E} \\\\\n",
    "        c_i (x) \\geq 0, \\qquad i \\in \\mathcal{I}\n",
    "    \\end{array}\n",
    "\\right.\n",
    "\\end{equation}\n",
    "\n",
    "*feasible region* is the set of points satisfying all the constraints, and the optimal point $x^{\\ast}$, the solution of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous versus Discrete Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*integer programming problem*\n",
    "\n",
    "The generic term *discrete optimization* usually refers to problems in which the solution we seek is one of a number of objects in a finite set. By contrast, *continuous optimization* problems find a solution from an uncountably infinite set - typically a set of vectors with real components.\n",
    "\n",
    "Some models contain variables that are allowed to vary continuously and others that can attain only integer values; we refer to these as *mixed integer programming* problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constrained and Unconstrained Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem with the general form can be classified according to the nature of the objective function and constraints (linear, nonlinear, convex), the number of variables (large or small), the smoothness of the functions (differentiable or nondifferentiable), and so on. Possibly the most important distinction is between problems that have constraints on the variables and those that do not.\n",
    "\n",
    "*Unconstrained optimization*\n",
    "\n",
    "*Constrained optimization*\n",
    "\n",
    "When both the objective function and all the constraints are linear functions of $x$, the problem is a *linear programming* problem.\n",
    "\n",
    "*Nonlinear programming* problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global and Local Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fastest optimization algorithms seek only a local solution, a point at which the objective function is smaller than at all other feasible points in its vicinity. They do not always find the best of all such minima, that is, the *global solution*. Global solutions are necessary (or at least hihgly desirable) in some applications, but they are usually difficult to identify and even more difficult to locate. An important special case is *convex programming*, in which all local solutions are also global solutions. Linear programming problems fall in the category of convex programming. However, general nonlinear problems, both constrained and unconstrained, may possess local solutions that are not global solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic and Deterministic Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stochastic optimization* algorithms use quatifications of the uncertainty to produce solutions that optimize the expected performance of the model.\n",
    "\n",
    "In *deterministic optimization* problems, the model is fully specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization algorithms are iterative. They begin with an initial guess of the optimal values of the variables and generate a sequence of improved estimates until they reach a solution. The strategy used to move from one iterate to the next distinguishes one algorithm from another. Most strategies make use of the values of the objective function $f$, the constraints $c$, and possibly the first and second derivatives of these functions. Some algorithms accumulate information gathered at previous iterations, while others use only local information from the current point. Regardless of these specifics, all good algorithms should possess the following properties:\n",
    "\n",
    "* Robustness. They should perform well on a wide variety of problems in their class, for all reasonable choices of the initial variables.\n",
    "\n",
    "* Efficiency. They should not require too much computer time or storage.\n",
    "\n",
    "* Accuracy. They should be able to identify a solution with precision, without being overly sensitive to errors in the data or to the arithmetic rounding errors that occur when the algorithm is implemented on a computer.\n",
    "\n",
    "These goals may conflict. For example, a rapidly convergent method for nonlinear programming may require too much computer storage on large problems. On the other hand, a robust method may also be the slowest. Tradeoffs between convergence rate and storage requirements, and between robustness and speed, and so on, are central issues in numerical optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of convexity is fundamental in optimization; it implies that the problem is benign in several respects. The term *convex* can be applied both to sets and to functions.\n",
    "\n",
    "$S \\in \\mathbb{R}^n$ is a *convex set* if the straight line segment connecting any two points in $S$ lies entirely inside $S$. Formally, for any two points $x \\in S$ and $y \\in S$, we have $\\alpha x + (1-\\alpha)y \\in S$ for all $\\alpha \\in [0, 1]$.\n",
    "\n",
    "$f$ is a *convex function* if its domain is a convex set and if for any two points $x$ and $y$ in this domain, the graph of $f$ lies below the straight line connecting $(x, f(x))$ to $(y, f(y))$ in the space $\\mathbb{R}^{n+1}$. That is, we have\n",
    "\n",
    "$$f(\\alpha x + (1-\\alpha)y) \\leq \\alpha f(x) + (1-\\alpha) f(y), \\qquad \\text{for all } \\alpha \\in [0, 1]$$\n",
    "\n",
    "When $f$ is smooth as well as convex and the dimension $n$ is 1 or 2, the graph of $f$ is bowl-shaped, and its contours define convex sets. A function $f$ is said to be *concave* if $-f$ is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes and References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization traces its roots to the calculus of variations and the work of Euler and Lagrange. The development of linear programming in the 1940s broadened the field and stimulated much of the progress in modern optimization theory and practice during the last 50 years.\n",
    "\n",
    "Optimization is often called *mathematical programming*, a term that is somewhat confusing because it suggests the writing of computer programs with a mathematical orientation. This term was coined in 1940s, before the word \"programming\" became inextricably linked with computer software. The original meaning of this word was more inclusive, with connotations of problem formulation and algorithm design and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初次编辑日期 (Initial Edit Date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年5月31日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献 (References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Wright, S. and Nocedal, J., 1999. Numerical optimization. Springer Science"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

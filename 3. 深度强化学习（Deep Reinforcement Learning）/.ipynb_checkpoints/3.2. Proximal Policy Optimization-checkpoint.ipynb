{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 使用PPO的动机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "策略梯度方法（policy gradient methods）是最近把深度神经网络应用在控制方面取得突破的基础，应用包括电子游戏、3D运动和围棋。但是因为策略梯度方法对步长（stepsize）的选择非常敏感，所以通过使用策略梯度方法得到好的结果是非常具有挑战性的。如果步长太小，进展就会很慢；如果步长太大信号就会被噪音盖过，或者性能就会经历灾难性的下降。它们的样本效率也常常很低，学习一个简单的任务需要百万帧的信息。\n",
    "\n",
    "研究人员尝试了通过约束或者优化策略更新的步长，使用比如像TRPO和ACER这些方法消除这些缺陷。这些方法都有各自的取舍。ACER（actor-critic with experience replay）的性能在Atari benchmark上只比PPO好一些，但算法比PPO复杂得多，需要额外的代码来做离线更正（off-policy corrections）和一个重播缓存（a replay buffer）。TRPO（trust region policy optimization）尽管在连续控制任务中非常有用，但与共享参数的算法的兼容性并不好，这里是指在那些视觉输入非常重要的任务中策略和价值函数或是辅助损失（auxiliary losses）的参数共享，比如Atari。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PPO的简易性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以用监督学习轻松地实现成本函数并在它上面使用梯度下降，而且我们很有信心在较少调参的前提下会得到很好的结果。强化学习的成功之路并没有那么明显，算法有许多难以调试的动态部分，取得较好的结果这往往需要大量调参方面的努力。PPO在实现的简易程度、样本复杂度和调参简易度上取得了相对的平衡，确保了计算单步更新时在最小化成本函数的同时保证了与之前策略的偏差也较小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 论文理论部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. 背景知识：策略优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. 策略梯度方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "策略梯度方法通过计算策略梯度的估计值并把这个估计值输入到一个随机梯度上升算法中。最常用的梯度估计的形式是这样的\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{g} = \\hat{\\mathbb{E}}_t \\left[ \\nabla_{\\theta} \\log \\pi_{\\theta} (a_t \\mid s_t) \\hat{A}_t \\right]\n",
    "\\end{equation}\n",
    "\n",
    "这里的$\\pi_{\\theta}$是一个随机策略，$\\hat{A}_t$是优势函数在$t$时刻的估计值。在这里，期望$\\hat{\\mathbb{E}}_t \\left[ \\ldots \\right]$表示了在一个交替使用取样和优化算法中的有限批样本的经验平均值。具体实现的时候会使用自动微分软件，这个自动微分软件会构建一个梯度是策略梯度估计值的目标函数，而这个估计值$\\hat{g}$是通过微分这个目标得到的\n",
    "\n",
    "\\begin{equation}\n",
    "L^{PG}(\\theta) = \\hat{\\mathbb{E}}_t \\left[ \\log \\pi_{\\theta} (a_t \\mid s_t) \\hat{A}_t \\right]\n",
    "\\end{equation}\n",
    "\n",
    "虽然使用相同的轨迹来来实现损失$L^{PG}$的多步优化很吸引人，但这样做并不是很合理，实践证明这种做法常常会导致策略更新过于剧烈而且效果并不好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. 信赖区方法（Trust Region Methods）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在TRPO中，最大化目标函数（也就是“代理”目标）会在策略更新幅度的限制下进行。具体就是，\n",
    "\n",
    "\\begin{split}\n",
    "& \\operatorname*{maximize}_{\\theta} \\enspace\\hat{\\mathbb{E}}_t \\left[ \\frac{\\pi_{\\theta} (a_t \\mid s_t)}{\\pi_{\\theta_{old}} (a_t \\mid s_t)} \\hat{A}_t \\right] \\\\\n",
    "& subject \\enspace to \\enspace \\hat{\\mathbb{E}}_t \\left[ KL [ \\pi_{\\theta_{old}} (\\cdot \\mid s_t), \\pi_{\\theta} (\\cdot \\mid s_t) ] \\right] \\leq \\delta\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里，$\\theta_{old}$是更新前的策略参数向量。这个问题可以在对目标进行线性近似和对约束进行二次近似后用共轭梯度算法来高效地近似求解。\n",
    "\n",
    "TRPO的理论证明实际上推荐使用惩罚项而不是约束，也就是说目标是求解无约束的优化问题\n",
    "\n",
    "\\begin{equation}\n",
    "\\operatorname*{maximize}_{\\theta} \\enspace \\hat{\\mathbb{E}}_t \\left[ \\frac{\\pi_{\\theta} (a_t \\mid s_t)}{\\pi_{\\theta_{old}} (a_t \\mid s_t)} \\hat{A}_t - \\beta KL [ \\pi_{\\theta_{old}} (\\cdot \\mid s_t), \\pi_{\\theta} (\\cdot \\mid s_t) ] \\right]\n",
    "\\end{equation}\n",
    "\n",
    "这里的$\\beta$是一个系数。代理目标计算了状态上的KL最大值而不是平均值。代理目标在策略$\\pi$的性能度量上需要有一个下界，也就是一个悲观约束界限，而这个形式满足了这个要求。因为选择单一$\\beta$值并在不同问题上都取得良好性能是非常困难的，就算在同一个问题的学习过程中系统特性也可能会发生改变，所以TRPO使用了硬约束而不是惩罚项。因此，为了实现我们使用一阶算法来达到与TRPO类似的单调改进性能的目标，实验结果显示简单地选择固定惩罚系数$\\beta$并用SGD来优化带惩罚项的目标函数是不够的，我们需要进行额外的修改。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. 经裁剪的代理目标（Clipped Surrogate Objective）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们用$r_t (\\theta)$来表示概率比$r_t (\\theta) = \\frac{\\pi_{\\theta} (a_t \\mid s_t)}{\\pi_{\\theta_{old}}}$，在这里$r(\\theta_{old})=1$。TRPO最大化了“代理”目标\n",
    "\n",
    "\\begin{equation}\n",
    "L^{CPI}(\\theta) = \\hat{\\mathbb{E}}_t \\left[ \\frac{\\pi_{\\theta} (a_t \\mid s_t)}{\\pi_{\\theta_{old}} (a_t \\mid s_t)} \\hat{A}_t \\right] = \\hat{\\mathbb{E}}_t \\left[ r_t (\\theta) \\hat{A}_t \\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上标$CPI$表示了保守策略迭代（conservative policy iteration）。没有这个约束的话，对$L^{CPI}$进行最大化会引起过大的策略更新。因此，我们现在来考虑如何修改这个目标，来惩罚使$r_t (\\theta)$远离$1$的策略改变。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们提出的主要目标如下\n",
    "\n",
    "\\begin{equation}\n",
    "L^{CLIP} (\\theta) = \\hat{\\mathbb{E}}_t \\left[ min(r_t (\\theta) \\hat{A}_t, clip(r_t (\\theta), 1-\\epsilon, 1+ \\epsilon) \\hat{A}_t) \\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的$\\epsilon$是一个超参数，比如说$\\epsilon = 0.2$。使用这个目标的动机如下。$min$函数中的第一项是$L^{CPI}$。第二项，也就是$clip(r_t (\\theta), 1-\\epsilon,1+\\epsilon)\\hat{A}_t$，通过裁剪概率比修改了代理目标，它移除了$r_t$在区间$\\left[ 1-\\epsilon, 1+\\epsilon \\right]$外的激励。最后，我们取裁剪前和裁剪后目标中的最小值，以保证最终目标是未裁剪目标的一个下界（也就是悲观约束界限）。在这个方案下，我们在使目标改进时忽略概率比的改变，但在目标变差时会考虑概率比的改变。注意这里在$r=1$时$L^{CLIP} (\\theta) = L^{CPI} (\\theta)$，但它们在$\\theta$远离$\\theta_{old}$时会变得非常不同。下图画出了$L^{CLIP}$中的其中一项（也就是在某一$t$时）。注意这里的概率比$r$会根据优势项的正负来在$1-\\epsilon$或$1+\\epsilon$会进行裁剪。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/clip_in_surrogate_function.png\" style=\"width: 600px;\" />\n",
    "\n",
    "代理函数$L^{CLIP}$是概率比$r$的函数，上图展示了$L^{CLIP}$中的一项（在$t$时刻时）。左图是正优势的情况，有图是负优势的情况。两幅图中的红点表示了优化的起始点位置，也就是$r=1$时的位置。这里值得注意的是$L^{CLIP}$是许多项的累加和。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下图提供了关于代理目标$L^{CLIP}$的直观感受。它展示了我们在连续控制问题上使用近端策略优化的过程中，当我们在策略更新方向上插值时多个目标是怎样变化的。我们可以看到$L^{CLIP}$是$L^{CPI}$在给过大策略更新惩罚时的一个下界。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/surrogate_objectives.png\" style=\"width: 600px;\" />\n",
    "\n",
    "代理目标，它是当我们在初始策略参数$\\theta_{old}$中插值并且更新后（也就是在一次PPO迭代后）的策略参数。更新后的策略与初始策略的KL散度大约是$0.02$，这个值也就是$L^{CLIP}$的最大值。上图对应了在Hopper-v1问题上的第一次策略更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初次编辑日期"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年4月30日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Schulman, J., Wolski, F., Dhariwal, P., Radford, A. and Klimov, O., 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.\n",
    "\n",
    "[2] https://blog.openai.com/openai-baselines-ppo/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

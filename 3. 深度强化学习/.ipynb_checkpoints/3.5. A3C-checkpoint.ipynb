{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度强化学习的异步方法 (Asynchronous Methods for Deep Reinforcement Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 标签 (Labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvard引用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D. and Kavukcuoglu, K., 2016, June. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning (pp. 1928-1937)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BibTex引用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@inproceedings{mnih2016asynchronous,\n",
    "  title={Asynchronous methods for deep reinforcement learning},\n",
    "  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},\n",
    "  booktitle={International Conference on Machine Learning},\n",
    "  pages={1928--1937},\n",
    "  year={2016}\n",
    "}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 摘要 (Abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic suceeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.\n",
    "\n",
    "我们提出了一个概念简单的轻量级深度强化学习框架，这个框架使用异步梯度下降来对深度神经网络控制器进行了优化。我们将会介绍四个标准强化学习算法的异步变体，此外我们也会展示并行actor-learner对训练的稳定作用，这使得四种方法都能成功训练神经网络控制器。性能最好的方法是actor-critic的异步变体，这个算法在玩Atari游戏方面不仅超过之前性能最好的算法，而且与之前性能最好的算法相比在多核的CPU上（而不是GPU）只需要一半的训练时间。此外，我们也会展示异步actor-critic在一系列的连续电机控制问题上取得了成功，还有在随机3D迷宫中用视觉输入进行导航的应用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 研究问题 (Research Problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asynchronous approaches for reinforcement learning.\n",
    "\n",
    "异步强化学习方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主要贡献 (Contributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Instead of experience replay (which reduce non-stationarity and decorrelates updates but require more memory, more computation and it must be off-policy), A3C (asynchronous advantage actor-critic) asynchronously execute multiple agents in parallel, on multiple instances of the environment to decorrelate data and make the process more stationary.\n",
    "\n",
    "- A3C (asynchronous advantage actor-critic) 没有使用经历回放（这缓解了数据的非静态分布问题，也减轻了更新之间的相关性，但同时需要更多的内存、更多的计算能力并且只能用脱离策略的算法），而是在多个环境的实例中并行执行了多个智能体，从而减轻了数据间的关联性并使得参数更新的过程更加稳定。\n",
    "\n",
    "(2) Achieving better results in far less time, A3C can run on a single machine with a standard multi-core CPU, rather than relying heavily on specialized hardware such as GPUs or massively distributed architectures.\n",
    "\n",
    "- A3C可以在一台拥有多核CPU的机器上运行，并在更短时间内取得更好的结果，而不是严重依赖于像GPU或是大规模分布式架构的专用硬件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概要 (Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors present multi-threaded asynchronous variants of one-step Sarsa, one-step Q-learning, n-step Q-learning, and advantage actor-critic. The aim in designing these methods was to find RL algortihms that can train deep neural network policies reliably and without large resource requirements. While the underlying RL methods are quite different, with actor-critic being an on-policy policy search method and Q-learning being an off-policy value-based method, we use two main ideas to make all four algorithms practical given our design goal.\n",
    "\n",
    "First, we use asynchronous actor-learners, similarly to the Gorila framework (Nair et al., 2015), but instead of using separate machines and a parameter server, we use multiple CPU threads on a single machine. Keeping the learners on a single machine removes the communication costs of sending gradients and parameters and enables us to use Hogwild! (Recht et al., 2011) style updates for training.\n",
    "\n",
    "Second, we make the observation that multiple actors-learners running in parallel are likely to be exploring different parts of the environment. Moreover, one can explicitly use different exploration policies in each actor-learner to maximize this diversity. By running different exploration policies in different threads, the overall changes being made to the parameters by multiple actor-learners applying online updates in parallel are likely to be less correlated in time than a single agent applying online updates. Hence, we do not use a replay memory and rely on parallel actors employing different exploration policies to perform the stabilizing role undertaken by experience replay in the DQN training algorithm.\n",
    "\n",
    "In addition to stabilizing learning, using multiple parallel actor-learners has multiple practical benefits. First, we obtain a reduction in training time that is roughly linear in the number of parallel actor-learners. Second, since we no longer rely on experience replay for stabilizing learning we are able to use on-policy reinforcement learning methods such as Sarsa and actor-critic to train neural networks in a stable way. We now describe our variants of one-step Q-learning, one-step Sarsa, n-step Q-learning and advantage actor-critic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/asynchronous_one-step_q-learning.png\" style=\"width: 400px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 之前的研究 (Previous Work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stabilize reinforcement learning with deep learning by experience replay\n",
    "\n",
    "- Riedmiller, M., 2005, October. Neural fitted Q iteration–first experiences with a data efficient neural reinforcement learning method. In European Conference on Machine Learning (pp. 317-328). Springer, Berlin, Heidelberg.\n",
    "- Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D. and Riedmiller, M., 2013. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602.\n",
    "- Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G. and Petersen, S., 2015. Human-level control through deep reinforcement learning. Nature, 518(7540), p.529.\n",
    "- Van Hasselt, H., Guez, A. and Silver, D., 2016, February. Deep Reinforcement Learning with Double Q-Learning. In AAAI (Vol. 16, pp. 2094-2100).\n",
    "- Schulman, J., Levine, S., Abbeel, P., Jordan, M. and Moritz, P., 2015, June. Trust region policy optimization. In International Conference on Machine Learning (pp. 1889-1897).\n",
    "\n",
    "#### Asynchronous reinforcement learning with massively distributed architecture\n",
    "\n",
    "- Nair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon, R., De Maria, A., Panneershelvam, V., Suleyman, M., Beattie, C., Petersen, S. and Legg, S., 2015. Massively parallel methods for deep reinforcement learning. arXiv preprint arXiv:1507.04296.\n",
    "- Ong, H.Y., Chavez, K. and Hong, A., 2015. Distributed deep Q-learning. arXiv preprint arXiv:1508.04186.\n",
    "- Li, Y. and Schuurmans, D., 2011, September. MapReduce for parallel reinforcement learning. In European Workshop on Reinforcement Learning (pp. 309-320). Springer, Berlin, Heidelberg.\n",
    "- Grounds, M. and Kudenko, D., 2008. Parallel reinforcement learning with linear function approximation. In Adaptive Agents and Multi-Agent Systems III. Adaptation and Multi-Agent Learning (pp. 60-74). Springer, Berlin, Heidelberg.\n",
    "- Tsitsiklis, J.N., 1994. Asynchronous stochastic approximation and Q-learning. Machine learning, 16(3), pp.185-202.\n",
    "- Bertsekas, D., 1982. Distributed dynamic programming. IEEE transactions on Automatic Control, 27(3), pp.610-616.\n",
    "\n",
    "#### Parallel evolutionary approaches for reinforcement learning\n",
    "\n",
    "- Tomassini, M., 1999. Parallel and distributed evolutionary algorithms: A review.\n",
    "- Koutník, J., Schmidhuber, J. and Gomez, F., 2014, July. Evolving deep unsupervised convolutional networks for vision-based reinforcement learning. In Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation (pp. 541-548). ACM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 假设 (Assumptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the standard reinforcement learning setting where an agent interacts with an environment $\\mathcal{E}$ over a number of discrete time steps. At each time step $t$, the agent receives a state $s_t$ and selects an action $a_t$ from some set of possible actions $\\mathcal{A}$ according to its policy $\\pi$, where $\\pi$ is a mapping from state $s_t$ to actions $a_t$. In return, the agent receives the next state $s_{t+1}$ and receives a scalar reward $r_t$. The process continues until the agent reaches a terminal state after which the process restarts. The return $R_t = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+k}$ is the total accumulated return from time step $t$ with discount factor $\\gamma \\in (0, 1]$. The goal of the agent is to maximize the expected return from each state $s_t$.\n",
    "\n",
    "The action value $Q^{\\pi}(s,a)=\\mathbb{E}[R_t\\mid s_t = s,a]$ is the expected return for selecting action $a$ in state $s$ and following policy $\\pi$. The optimal value function $Q^{\\ast}(s,a)=\\max_{\\pi}Q^{\\ast}(s,a)$ gives the maximum action value for state $s$ and action $a$ achievable by any policy. Similarly, the value of state $s$ under policy $\\pi$ is defined as $V^{\\pi}(s)=\\mathbb{E}[R_t\\mid s_t = s]$ and is simply the expected return for following policy $\\pi$ from state $s$.\n",
    "\n",
    "In value-based model-free reinforcement learning methods, the action value function is represented using a function approximator, such as a neural network. Let $Q(s,a;\\theta)$ be an approximate action-value function with parameters $\\theta$. The updates to $\\theta$ can be derived from a variety of reinforcement learning algorithms. One example of such an algorithm is Q-learning, which aims to directly approximate the optimal action value function: $Q^{\\ast}(s,a)\\approx Q(s,a;\\theta)$. In one-step Q-learning, the parameters $\\theta$ of the action value function $Q(s,a;\\theta)$ are learned by iteratively minimizing a sequence of loss functions, where the $i$th loss function defined as\n",
    "\n",
    "$L_i (\\theta_i)=\\mathbb{E}\\left( r + \\gamma \\max_{a'} Q(s',a';\\theta_{i-1})-Q(s,a;\\theta_i) \\right)^2$\n",
    "\n",
    "where $s'$ is the state encountered after state $s$.\n",
    "\n",
    "We refer to the above method as one-step Q-learning because it updates the action value $Q(s,a)$ toward the one-step return $r+\\gamma \\max_{a'}Q(s',a';\\theta)$. One drawback of using one-step methods is that obtaining a reward $r$ only directly affects the value of the state action pairs are affected only indirectly through the updated value $Q(s,a)$. This can make the learning process slow since many updates are required the propagate a reward to the relevant preceding states and actions.\n",
    "\n",
    "One way of propagating rewards faster is by using $n$-step returns (Watkins, 1989; Peng & Williams, 1996). In $n$-step Q-learning, $Q(s,a)$ is updated toward the $n$-step return defined as $r_t + \\gamma r_{t+1}+\\ldots + \\gamma^{n-1}r_{t+n-1}+\\max_{a}\\gamma^n Q(s_{t+n},a)$. This results in a single reward $r$ directly affecting the values of $n$ preceding state action pairs. This makes the process of propagating rewards to relevant state-action pairs potentially much more efficient.\n",
    "\n",
    "In contrast to value-based methods, policy-based model-free methods directly parameterize the policy $\\pi (a\\mid s;\\theta)$ and update the parameters $\\theta$ by performing, typically approximate, gradient ascent on $\\mathbb{E}[R_t]$. One example of such a method is the REINFORCE family of algorithms due to Williams (1992). Standard REINFORCE updates the policy parameters $\\theta$ in the direction $\\nabla_\\theta \\log \\pi (a_t \\mid s_t;\\theta) R_t$, which is an unbiased estimate while keeping it unbiased by subtracting a learned function of the state $b_t (s_t)$, known as a baseline (Williams, 1992), from the return. The resulting gradient is $\\nabla_\\theta \\log \\pi (a_t \\mid s_t ;\\theta)(R_t - b_t (s_t))$.\n",
    "\n",
    "A learned estimate of the value function is commonly used as the baseline $b_t (s_t) \\approx V^{\\pi}(s_t)$ leading to a much lower variance estimate of the policy gradient. When an approximate value function is used as the baseline, the quality $R_t - b_t$ used to scale the policy gradient can be seen as an estimate of the *advantage* of action $a_t$ in state $s_t$, or $A(a_t, s_t)=Q(a_t, s_t)-V(s_t)$, because $R_t$ is an estimate of $Q^{\\pi}(a_t, s_t)$ and $b_t$ is an estimate of $V^{\\pi}(s_t)$. This approach can be viewed as an actor-critic architecture where the policy $\\pi$ is the actor and the baseline $b_t$ is the critic (Sutton & Barto, 1998; Degris et al., 2012)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 论文其它部分 (Other Sections of Paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "Deep neural networks provide rich representations that can enable reinforcement learning (RL) algorithms to perform effectively. However, it was previously thought that the combination of simple online RL algorithms with deep neural networks was fundamentally unstable. Instead, a variety of solutions have been proposed to stabilize the algorithm. These approaches share a common idea: the sequence of observed data encountered by an online RL agent is non-stationary, and online RL updates are strongly correlated. By storing the agent's data in an experience replay memory, the data can be batched or randomly sampled from different time-steps. Aggregating over memory in this way reduces non-stationary and decorrelates updates, but at the same time limits the methods to off-policy reinforcement learning algorithms.\n",
    "\n",
    "Deep RL algorithms based on experience replay have achieved unprecedented success in challenging domain such as Atari 2600. However, experience replay has several drawbacks: it uses more memory and computation per real interaction; and it requires off-policy learning algorithms that can update from data generated by an older policy.\n",
    "\n",
    "In this paper we provide a very different paradigm for deep reinforcement learning. Instead of experience replay, we asynchronously execute multiple agents in parallel, on multiple instances of the environment. This parallelism also decorrelates the agent's data into a more stationary process, since at any given time-step the parallel agents will be experiencing a variety of different states. This simple idea enables a much larger spectrum of fundamental on-policy RL algorithms, such as Sarsa, n-step methods, and actor-critic methods, as well as off-policy RL algorithms such as Q-learning, to be applied robustly and effectively using deep neural networks.\n",
    "\n",
    "Our parallel reinforcement learning paradigm also offers practical benefits. Whereas previous approaches to deep reinforcement learning rely heavily on specialized hardware such as GPUs or massively distributed architectures, our experiments run on a single machine with a standard multi-core CPU. When applied to a variety of Atari 2600 domains, on many games asynchronous reinforcement learning achieves better results, in far less time than previous GPU-based algorithms, using far less resource than massively distributed approaches. The best of the proposed methods, asynchronous advantage actor-critic (A3C), also mastered a variety of continuous motor control tasks as well as learned general strategies for exploring 3D mazes purely from visual inputs. We believe that the success of A3C on both 2D and 3D games, discrete and continuous action spaces, as well as its ability to train feedforward and recurrent agents makes it the most general and successful reinforcement learning agent to date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Related work\n",
    "\n",
    "The General Reinforcement Learning Architecture (Gorila) of (Nair et al., 2015) performs asynchronous training of reinforcement learning agents in a distributed setting. In Gorila, each process contains an actor that acts in its own copy of environment, a separate replay memory, and a learner that samples data from the replay memory and computes gradients of the DQN loss (Mnih et al., 2015) with respect to the policy parameters. The gradients are asynchronously sent to a central parameter server which updates a central copy of the model. The updated policy parameters are sent to the actor-learner processes and 30 parameter server instances, a total of 130 machines, Gorila was able to significantly outperform DQN over 49 Atari games. On many games Gorila reached the score achieved by DQN over 20 times faster than DQN. We also note that a similar way of parallelizing DQN was proposed by (Chavez et al., 2015).\n",
    "\n",
    "In earlier work, (Li & Schuurmans, 2011) applied the Map Reduce framework to parallelizing batch reinforcement learning methods with linear function approximation. Parallelism was used to speed up large matrix operations but not to parallelize the collection of experience or stabilize learning. (Grounds & Kudenko, 2008) proposed a parallel version of the Sarsa algorithm that uses multiple separate actor-learners to accelerate training. Each actor-learner learns separately and periodically sends updates to weights that have changed significantly to the other learner using peer-to-peer communication.\n",
    "\n",
    "(Tsitsiklis, 1994) studied convergence properties of Q-learning in the asynchronous optimization setting. These results show that Q-learning is still guaranteed to converge when some of the information is outdated as long as outdated information is always eventually discarded and several other technical assumptions are satisfied. Even earlier, (Bertsekas, 1982) studied the related problem of distributed dynamic programming.\n",
    "\n",
    "Another related area of work is in evolutionary methods, which are often straightforward to parallelize by distributing fitness evaluations over multiple machines or threads (Tomassini, 1999). Such parallel evolutionary approaches have recently been applied to some visual reinforcement learning tasks. In one example, (Koutnik et al., 2014) evolved convolutional neural network controllers for the TORCS driving simulator by performing fitness evaluations on 8 CPU cores in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初次编辑日期 (Initial Edit Date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年5月16日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献 (References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D. and Kavukcuoglu, K., 2016, June. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning (pp. 1928-1937)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

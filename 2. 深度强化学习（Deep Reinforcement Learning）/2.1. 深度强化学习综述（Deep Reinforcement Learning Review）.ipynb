{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度强化学习入门的两篇论文"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在我访问L-CAS时，Max给我推荐了几篇深度强化学习领域的入门论文，我在这里挑选出了两篇："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Arulkumaran, K., Deisenroth, M.P., Brundage, M. and Bharath, A.A., 2017. A Brief Survey of Deep Reinforcement Learning. arXiv preprint arXiv:1708.05866.**\n",
    "\n",
    "**2. Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D. and Meger, D., 2017. Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面就是我自己翻译的\"A Brief Survey of Deep Reinforcement Learning\"的版本，对于有英文阅读能力的同学还是推荐直接阅读原文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Brief Survey of Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 摘要（Abstract）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度强化学习（deep reinforcement learning）有望给人工智能领域带来革命性的改变，并且它也是构建对视觉世界有更高层理解的自动化系统的一个里程碑。如今，深度学习促使强化学习能够应用在以前难以处理的问题上，比如说直接基于像素信息学习玩电子游戏。深度强化学习算法同样也适用于机器人领域，使得机器人的控制策略可以直接从相机输入的现实世界的信息中直接习得。在这篇综述中，我们开头会介绍强化学习的总体概述，然后会涉及到主流的基于价值（value-based）和基于策略（policy-based）的方法。我们的综述会涵盖深度强化学习的核心算法，包涵deep Q-network、trust region policy optimisation还有asynchronous advantage actor-critic。与此同时，我们也强调了深度神经网络的独特优势，专注于基于强化学习的视觉理解。在结尾部分，我们描述了几个在这个领域的研究方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 引言（Introduction）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "人工智能（AI，Artificial Intelligence）领域的主要目标之一就是创造完全自主的智能体，这种智能体可以通过在与环境的交互中试错来学习最优的行为策略。制造可以交互并且能有效地进行学习AI系统是一项长期的挑战，这包括了可以感知并且对周围世界做出反应的机器人，还有可以与自然语言和多媒体交互的完全基于软件的智能体。强化学习（RL，Reinforcement Learning）是经历驱动自主学习（experience-driven autonomous learning）的一个基础数学框架。尽管RL在过去取得了一定的成功，过去的方法缺乏可扩展性，本质上受限于维度较低的问题。存在这些限制的原因是强化学习算法和其他算法一样都面临着复杂性问题：内存复杂度、计算复杂度、在机器学习算法应用的问题上相同的复杂度。我们在最近几年见证了深度学习的崛起，深度学习的成功依仗于深度神经网络强大的函数近似（function approximation）和表示学习（representation learning）能力，这也为我们克服这些问题提供了新的工具。\n",
    "\n",
    "深度学习的出现对机器学习的许多领域都产生了显著的影响，极大地提高了在一些任务上的最高水准，比如物体识别、语音识别还有语言翻译。深度学习最重要的特点就是深度神经网络可以自动寻找高维数据（比如图片、文本和音频）的简洁低维度表示（特征）。通过人工在神经网络结构中引入归纳偏置，尤其是在层级表示上，机器学习从业者已经在克服维度灾难（curse of dimensionality）上取得了有效的进展。同样的，深度学习也加速了强化学习的发展，在强化学习的框架下使用深度学习算法就是我们所说的深度强化学习（deep reinforcement learning, or DRL）领域。这篇综述会讲到DRL的开创性文献和最近的发展，我们想要传达神经网络应用的新思路，希望能让我们离开发出自主智能体更进一步。想要阅读关于DRL近期进展更详尽综述的同学我们推荐阅读："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Li, Y., 2017. Deep reinforcement learning: An overview. arXiv preprint arXiv:1701.07274.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度学习使强化学习可以被扩展到以前难以处理的决策问题上，也就是具有高维度状态和动作空间的问题。近期DRL的工作中有两个显著的成果。第一个就是开创DRL革新的直接基于图像像素玩Atari 2600游戏并取得超越人类水平成绩的算法。这为应对函数近似技巧在强化学习中的不稳定性提供了方案，这项研究是第一个显示强化学习智能体可以被用在高维原始观测上并只基于奖励信号的有力证明。第二个巨大成功就是一个混合DRL系统的开发，这个系统就是AlphaGo。AlphaGo击败了人类围棋世界冠军，此前这方面的历史成就有IBM的Deep Blue在二十年前击败人类国际象棋冠军，还有IBM的Watson DeepQA系统击败了“Jeopardy！”游戏最强的人类玩家。与在国际象棋系统中主流的人工编写规则方法不同，AlphaGo是在传统启发式搜索算法基础上由神经网络组成并用监督学习和强化学习进行训练的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DRL算法已经被应用在各种各样的问题上了，其中就包括机器人领域。继人工设计的和从低维机器人状态特征中学习的机器人控制器后，DRL使得控制策略可以基于现实世界的相机输入被直接学习。DRL已经被用在创造可以进行元学习（meta-learn, or \"learn to learn\"）的智能体上了，使得这些智能体可以被应用在先前未见过的复杂视觉环境中，这也向实现更强大的智能体更进了一步。我们在下图中展示了DRL的一些应用领域，涉及从电子游戏到室内导航。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/a_range_of_visual_rl_domains.png\" style=\"width: 800px;\" />\n",
    "\n",
    "这是一系列基于视觉强化学习的应用\n",
    "\n",
    "(a) 图片是两个经典的Atari 2600电子游戏，分别是Arcade学习环境（Arcade Learning Environment, ALE）中的\"Freeway\"和\"Seaquest\"。由于支持各种各样种类的游戏，包括了不同的视觉输入和难度，ALE已经成为了DRL算法的标准测试平台。我们将在后面讨论，ALE是现在用来将强化学习的评估标准化的众多基准之一。\n",
    "\n",
    "(b) 图片是TORCS赛车模拟器，它已经被用来测试可以输出连续动作的DRL算法（因为ALE的游戏只支持离散的动作）。\n",
    "\n",
    "(c) 我们可以利用在机器人模拟器中取得的理论上可以是无限的训练数据，有几种方法都被设计来把知识从模拟器中迁移到现实世界。\n",
    "\n",
    "(d) 图片对应Levine et al.设计的四个机器人任务之二：拧瓶盖和把积木放到对应形状的孔中。Levine et al.以端到端的方式成功训练了视觉运动策略，证明了视觉伺服可以用神经网络直接从原始相机输入中学习。\n",
    "\n",
    "(e) 图片是一个现实中的房间，在这里一个轮式机器人被训练来在建筑物中导航。在机器人会被给定视觉输入提示，要找到对应的位置。\n",
    "\n",
    "(f) 图片是一张被用神经网络标注的自然图像，其中用强化学习来选择看哪里。通过对每个生成的单词都处理图片的一小部分，网络模型可以把它的注意力集中在最突出的点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "电子游戏也许是一个有趣的挑战，但是学习怎样玩游戏并不是DRL的最终目的。DRL背后的推力之一就是创造能够学习怎样适应现实世界系统的愿景。从节约功耗到抓取和收纳物品，DRL所主张的是把更多的体力活通过学习来自动化。然而，DRL并没有就此止步，因为强化学习是通过试错来对问题进行优化的一种方法。从设计最前沿的机器翻译模型到构建新的优化函数，DRL已经被用来尝试解决各种机器学习问题。此外，就像深度学习已经被用在机器学习的许多分支一样，DRL成为构建通用AI系统的重要部件在未来似乎是有可能的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 奖励驱动的行为"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们会在回顾深度神经网络对强化学习的贡献之前大体介绍一下强化学习领域。强化学习的本质就是通过交互进行学习。一个强化学习智能体会与它的环境进行交互，智能体在观察自己动作的结果后根据得到的奖励对自己的行为进行修改。这种试错学习的框架起源于行为主义心理学，也是强化学习的主要根基之一。影响强化学习的另一个关键要素就是最优控制，最优控制（最主要是动态规划）也是强化学习领域的数学根基。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在强化学习的框架中，一个被机器学习算法控制的自主智能体在$t$时刻从它的环境中观测到状态$s_t$。这个智能体通过在状态$s_t$采取动作$a_t$来与环境交互。当这个智能体采取一个动作时，环境和智能体都会基于当前的状态和选择的动作过渡到一个新的状态$s_{t+1}$。这个状态是对环境的充分统计描述，因此包涵了智能体选择最佳动作的必要信息，这里的环境也包括了智能体的一部分，比如促动器和传感器的位置。在最优控制的文献中，状态和动作一般被分别记作$x_t$和$u_t$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "动作执行的最佳顺序是由环境提供的奖励所决定的。环境每过渡到一个新的状态，它同时也会给智能体提供一个标量奖励$r_{t+1}$作为反馈。智能体的目标就是学习一个控制策略$\\pi$来最大化期望回报（累积和折扣后的奖励）。给定一个状态，策略会返回一个要执行的动作；最优策略就是任何一个在环境中最大化期望回报的策略。从这个角度看，强化学习想要解决的问题和最优控制是相同的。然而，与最优控制不同的是智能体并不知道状态转移的动力学模型，也就是说强化学习所面临的挑战是智能体需要通过试错来学习在环境中执行特定动作的结果。每次与环境的交互都能产生信息，而智能体就用这些信息来更新它的知识。这个感知-行动-学习的循环如下图所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/the_perception-action-learning_loop.png\" style=\"width: 600px;\" />\n",
    "\n",
    "感知-行动-学习循环。在$t$时刻，智能体从环境中接收到状态$s_t$。智能体用它的策略来选择一个动作$a_t$。当动作被执行时，环境会单步转移到下个状态$s_{t+1}$并获得以奖励$r_{t+1}$形式呈现的反馈。智能体会使用它对状态转移$(s_t, a_t, s_{t+1}, r_{t+1})$的知识来学习和改进策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 马尔科夫决策过程（Markov Decision Process）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强化学习可以在形式上被描述为一个马尔科夫决策过程（Markov Decision Process, or MDP），它的构成有：\n",
    "\n",
    "- 状态的一个集合$\\mathcal{S}$，加上一个起始状态的分布$p(s_0)$\n",
    "- 动作的一个集合$\\mathcal{A}$\n",
    "- 将$t$时刻状态-动作组合映射到$t+1$时刻状态分布的状态转移动力函数$\\mathcal{T} (s_{t+1}\\mid s_t, a_t)$\n",
    "- 一个即时奖励函数$\\mathcal{R}(s_t, a_t, s_{t+1})$\n",
    "- 一个折扣系数$\\gamma \\in [0, 1]$，折扣系数取值较低时强调更近期的奖励"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总的来说，策略$\\pi$是一个从状态到动作概率分布的映射：$\\pi : \\mathcal{S}\\rightarrow p(\\mathcal{A}=a\\mid \\mathcal{S})$。如果MDP是回合制的（episodic），也就是状态在每个长度为$T$的回合结束时都会被重置，一个回合中的状态、动作和奖励构成了这个策略的一个轨迹（trajectory or rollout）。一个策略的每段轨迹都会从环境中累积奖励，这个累积的奖励就是回报$R=\\sum_{t=0}^{T-1} \\gamma^t r_{t+1}$。强化学习的目标就是寻找一个可以在所有状态中都取得最大期望回报的最优策略$\\pi^{\\ast}$：\n",
    "\n",
    "\\begin{equation}\n",
    "\\pi^{\\ast} = \\operatorname*{argmax}_{\\pi} \\mathbb{E}[R\\mid \\pi]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑$T=\\infty$的非回合制的MDP也是可能的。在这种情况下，$\\gamma < 1$就避免了奖励的累积是无穷大的情况。此外，依赖于完整轨迹的方法在这里就不再适用了，但是在状态转移是一个有限集合的情况下还是适用的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强化学习中的一个关键概念就是马尔科夫性质 - 只有当前的状态会影响下一个状态，换句话说就是未来在给定当前状态的情况下是条件独立于过去的。这意味着在$s_t$作出的任何决策都只基于$s_{t-1}$，而不是$\\{ s_0, s_1, \\ldots, s_{t-1} \\}$。尽管大多数的强化学习算法都会基于这个假设，但这也有些不切实际，因为这要求状态都是完全可观测的（fully observable）。MDP的一般化情况就是部分可观测MDP（partially observable MDP, or POMDP），在POMDP中智能体接收到的观测$o_t\\in \\Omega$的分布$p(o_{t+1}\\mid s_{t+1}, a_t)$只取决于当前的状态和之前的动作。从控制和信号处理的角度看，观测值可以用一个取决于当前状态和之前动作的在一个状态空间模型中的对应表示来描述。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POMDP算法一般都会有一个基于过去信念状态、采用的动作和当前观测的关于当前状态的信念（POMDP algorithms typically maintain a belief over the current state given the previous belief state, the action taken and the current observation）。一种在深度学习中更常见的方法就是使用递归神经网络（recurrent neural network, or RNN）。与前馈神经网络（feedforward neural network）不同，递归神经网络是动力系统（dynamical systems）。这种解决POMDP的方法和其它真实状态只能被估计的用动力系统和状态空间模型的问题相关。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 强化学习面临的挑战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强调强化学习所面临的一些挑战对我们是有帮助的：\n",
    "\n",
    "- 最优策略必须通过与环境的试错交互来推测。奖励是智能体能接收到的唯一学习信号。\n",
    "- 智能体的观测取决于它的动作，这也会包含较强的在时间上的相关性。\n",
    "- 智能体必须能应对长期的时间关联性：一个动作的结果往往只在许多次环境状态转移后体现出来。这就是我们所知道的（在时间上的）功劳分配问题（credit assignment problem）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们下面用室内机器人视觉导航的例子来说明这些挑战：如果规定了目标位置，我们可能可以估计剩余距离（并且用它作为奖励信号），但是确切地知道机器人达到目标所要执行的动作序列是不太现实的。因为机器人在建筑物中行进的时候必须选择目标位置，而机器人做出的决策也会影响它接下来看到的房间，也就是得到的视觉序列中的统计信息。最后在机器人经过了几个路口后，机器人也许会发现自己走进了死胡同。这有从学习动作序列到权衡探索新信息与应用已知信息的一系列问题，但这些问题都最终会在强化学习的框架中被解决。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到目前为止，我们已经介绍了在强化学习中使用的主要框架，也就是MDP，并且简要指出了强化学习中的一些挑战。接下来，我们会区分一些不同的强化学习算法。解决强化学习问题的算法主要有两种：基于价值函数（value functions）的方法和基于策略搜索（policy search）的方法。还有一种算法是这两种的结合，也就是行动者-评价者（actor-critic）方法，它既使用了价值函数也是用了策略搜索。我们现在就来解释一下这些方法还有其它一些对求解强化学习问题有用的概念。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 价值函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "价值函数方法是基于估计所处状态价值（期望回报）的方法。状态价值函数（state-value function）$V^{\\pi}(s)$是从状态$s$开始并在之后遵循策略$\\pi$时的预期回报：\n",
    "\n",
    "\\begin{equation}\n",
    "V^{\\pi}(s)=\\mathbb{E} [ R \\mid s, \\pi ]\n",
    "\\end{equation}\n",
    "\n",
    "最优策略$\\pi^{\\ast}$有一个对应的状态价值函数$V^{\\ast}(s)$，反过来就是状态价值函数$V^{\\ast}(s)$也会有一个对应的最优策略$\\pi^{\\ast}$。最优状态价值函数可以被定义为\n",
    "\n",
    "\\begin{equation}\n",
    "V^{\\ast}(s) = \\max_{\\pi} V^{\\pi}(s) \\quad \\forall s \\in \\mathcal{S}\n",
    "\\end{equation}\n",
    "\n",
    "如果有可用的$V^{\\ast}(s)$，我们就可以通过在状态$s_t$时从所有可用的动作中选择最大化$\\mathbb{E}_{s_{t+1}\\sim \\mathcal{T}(s_{t+1} \\mid s_t , a)}[V^{\\ast}(s_{t+1})]$的动作得到最优策略。\n",
    "\n",
    "在强化学习的框架中，我们并没有过渡动态$\\mathcal{T}$。因此，我们就构造了另一个函数，也就是状态动作价值函数（state-action-value function）或者叫品质函数（quality function）$Q^{\\pi}(s, a)$。$Q^{\\pi}(s, a)$与$V^{\\pi}$大部分地方都类似，但是$Q^{\\pi}(s, a)$包含了初始动作$a$并且我们在下个状态之后才开始遵循$\\pi$，\n",
    "\n",
    "\\begin{equation}\n",
    "Q^{\\pi}(s,a)=\\mathbb{E}[R \\mid s,a,\\pi]\n",
    "\\end{equation}\n",
    "\n",
    "在给定$Q^{\\pi}(s,a)$的情况下，最佳策略可以通过在每个状态时贪心地（greedily）选择动作$a$来得到：$\\operatorname*{arg\\,max}_{a} Q^{\\pi}(s,a)$。根据这个策略，我们同样可以通过最大化$Q^{\\pi}(s,a)$来定义$V^{\\pi}(s)$：$V^{\\pi}(s)=\\max_{a}Q^{\\pi}(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**动态规划（Dynamic Programming）**：为了实际学习$Q^{\\pi}$，我们利用马尔科夫性质并且把函数定义为一个贝尔曼方程，这个函数具有下面这个递归的形式：\n",
    "\n",
    "\\begin{equation}\n",
    "Q^{\\pi}(s_t , a_t) = \\mathbb{E}_{s_{t+1}}[r_{t+1} + \\gamma Q^{\\pi}(s_{t+1},\\pi (s_{t+1}))]\n",
    "\\end{equation}\n",
    "\n",
    "这意味着$Q^{\\pi}$可以通过自举（bootstrapping）来进行改进，也就是我们可以通过使用当前对$Q^{\\pi}$的估计价值来改进我们的估值。这是$Q$-learning的和state-action-reward-state-action（SARSA）算法的基础：\n",
    "\n",
    "\\begin{equation}\n",
    "Q^{\\pi}(s_t,a_t)\\leftarrow Q^{\\pi}(s_t,a_t)+\\alpha\\delta\n",
    "\\end{equation}\n",
    "\n",
    "这里的$\\alpha$是学习率，$\\delta = Y - Q^{\\pi}(s_t, a_t)$是时差误差（temporal difference (TD) error）；时差误差中的$Y$也就是传统回归问题中的目标。作为一个依附策略（on-policy）的学习算法，SARSA会使用基于行为策略（behaviour policy, 也就是从$Q^{\\pi}$中衍生出的策略）生成的状态转移来改进$Q^{\\pi}$的估值，改进后的值$Y=r_t + \\gamma Q^{\\pi}(s_{t+1}, a_{t+1})$。$Q$-learning是脱离策略的（off-policy），也就是说用来更新$Q^{\\pi}$的状态转移并不一定要从当前的策略中生成。$Q$-learning使用的是$Y=r_t + \\gamma \\max_{a} Q^{\\pi}(s_{t+1},a)$，这就直接近似了$Q^{\\ast}$。\n",
    "\n",
    "为了借助$Q^{\\pi}$来寻找$Q^{\\ast}$，我们会使用广义策略迭代（generalised policy iteration）。在广义策略迭代中，策略迭代包含了策略评估（policy evaluation）和策略改进（policy improvement）。策略评估通过改进了价值函数的估值，其中的改进方法就是最小化策略经历轨迹的时差误差。在$Q^{\\pi}$的估值改进时，策略可以通过贪心地根据更新后的价值函数选择动作来改进。与其将策略评估和策略改进分开进行并在每一步都要求它们各自收敛（就像在策略迭代中一样），广义策略迭代让这两步交替进行，从而使得这个整个过程更快。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 取样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "动态规划方法对价值函数进行了自举（bootstrap），而蒙特卡洛方法（Monte Carlo method）并没有用到自举。蒙特卡洛方法首先会多次执行同一个策略并记录多条经历，再基于多条经历对每个状态求平均预期回报。因为纯蒙特卡洛方法不依赖自举（bootstrap），所以它也能被应用在非马尔科夫的环境中（non-Markovian environment）。但反过来说，因为回报必须等到回合结束才能进行计算，所以蒙特卡洛方法只能被应用在回合制的MDP中。我们可以使用TD($\\lambda$)算法来结合时差学习和蒙特卡洛策略评估的长处。与折扣系数类似，TD($\\lambda$)中的$\\lambda$定义了它是更倾向于蒙特卡洛评估还是自举。如下图所示，基于使用的样本的量，最终使用的强化学习算法也会有所不同。\n",
    "\n",
    "<img src=\"files/figures/two_dimensions_of_rl_algorithms.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\ldots$未完待续"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初次编辑日期"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年5月1日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Arulkumaran, K., Deisenroth, M.P., Brundage, M. and Bharath, A.A., 2017. A Brief Survey of Deep Reinforcement Learning. arXiv preprint arXiv:1708.05866.\n",
    "\n",
    "[2] Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D. and Meger, D., 2017. Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

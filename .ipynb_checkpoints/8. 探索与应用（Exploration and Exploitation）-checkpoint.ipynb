{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 探索与应用困境（Exploration vs Exploitation Dilemma）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 实时决策都涉及一个根本性的选择（online decision-making involves a fundamental choice）：\n",
    "    - 应用（exploitation）：在现有信息的基础上做出最佳决策（make the best decision given current information）\n",
    "    - 探索（exploration）：收集更多的信息（gather more information）\n",
    "- 最佳的长期策略可能涉及牺牲短期利益（the best long-term strategy may involve short-term sacrifices）\n",
    "- 收集足够的信息来做出最好的全局决策（gather enough information to make the best overall decisions）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 范例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 餐厅的选择（restaurant selection）\n",
    "    - 应用：去你最喜欢的饭店\n",
    "    - 探索：尝试新的饭店\n",
    "- 实时横幅广告（online banner advertisements）\n",
    "    - 应用：展示最成功的广告\n",
    "    - 探索：展示不同的广告\n",
    "- 石油钻探（oil drilling）\n",
    "    - 应用：在最佳已知位置钻探\n",
    "    - 探索：在新的位置钻探\n",
    "- 玩游戏（game playing）\n",
    "    - 应用：用你相信最好的策略\n",
    "    - 探索：采用试验性的策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 原理（Principles）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 朴素探索策略（naive exploration）\n",
    "    - 向贪心策略中加入噪音项，比如$\\epsilon$-贪心策略（add noise to greedy policy, like $\\epsilon - greedy$）\n",
    "- 乐观初始化策略（optimistic initialisation）\n",
    "    - 用优于真实值的估计值作为初始值，直到用真实样本来更新它们（assume the best until proven otherwise）\n",
    "- 面对不确定性时的乐观策略（optimism in the face of uncertainty）\n",
    "    - 倾向选择不确定性大的动作（prefer actions with uncertain values）\n",
    "- 概率匹配策略（probability matching）\n",
    "    - 根据每个动作可能成为最优选择的概率来进行选择（select actions according to probability they are best）\n",
    "- 信息状态搜索策略（information state search）\n",
    "    - 基于信息价值的前瞻预测搜索（lookahead search incorporating value of information）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多臂老虎机问题（The Multi-Armed Bandit）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/the_multi-armed_bandit.png\" style=\"width: 300px;\" />\n",
    "\n",
    "- 一个多臂老虎机是一个元组（a tuple）$<\\mathcal{A}, \\mathcal{R}>$\n",
    "- $\\mathcal{A}$是一个含$m$个动作的已知集合（a known set）\n",
    "- $\\mathcal{R}^a (r) = \\mathbb{P}[r \\mid a]$是一个奖励的未知概率分布（an unknown probability distribution over rewards）\n",
    "- 在每一时刻$t$智能体都会选择一个动作$a_t \\in \\mathcal{A}$\n",
    "- 环境会生成一个奖励（a reward）$r_t \\sim \\mathcal{R}^{a_t}$\n",
    "- 我们的目标就是最大化累计奖励（maximise cumulative reward）$\\sum_{\\tau =1}^t r_{\\tau}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 遗憾（Regret）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 动作价值是动作$a$的平均奖励\n",
    "\\begin{equation}\n",
    "Q(a)=\\mathbb{E}[r\\mid a]\n",
    "\\end{equation}\n",
    "\n",
    "- 最优价值$V^{\\ast}$是\n",
    "\\begin{equation}\n",
    "V^{\\ast}=Q(a^{\\ast})=\\max_{a\\in\\mathcal{A}}Q(a)\n",
    "\\end{equation}\n",
    "\n",
    "- 遗憾（regret）是单步的机会损失（opportunity loss for one step）\n",
    "\\begin{equation}\n",
    "I_t = \\mathbb{E}[V^{\\ast}-Q(a_t)]\n",
    "\\end{equation}\n",
    "\n",
    "- 总遗憾（total regret）是总机会损失（total opportunity loss）\n",
    "\\begin{equation}\n",
    "L_t = \\mathbb{E}\\left[\\sum_{\\tau = 1}^t V^{\\ast} - Q(a_{\\tau})\\right]\n",
    "\\end{equation}\n",
    "\n",
    "- 最大化累计奖励（maximise cumulative reward）$\\equiv$最小化总遗憾（minimise total regret）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 给遗憾计数（Counting Regret）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 计数（count）$N_t (a)$是选择动作$a$的期望次数（expected number of selections for action $a$）\n",
    "- 间隙（gap）$\\Delta_a$是动作$a$和最优动作$a^{\\ast}$之间的价值差，$\\Delta_a = V^{\\ast} - Q(a)$\n",
    "- 遗憾是一个关于间隙和计数的函数（regret is a function of gaps and the counts）\n",
    "\\begin{align}\n",
    "L_t & = \\mathbb{E}\\left[ \\sum_{\\tau =1}^t V^{\\ast}-Q(a_{\\tau}) \\right] \\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}}\\mathbb{E}[N_t (a)](V^{\\ast}-Q(a)) \\\\\n",
    "& = \\sum_{a \\in \\mathcal{A}}\\mathbb{E}[N_t (a)]\\Delta_a\n",
    "\\end{align}\n",
    "\n",
    "- 一个好的算法会确保大间隙的计数很少（small counts for large gaps）\n",
    "- 问题：间隙是未知的！（gaps are not known!）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性或次线性的遗憾（Linear or Sublinear Regret）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/linear_or_sublinear_regret.png\" style=\"width: 500px;\" />\n",
    "\n",
    "- 如果一个算法持续地探索（forever explores），它会有一个线性的总遗憾（linear total regret）\n",
    "- 如果一个算法从不探索（never explores），它会有一个线性的总遗憾（linear total regret）\n",
    "- 那么我们可不可能取得一个次线性的总遗憾（sublinear total regret）呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 贪心算法（Greedy Algorithm）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 让我们来考虑估值$\\hat{Q}_t (a) \\approx Q(a)$的算法\n",
    "- 用蒙特卡洛评估法来估计每个动作的价值（estimate the value of each action by Monte-Carlo evaluation）\n",
    "\\begin{equation}\n",
    "\\hat{Q}_t (a) = \\frac{1}{N_t (a)}\\sum_{t=1}^T r_t \\textbf{1} (a_t = a)\n",
    "\\end{equation}\n",
    "\n",
    "- 贪心算法选择价值最高的动作（the greedy algorithm selects action with highest value）\n",
    "\\begin{equation}\n",
    "a_t^{\\ast} = \\operatorname*{argmax}_{a \\in \\mathcal{A}}\\hat{Q}_t (a)\n",
    "\\end{equation}\n",
    "\n",
    "- 贪心策略会一直停留在次优动作的选项上（greedy can lock onto a suboptimal action forever）\n",
    "- $\\Rightarrow$ 贪心策略的总遗憾是线性的（greedy has linear total regret）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最初编辑日期"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年4月25日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/XX.pdf\n",
    "\n",
    "[2] https://www.youtube.com/watch?v=sGuiWX07sKw"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

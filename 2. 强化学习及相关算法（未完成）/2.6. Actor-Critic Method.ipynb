{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actor-critic-based algorithms are among the most popular methods in reinforcement learning. For example, the Deep Deterministic Policy Gradient (DDPG) algorithm is an actor-critic, model-free method. Moreover the actor-critic framework has many links with neuroscience and animal learning, in particular with models of basal ganglia (Takahashi et al., 2008)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic Methods (and Rats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning is deeply connected with **neuroscience**, and often the research in this area pushed the implementation of new algorithms in the computational field. Following this observation I will introduce actor-critic methods with a brief excursion in the neuroscience field. If you have a pure computational background you will learn something new. My objective is to give you a deeper insight into the reinforcement learning (exteded) world. To understand this introduction you should be familiar with the basic structure of the nervous system. What is a neuron? How do neurons communicate using synapses and neurotransmitters? What is cerebral cortex? You do not need to know the details, here I want you to get the general scheme. Let's start from Dopamine. **Dopamine** is a neuromodulator which is implied in some of the most important process in human and animal brains. You can see the dopamine as a messenger which allows neurons to communicate. Dopamine has an important role in different processes in the mammalian brain (e.g. learning, motivation, addiction), and it is produced in two specific areas: **substantia nigra pars compacta** and **ventral tegmental area**. These two areas have direct projections to another area of the brain, the **striatum**. The striatum is divided in two parts: **ventral striatum** and **dorsal striatum**. The output of the striatum is directed to **motor areas** and **prefrontal cortex**, and it is involved in motor control and planning.\n",
    "\n",
    "<img src=\"files/figures/reinforcement_learning_model_free_active_actor_critic_neural_implementation_no_group.png\" style=\"width: 500px;\" />\n",
    "\n",
    "\n",
    "Most of the areas cited before are part of the **basal ganglia**. There are different models that found a connection between basal ganglia and learning. In particular it seems that the phasic activity of the dopaminergic neurons can deliver an error between an old and a new estimate of expected future rewards. The error is very similar to the error in TD learning. Before going into details I would like to simplify the basal ganglia mechanism distinguishing between two groups:\n",
    "\n",
    "1. Ventral striatum, substantia nigra, ventral tegmental area\n",
    "2. Dorsal striatum and motor areas\n",
    "\n",
    "There are no specific biological names for these groups but I will create two labels for the occasion. The first group can evaluate the saliency of a stimulus based on the associated reward. At the same time it can estimate an error measure comparing the result of the action and the direct consequences, and use this value to calibrate an executor. For these reasons I will call it the **critic**. The second group has direct access to actions but no way to estimate the utility of a stimulus, because of that I will call it the **actor**.\n",
    "\n",
    "<img src=\"files/figures/reinforcement_learning_model_free_active_actor_critic_neural_implementation.png\" style=\"width: 500px;\" />\n",
    "\n",
    "The **interaction between actor and critic** has an important role in learning. In particular a consistent research showed that basal ganglia are involved in Pavlovian learning and in procedural (implicit) memory, meaning unconscious memories such as skills and habits. On the other hand the acquisition of declarative (explicit) memory, which is implied in the recollection of factual information, seems to be connected with another area called hippocampus. The only way actor and critic can communicate is through the dopamine released from the substantia nigra after the stimulation of the ventral striatum. **Drug abuse** can have an effect on the **dopaminergic system**, altering the communication between actor and critic. Experiments of Tkahashi et al. (2007) showed that cocaine sensitisation in rats can have as effect maladaptive decision-making. In particular rather than being influenced by long-term goal the rats are driven by immediate rewards. This issue is present in any standard computational frameworks and is known as the **credit assignment problem**. For example, when playing chess it is not easy to isolate the critical actions that lead to the final victory.\n",
    "\n",
    "<img src=\"files/figures/reinforcement_learning_model_free_active_actor_critic_cocaine_rats.png\" style=\"width: 500px;\" />\n",
    "\n",
    "To understand how the neuronal actor-critic mechanism was involved in the credit assignment problem, Takashi et al. (2008) observed the performances of rats pre-sensitised with cocaine in a **Go/No-Go task**. The procedure of a Go/No-Go task is simple. The rat is in a small metallic box and it has to learn to poke a button with the nose when a specific odour (cue) is released. I the rat pokes the button when a positive odour is present it gets a reward (sugar). If the rat pokes the button when a negative odour is present it gets a bitter substance (e.g. quinine). Here positive and negative odours do not mean that they are pleasant or unpleasant, we can consider them as neutral. Learning means to associate a specific odour to a reward and another specific odour to punishment. Finally, if the rat does not move (No-Go) then neither reward nor punishment are given. In total there are four possible conditions.\n",
    "\n",
    "<img src=\"files/figures/reinforcement_learning_model_free_active_actor_critic_go_nogo_rats.png\" style=\"width: 500px;\" />\n",
    "\n",
    "It has been observed that rats pre-sensitised with cocaine do not learn this task, probably because cocaine damages the basal ganglia and the signal returned by the critic became awry. To test this hypothesis Takahashi et al. (2008) sensitised a group of rats 1-3 months before the experiment and then compared it with a non-sensitised control group. The results of the experiment showed that the **rat in the control group** could **learn how to obtain the reward** when the positive odour was presented and how to avoid the punishment with a no-go strategy when the negative odour was presented. The observation of the basal ganglia showed that the ventral striatum (critic) developed some cue-selectivity neurons, which fired when the odour appeared. The observation of the basal ganglia showed that the ventral striatum (critic) developed some cue-selectivity neurons, which fired when the odour appeared. This neurons developed during the training and their activity preceded the responding in the dorsal striatum (actor).\n",
    "\n",
    "<img src=\"files/figures/reinforcement_learning_model_free_active_actor_critic_normal_rats.png\" style=\"width: 500px;\" />\n",
    "\n",
    "On the other hand the **cocaine sensitised rat** did not show any kind of cue-selectivity during the training. Moreover post-mortem analysis showed that those rats did not developed cue-selective neurons in the ventral striatum (critic). These results confirm the hypothesis that the critic learns the value of the cue and it trains the actor regarding the action to execute.\n",
    "\n",
    "<img src=\"files/figures/reinforcement_learning_model_free_active_actor_critic_sensitised_rats.png\" style=\"width: 500px;\" />\n",
    "\n",
    "In this section I showed you now the actor-critic framework is deeply correlated with the **neurobiology of mammalian brain**. This computational model is elegant and it can explain phenomena like Pavlovian learning and drug addiction. However the elegance of the model should not prevent us to criticize it. In fact different experiments did not confirm it. For example, some form of stimulus-reward learning can take place in the absence of dopamine. Moreover dopamine cells can fire before the stimulus, meaning that its value cannot be used for the update. For a good review of neuronal actor-critic models and their limits I suggest you to read the article of Joel et al. (2002)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewiring Actor-Critic Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last section I presented a neuronal model of the basal ganglia based on the actor-critic framework. Here I will rewire that model using the reinforcement learning techniques we studied until now. The objective is to obtain a computational version which can be used in generic cases (e.g. the 4x3 grid world). The first implementation of an actor-critic algorithm is due to Witten (1977), however the terms Actor and Critic have been introduced later by Barto et al. (1988) to solve the pole-balancing problem. First of all, **how can we represent the critic?** In the neural version the critic does not have access to the actions. Input to the critic is the information obtained through the cerebral cortex which we can compare to the information obtained by the agent through the sensors (state estimation). Moreover the critic receives as input a reward, which arrives directly from the environment. The critic can be represented by an **utility function**, which is updated based on the reward signal received at each iteration. In model free reinforcement learning we can use the **TD(0) algorithm** to represent the critic. The dopaminergic output from substantia nigra and ventral tegmental area can be represented by the two signals which TD(0) returns, meaning the update value and the error estimation $\\delta$. In practice we use the update signal to improve the utility function and the error to update the actor. **How can we represent the actor?** In the neural system the actor receives an input from the cerebral cortex, which we can translate in sensor signals (current state). The dorsal striatum projects to the motor areas and executes an action. Similarly we can use a **state-action matrix** containing the possible actions for each state. The action can be selected with an $\\epsilon$-greedy (or softmax) strategy and then updated using the error returned by the critic. As usual a picture is worth a thousand words:\n",
    "\n",
    "<img src=\"files/figures/reinforcement_learning_model_free_active_actor_critic_neural_implementation_outcome.png\" style=\"width: 500px;\" />\n",
    "\n",
    "We can summarise the steps of the actor-critic algorithm as follow:\n",
    "\n",
    "1. Produce the action $a_t$ for the current state $s_t$\n",
    "2. Observe next state $s_{t+1}$ and the reward $r$\n",
    "3. Update the utility of state $s_t$ (critic)\n",
    "4. Update the probability of the action using $\\delta$ (actor)\n",
    "\n",
    "In **step 1**, the agent produces an action following the current policy. In the previous posts I used an $\\epsilon$-greedy strategy to select the action and to update the policy. Here I will select a certain action using a softmax function:\n",
    "\n",
    "$$P\\{ a_t = a \\mid s_t = s \\} = \\frac{e^{p(s,a)}}{\\sum_b e^{p(s, b)}}$$\n",
    "\n",
    "After the action we observe the new state and the reward (**step 2**). In **step 3** we plug the reward, the utility of $s_t$ and $s_{t+1}$ in the standard update formula used in TD(0):\n",
    "\n",
    "$$U(s_t) \\leftarrow U(s_t) + \\alpha [r_{t+1} + \\gamma U(s_{t+1}) - U(s_t)]$$\n",
    "\n",
    "In **step 4** we use the error estimation $\\delta$ to update the policy. In practical terms step 4 consists in strengthening or weakening the probability of the action using the error $\\delta$ and a positive step-size parameter $\\beta$:\n",
    "\n",
    "$$p(s_t, a_t) \\leftarrow p(s_t, a_t) + \\beta \\delta_t$$\n",
    "\n",
    "Like in the TD case, we can integrate the **eligibility traces** mechanism in actor-critic methods. However in the actor-critic case we need two set of traces, one for the actor and one for the critic. For the **critic** we need to store a trace for each state and update them as follow:\n",
    "\n",
    "\\begin{equation}\n",
    "e_t (s) = \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "\\gamma \\lambda e_{t-1} (s) & \\text{if } s \\neq s_t \\text{;} \\\\\n",
    "\\gamma \\lambda e_{t-1} (s) + 1 & \\text{if } s = s_t \\text{;}\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{equation}\n",
    "\n",
    "Once we estimated the trace we can update the state as follow:\n",
    "\n",
    "$$U(s_t) \\leftarrow U(s_t) + \\alpha \\delta_t e_t (s)$$\n",
    "\n",
    "Great, we obtained our generic computational model to use in a standard reinforcement learning scenario. Now I would like to close the loop giving an answer to a simple question: **does the computational model explain the neurobiological observation?** Apparently yes. In the previous section we saw how Takahashi et al. (2008) observed some anomalies in the interaction between actor and critic in rats sensitised with cocaine. Drug abuse seems to deteriorate the dopaminergic feedback from the critic to the actor. From the computational point of view we can observe a similar result when all the $U(s)$ are the same regardless of the current state. In this case the prediction error $\\delta$ generated by the critic (with $\\gamma=1$) reduces to the immediate available reward:\n",
    "\n",
    "$$\\delta_t = r_{t+1} + U(s_{t+1}) - U(s_t) = r_{t+1}$$\n",
    "\n",
    "This result explains why the **credit assignment problem** emerges during the training of cocaine sensitised rats. The rats prefer the immediate reward and do not take into account the long-term drawbacks. A learning signal based only on immediate reward it is not sufficient to learn a complex Go/No-Go tasks. Paradoxically the same result explains why in simple task in which actions are immediately rewarded learning can be faster in cocaine sensitised rats. However for a neuroscientist this explanation could be too tidy. Recent work has highlighted the existence of multiple learning systems operating in parallel in the mammalian brain. Some of these systems (e.g. amygdala and/or nucleus accumbens) can replace a malfunctioning critic and compensate the damage caused by cocaine sensitisation. In conclusion, additional experiments are needed in order to shed light on the neuronal actor-critic architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic Python Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the knowledge acquired in the previous posts we can easily create a Python script to implement an actor-critic algorithm. As usual I will use the robot cleaning example and the 4x3 grid world. First of all I will describe step-by-step the algorithm in a single episode. Finally I will implement everything in Python. In the complete architecture we can represent the **critic** using a utility function (state matrix). The matrix is initialised with zeros and updated at each iteration through TD learning. For example, after the first step the robot moves from (1, 1) to (1, 2) and obtains a reward of -0.04. The **actor** is represented by a state-action matrix similar to the one introduced to model the Q-function. Each time a new state is observed an action is returned and the robot moves. Here for graphical reason I will draw an empty state-action matrix, but imagine that the values inside the table have been initialised randomly sampling real numbers in the range [0, 1].\n",
    "\n",
    "<img src=\"files/figures/reinforcement_learning_model_free_active_actor_critic_robot_actor_critic.png\" style=\"width: 500px;\" />\n",
    "\n",
    "In the episode considered here the robot starts in the bottom-left corner at state (1, 1) and it reaches the charging station (reward=+1.0) after seven steps.\n",
    "\n",
    "<img src=\"files/figures/reinforcement_learning_model_free_active_actor_critic_robot_first_episode.png\" style=\"width: 500px;\" />\n",
    "\n",
    "The first thing to do is to take action. A query to the state-action table (actor) allows returning the action vector for the current state which in our case is `[0.48, 0.08, 0.15, 0.37]`. The action vector is passed to the softmax function which returns a probability distribution `[0.30, 0.20, 0.22, 0.27]`. Sampling from the distribution using the Numpy method `np.random.choice()` returns UP.\n",
    "\n",
    "<img src=\"files/figures/reinforcement_learning_model_free_active_actor_critic_robot_actor.png\" style=\"width: 500px;\" />\n",
    "\n",
    "Here the softmax function took as input the N-dimensional action vector $\\boldsymbol{x}$ and returned an N-dimensional vector of real values in the range [0, 1] that add up to 1. The softmax function can be easily implemented in Python, however differently from the original softmax equation here I will use `numpy.max()` in the exponents to avoid approximation errors:\n",
    "\n",
    "```python\n",
    "def softmax(x):\n",
    "    '''Compute softmax values of array x.\n",
    "    \n",
    "    @param x the input array\n",
    "    @return the softmax array\n",
    "    '''\n",
    "    return np.exp(x - np.max(x)) / np.sum(np.exp(x - np.max(x)))\n",
    "```\n",
    "\n",
    "After the action a new state is reached and a reward is available (-0.04). It is time for the critic to update the state value and to estimate the error $\\delta$. Here I used the following parameters: $\\alpha=0.1$, $\\beta=1.0$ and $\\gamma=0.9$. Applying the update rule (step 3 of the algorithm) we obtain the new value for the state (1, 1): `0.0 + 0.1[-0.04 + 0.9(0.0) - 0.0] = -0.004`. At the same time it is possible to calculate the error $\\delta$ as follow: `-0.04 + 0.9(0.0) - 0.0 = -0.04`\n",
    "\n",
    "<img src=\"files/figures/reinforcement_learning_model_free_active_actor_critic_robot_critic.png\" style=\"width: 500px;\" />\n",
    "\n",
    "The robot is in a new state and the critic evaluated the error which now must be used to update the state-action table of the actor. In this step the action UP for state (1, 1) is weakened, adding the negative term $\\delta$. In case of a positive $\\delta$ the action would be strengthened.\n",
    "\n",
    "<img src=\"files/figures/reinforcement_learning_model_free_active_actor_critic_robot_actor_2.png\" style=\"width: 500px;\" />\n",
    "\n",
    "\n",
    "Now we can imagine to repeat the same steps until the end of the episode. All the action will be weakened but the last one, which will be strengthened by a factor of +1.0. Repeating the process for many other episodes lead to an optimal utility matrix and an optimal policy.\n",
    "\n",
    "Now the Python implementation. First of all we have to create a function to update the utility matrix (critic). I called this function `update_critic`. The input are the `utility_matrix`, the `observation` and `new_observation` states, then we have the usual hyper-parameters. The function returned an updated utility matrix and the estimation error `delta` to use for updating the actor.\n",
    "\n",
    "```python\n",
    "def update_critic(utility_matrix, observation, new_observation, reward, alpha, gamma):\n",
    "    '''Return the updated utility matrix\n",
    "    \n",
    "    @param utility_matrix the matrix before the update\n",
    "    @param observation the state observed at t\n",
    "    @param new_observation the state observed at t+1\n",
    "    @param reward the reward observed after the action\n",
    "    @param alpha the step size (learning rate)\n",
    "    @param gamma the discount factor\n",
    "    @return the updated utility matrix\n",
    "    @return the estimation error delta\n",
    "    '''\n",
    "    u = utility_matrix[observation[0], observation[1]]\n",
    "    u_t1 = utility_matrix[new_observation[0], new_observation[1]]\n",
    "    delta = reward + gamma * u_t1 - u\n",
    "    utility_matrix[observation[0], observation[1]] += alpha * (delta)\n",
    "    return utility_matrix, delta\n",
    "```\n",
    "\n",
    "The function `update_actor` is used in order to update the state-action matrix. The parameter passed to the function are the `state_action_matrix`, the `observation`, the `action`, the estimation error `delta` returned by `update_critic`, and the hyper-parameter beta that can take the form of a matrix where each element counts how many times a particular state-action pair has been visited.\n",
    "\n",
    "```python\n",
    "def update_actor(state_action_matrix, observation, action, delta, beta_matrix=None):\n",
    "    '''Return the updated state-action matrix\n",
    "    \n",
    "    @param state_action_matrix the matrix before the update\n",
    "    @param observation the state observed at t\n",
    "    @param action taken at time t\n",
    "    @param delta the estimation error returned by the critic\n",
    "    @param beta_matrix a visit counter for each state-action pair\n",
    "    @return the updated matrix\n",
    "    '''\n",
    "    col = observation[1] + (observation[0]*4)\n",
    "    if beta_matrix is None: beta = 1\n",
    "    else: beta = 1 / beta_matrix[action, col]\n",
    "    state_action_matrix[action, col] += beta * delta\n",
    "    return state_action_matrix\n",
    "```\n",
    "\n",
    "The two functions are used in the main loop. The exploring start assumption is kept in order to guarantee uniform exploration. The `beta_matrix` parameter is not used here but it can be easily enabled.\n",
    "\n",
    "```python\n",
    "for epoch in range(tot_epoch):\n",
    "    # Reset and return the first observation\n",
    "    observation = env.reset(exploring_starts=True)\n",
    "    for step in range(1000):\n",
    "        # Estimating the action through Softmax\n",
    "        col = observation[1] + (observation[0]*4)\n",
    "        action_array = state_action_matrix[:, col]\n",
    "        action_distribution = softmax(action_array)\n",
    "        # Sampling an action using the probability\n",
    "        # distribution returned by softmax\n",
    "        action = np.random.choice(4, 1, p=action_distribution)\n",
    "        # beta_matrix[action, col] += 1 # increment the counter\n",
    "        # Move one step in the environment and get obs and reward\n",
    "        new_observation, reward, done = env.step(action)\n",
    "        # Updating the critic (utility_matrix) and getting the delta\n",
    "        utility_matrix, delta = update_critic(utility_matrix, observation, new_observation, reward, alpha, gamma)\n",
    "        # Updating the actor (state-action matrix)\n",
    "        state_action_matrix = update_actor(state_action_matrix, observation, action, delta, beta_matrix=None)\n",
    "        observation = new_observation\n",
    "        if done: break\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Only and Critic-Only Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Sutton and Barto's book actor-critic methods are considered part of TD methods. That makes sense, because the critic is an implementation of the TD(0) algorithm and it is updated following the same rule. The question is: **why should we use actor-critic methods instead of TD learning?** The main advantage of actor-critic methods is that the $\\delta$ returned by the critic is an error measure produced by an external supervisor. We can use this measure to adjust the policy with **supervised learning**. In this sense the use of an external supervisor reduce the variance respect to pure actor-only methods. These aspects will be clearer when I introduce function approximation. Another advantage of actor-critic methods is that the **action selection requires minimal computation**. Until now we always had a discrete number of possible actions. When the action space is continuous and the possible number of action infinite, it is computationally prohibitive to search for the optimal action in this infinite set. Actor-critic methods can represent the policy in a separate discrete structure and use it to find the best action. Another advantage of actor-critic methods is their **similarity to the brain mechanisms of reward** in the mammalian brain. This similarity makes actor-critic methods appealing as psychological and biological models. To summarise there are three advantages in using actor-critic methods:\n",
    "\n",
    "1. **Variance** reduction in function approximation\n",
    "2. **Computationally efficiency** in continuous action space\n",
    "3. **Similarity** to biological reward mechanisms in the mammalian brain\n",
    "\n",
    "Actor-critic methods are considered as a meta-category which can be used to assign all the techniques to three groups: actor-critic method, critic-only, actor-only. In actor-critic methods the actor and the critic are represented explicitly and trained separately, but **is it possible to use only the actor or only the critic**? In fact in TD learning we are always relying on the utility estimation even when the emphasis is on the policy (SARSA and Q-learning). All these methods can be broadly grouped in a category called **critic-only**. Critic-only methods always build a policy on top of a utility function and as I said the utility function is the critic in the actor-critic framework.\n",
    "\n",
    "<img src=\"files/figures/reinforcement_learning_model_free_active_actor_critic_scheme_actor_only_critic_only.png\" style=\"width: 500px;\" />\n",
    "\n",
    "What if we search for an optimal policy without using a utility function? Is that possible? The answer is yes. We can search directly in policy space using an **actor-only** approach. A class of algorithms called **REINFORCE** (REward Increment = Nonnegative Factor x Offset Reinforcement x Characteristic Eligibility) can be considered part of the actor-only group. REINFORCE measures the correlation between the local behaviour and the global performance of the agent and updates the weights of a neural network. Since to understand REINFORCE it is necessary to know gradient descent and generalisation through neural networks I would like to focus on another kind of actor-only techniques which are called **evolutionary algorithms**. The *evolutionary algorithms* label can be applied to a wide range of techniques, but in reinforcement learning genetic algorithms are often used. Using genetic algorithms means to represent each policy as a possible solution to the agent problem. Imagine to have 10 cleaning robots working in parallel, each one using a different (random initialised policy). After 100 episodes we can have an estimation of how good the policy of each single robot is. We can keep the best robots and randomly mutate their policies in order to generate new ones. After some generations the evolution selects the best policies and among them we can (probably) find the optimal one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from the neurobiology of the mammalian brain I introduced actor-critic methods, a class of reinforcement learning algorithms which is widely used by the research community. The neuronal actor-critic model can describe phenomena like Pavlovian learning and drug addiction, whereas its computational counterpart can be easily applied in robotics and machine learning. Considering the TD(0) algorithm as part of the actor-critic scheme, could lead us to the conclusion that actor-critic methods are a TD variation. However, we can turn it around and consider the actor-critic methods as the largest category and TD algorithms as a sub-part. From this point of view I categorised TD algorithms as critic-only methods and techniques such as REINFORCE and genetic algorithm as actor-only methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初次编辑日期 (Initial Edit Date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年6月2日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献 (References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] https://mpatacchiola.github.io/blog/2017/02/11/dissecting-reinforcement-learning-4.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 什么是动态规划(Dynamic Programming)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "动态（Dynamic）：问题的序列或是时序元素\n",
    "\n",
    "规划（Programming）：优化一个方案，或是说优化一个策略。例如线性规划（linear programming）\n",
    "\n",
    "- 一种解决复杂问题的方法\n",
    "\n",
    "- 把大问题化解程子问题\n",
    "\n",
    "    - 解决子问题\n",
    "    \n",
    "    - 把子问题的解决方案结合起来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 动态规划的前提"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "动态规划对满足以下两点性质的问题是非常普适的方案：\n",
    "\n",
    "- 最优子结构（optimal substructure）\n",
    "\n",
    "    - 适用优化原则\n",
    "    \n",
    "    - 最优解可以被分解成子问题\n",
    "\n",
    "- 子问题有重叠部分（overlapping subproblems）\n",
    "\n",
    "    - 子问题会重复出现多次\n",
    "    \n",
    "    - 解能够被缓存并再次利用\n",
    "\n",
    "- 马尔科夫决策过程满足了以上两点性质\n",
    "\n",
    "    - 贝尔曼方程给出了递归分解\n",
    "    \n",
    "    - 价值函数储存并再次使用这些解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用动态规划来规划"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 动态规划的前提假设是我们有MDP的完全信息\n",
    "\n",
    "- 它被用来给一个MDP做规划\n",
    "\n",
    "- 对于预测问题：\n",
    "\n",
    "    - 输入： MDP$<S,A,P,R,\\gamma>$和策略$\\pi$（或是MRP$<S,P^{\\pi},R^{\\pi},\\gamma>$）\n",
    "    \n",
    "    - 输出： 价值函数$v_{\\pi}$\n",
    "\n",
    "- 对于控制问题：\n",
    "\n",
    "    - 输入： MDP $<S,A,P,R,\\gamma>$\n",
    "    \n",
    "    - 输出： 最优价值函数$v_{\\ast}$和最优策略$\\pi_{\\ast}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 迭代策略评估（Iterative Policy Evaluation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 问题： 评估一个给定的策略$\\pi$\n",
    "\n",
    "- 方案： 贝尔曼期望备份的迭代应用\n",
    "\n",
    "- $v_1 \\rightarrow v_2 \\rightarrow \\ldots \\rightarrow v_{\\pi}$\n",
    "\n",
    "- 使用同步备份（synchronous backups），\n",
    "\n",
    "    - 在每次$（k+1）$迭代\n",
    "    \n",
    "    - 对所有状态$s \\in S$\n",
    "    \n",
    "    - 根据$v_{k}(s')$来更新$v_{k+1}(s)$\n",
    "    \n",
    "    - $s'$是$s$的后继状态\n",
    "\n",
    "- 在后面我们会提到异步备份（asynchronous backups）\n",
    "\n",
    "- 后面会证明向$v_{\\pi}$的收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/iterative_policy_evaluation.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "\\begin{equation}\n",
    "v_{k+1}(s) = \\sum_{a \\in A} \\pi(a|s)(R_{s}^{a} + \\gamma \\sum_{s' \\in S} P_{s s'}^{a} v_{k}(s')) \\\\\n",
    "v^{k+1} = R^{\\pi} + \\gamma P^{\\pi} v^{k}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评估在小型Gridworld中的一个随机策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/small_gridworld_1.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "- 无折扣的片段MDP（undiscounted episodic MDP），也就是$\\gamma=1$\n",
    "\n",
    "- 非终止状态 $1,\\ldots,14$\n",
    "\n",
    "- 一个终止状态（用两个带阴影的格子来表示）\n",
    "\n",
    "- 越出格子的动作并不会改变状态\n",
    "\n",
    "- 在没有达到终点前的奖励都是$-1$\n",
    "\n",
    "- 智能体遵循均匀随机策略\n",
    "\n",
    "\\begin{equation}\n",
    "\\pi(n|\\cdot) = \\pi(e|\\cdot) = \\pi(s|\\cdot) = \\pi(w|\\cdot) = 0.25\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/small_gridworld_2.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/small_gridworld_3.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 怎样改进一个策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 给定一个策略$\\pi$\n",
    "\n",
    "    - 评估策略$\\pi$\n",
    "    \n",
    "    \\begin{equation}\n",
    "    v_{\\pi}(s) = E[R_{t+1} + \\gamma R_{t+2} + \\ldots | S_{t}=s]\n",
    "    \\end{equation}\n",
    "\n",
    "    - 以$v_{\\pi}$为基础通过贪心的动作选取来改进策略\n",
    "    \n",
    "    \\begin{equation}\n",
    "    \\pi' = greedy(v_{\\pi})\n",
    "    \\end{equation}\n",
    "\n",
    "- 在小型Gridworld中改进过的策略是最优的，$\\pi'=\\pi^{\\ast}$\n",
    "\n",
    "- 在一般情况下，我们需要更多的迭代来做改进和评估\n",
    "\n",
    "- 但这个策略迭代过程终究会收敛到$\\pi_{\\ast}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 策略迭代（Policy Iteration）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "策略评估（policy evaluation）： 用迭代策略评估来估算$v_{\\pi}$\n",
    "\n",
    "策略改进（policy improvement）： 用贪心策略改进来生成$\\pi' \\geq \\pi$\n",
    "\n",
    "<img src=\"files/figures/policy_iteration_1.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "<img src=\"files/figures/policy_iteration_2.png\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 杰克的汽车租赁问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 状态： 两个地点，每个地点最多容纳20辆汽车\n",
    "\n",
    "- 动作： 一个晚上可以在两个地点之间转移5辆汽车\n",
    "\n",
    "- 奖励： 每辆出租的汽车都会带来$10的收益（汽车必须可用）\n",
    "\n",
    "- 状态转移： 汽车的返还和申请都是随机的\n",
    "\n",
    "    - 泊松分布，$n$返还/申请的概率是$\\frac{\\lambda^{n}}{n !}e^{-\\lambda}$\n",
    "    \n",
    "    - 第一个地点： 平均申请=3，平均返还=3\n",
    "    - 第二个地点： 平均申请=4，平均返还=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 杰克的汽车租赁问题中的策略迭代"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/policy_iteration_in_jacks_car_rental.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 策略改进（Policy Improvement）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 考虑一个确定性的策略，$a=\\pi(s)$\n",
    "\n",
    "- 我们可以通过贪心的动作选择（acting greedily）来改进这个策略\n",
    "\n",
    "\\begin{equation}\n",
    "\\pi'(s) = argmax_{a \\in A} q_{\\pi} (s, a)\n",
    "\\end{equation}\n",
    "\n",
    "- 我们像这样在所有状态$s$上通过单步更新来改进价值函数，\n",
    "\n",
    "\\begin{equation}\n",
    "q_{\\pi}(s, \\pi'(s)) = max_{a \\in A} q_{\\pi}(s, a) \\geq q_{\\pi} (s, \\pi(s)) = v_{\\pi}(s)\n",
    "\\end{equation}\n",
    "\n",
    "- 我们用这种方法来改进价值函数，$v_{\\pi'}(s) \\geq v_{\\pi}(s)$\n",
    "\n",
    "\\begin{align}\n",
    "v_{\\pi}(s) & \\leq q_{\\pi}(s, \\pi'(s)) = E_{\\pi'}[R_{t+1}+\\gamma v_{\\pi}(S_{t+1})|S_t=s] \\\\\n",
    "& \\leq E_{\\pi'}[R_{t+1}+\\gamma q_{\\pi}(S_{t+1},\\pi'(S_{t+1}))|S_t=s] \\\\\n",
    "& \\leq E_{\\pi'}[R_{t+1}+\\gamma R_{t+2}+\\gamma^2 q_{\\pi}(S_{t+2},\\pi'(S_{t+2}))|S_t=s] \\\\\n",
    "& \\leq E_{\\pi'}[R_{t+1}+\\gamma R_{t+2}+\\ldots|S_t=s]=v_{\\pi'}(s)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 当改进停止时，\n",
    "\n",
    "\\begin{equation}\n",
    "q_{\\pi}(s, \\pi'(s)) = max_{a \\in A} q_{\\pi}(s, a) = q_{\\pi}(s, \\pi(s)) = v_{\\pi}(s)\n",
    "\\end{equation}\n",
    "\n",
    "- 这时贝尔曼最优方程满足以下条件\n",
    "\n",
    "\\begin{equation}\n",
    "v_{\\pi}(s) = max_{a \\in A} q_{\\pi} (s, a)\n",
    "\\end{equation}\n",
    "\n",
    "- 因此$v_{\\pi}(s) = v_{\\ast}(s), \\forall s \\in S$\n",
    "\n",
    "- 所以$\\pi$就是一个最优策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 策略迭代的变体"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 那么我们要问策略迭代真的需要价值函数收敛到$v_{\\pi}$吗？\n",
    "\n",
    "- 或者说我们需要引入停止条件吗\n",
    "\n",
    "    - 比如价值函数的$\\epsilon$-收敛\n",
    "\n",
    "- 或者说我们只是简单地在$k$次策略迭代评估后停止？\n",
    "\n",
    "- 比如说在小型Gridworld中，当$k=3$时就已经足够我们取得最优策略了\n",
    "\n",
    "- 那我们为什么不在每次策略评估后更新策略呢？也就是说我们在$k=1$时就停止\n",
    "\n",
    "    - 这和价值迭代（value iteration）是等价的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 广义策略迭代（Generalised Policy Iteration）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "策略评估： 用任何策略评估算法来估计$v_{\\pi}$\n",
    "\n",
    "策略改进： 用任何策略改进算法来生成$\\pi' \\geq \\pi$\n",
    "\n",
    "<img src=\"files/figures/generalised_policy_iteration_1.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "<img src=\"files/figures/generalised_policy_iteration_2.png\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最优原则（Principle of Optimality）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任何的最优策略都可以被分解成两个元素：\n",
    "\n",
    "- 一个最优动作$A_{\\ast}$\n",
    "\n",
    "- 遵循基于后继状态$S'$的一个最优策略\n",
    "\n",
    "我们说策略$\\pi(a|s)$在状态$s$上取得了最优价值$v_{\\pi}(s) = v_{\\ast}(s)$当且仅当\n",
    "\n",
    "- 对于任何从$s$可及的状态$s'$来说\n",
    "\n",
    "- 如果$v_{\\pi}(s')=v_{\\ast}(s')$，我们可以说$\\pi$取得了最优价值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初次编辑时间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年4月18日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/dyna.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 无模型强化学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 无模型预测（model-free prediction）\n",
    "    - 估计（estimate）一个未知MDP的价值函数\n",
    "- 无模型控制（model-free control）\n",
    "    - 优化（optimise）一个未知MDP的价值函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 无模型控制的应用（Uses of Model-Free Control）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some example problems that can be modelled as MDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | \n",
    "| :---: | :---:\n",
    "| 电梯 | 机器人足球\n",
    "| 平行停车 | 射击游戏\n",
    "| 船舶转向 | 投资组合管理\n",
    "| 生物反应器 | 蛋白质折叠\n",
    "| 直升机 | 机器人行走\n",
    "| 飞机物流 | 围棋"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些中的大部分问题都能被划归进这两类：\n",
    "\n",
    "- MDP模型是未知的，但是经验可以被取样\n",
    "- MDP模型是已知的，但是模型太大以至于不能用取样以外的方式使用\n",
    "\n",
    "无模型控制就可以解决这些问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 依附策略和脱离策略学习(On-Policy and Off-Policy Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 依附策略学习（on-policy learning）\n",
    "    - “在工作中学习”（\"learn on the job\"）\n",
    "    - 从使用策略$\\pi$时取样经验，并从这些经验中学习策略$\\pi$\n",
    "- 脱离策略学习（off-policy learning）\n",
    "    - “骑驴找马”（\"look over someone's shoulder\"）\n",
    "    - 从使用策略$\\mu$时取样经验，并从这些经验中学习策略$\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 广义策略迭代（Generalised Policy Iteration）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "策略评估： 用任何策略评估算法来估计$v_{\\pi}$（例如迭代策略评估）\n",
    "\n",
    "策略改进： 用任何策略改进算法来生成$\\pi' \\geq \\pi$（例如贪心策略改进）\n",
    "\n",
    "<img src=\"files/figures/generalised_policy_iteration_1.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "<img src=\"files/figures/generalised_policy_iteration_2.png\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用蒙特卡洛评估的广义策略迭代（Generalised Policy Iteration with Monte-Carlo Evaluation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/generalised_policy_iteration_1.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "策略评估： 蒙特卡洛策略评估，$V=v_{\\pi}$？\n",
    "\n",
    "策略改进： 贪心策略改进？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用动作价值函数的无模型策略迭代（Model-Free Policy Iteration Using Action-Value Function）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在$V(s)$的基础上进行贪心策略改进需要MDP的模型\n",
    "\\begin{equation}\n",
    "\\pi'(s)=argmax_{a \\in A}\\mathcal{R}_s^a + \\mathcal{P}_{s s'}^a V(s')\n",
    "\\end{equation}\n",
    "- 基于$Q(s, a)$的贪心策略改进是无模型的\n",
    "\\begin{equation}\n",
    "\\pi'(s)=argmax_{a \\in A}Q(s, a)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用动作价值函数的广义策略迭代（Generalised Policy Iteration with Action-Value Function）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/generalised_policy_iteration_with_action-value_function.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "策略评估： 蒙特卡洛策略评估，$Q = q_{\\pi}$\n",
    "\n",
    "策略改进： 贪心策略改进？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 范例：贪心动作选择（Example of Greedy Action Selection）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/example_of_greedy_action_selection.png\" style=\"width: 200px;\"/>\n",
    "\n",
    "- 有两扇门在你面前\n",
    "- 你打开了左边的门并获得了奖励$0$\n",
    "\n",
    "$V(left)=0$\n",
    "\n",
    "- 你打开了右边的门并获得了奖励$+1$\n",
    "\n",
    "$V(right)=+1$\n",
    "\n",
    "- 你打开了右边的门并获得了奖励$+3$\n",
    "\n",
    "$V(right)=\\frac{+1+3}{2}=+2$\n",
    "\n",
    "- 你打开了右边的门并获得了奖励$+2$\n",
    "\n",
    "$V(right)=\\frac{+1+3+2}{3}=+2 \\\\\n",
    "\\quad\\vdots$\n",
    "\n",
    "- 你确定你选择了最好的那扇门吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\epsilon$-贪心探索（$\\epsilon$-Greedy Exploration）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这是确保持续探索（continual exploration）最简单的思想\n",
    "- 所有$m$个动作都有非零的概率被尝试\n",
    "- 贪心的动作（也就是此刻估计价值最高的动作）有$1-\\epsilon$的概率被选择\n",
    "- 随机选择动作的概率为$\\epsilon$\n",
    "\n",
    "\\begin{equation}\n",
    "\\pi (a|s)=\\left\\{\n",
    "\\begin{array}{lr}\n",
    "\\begin{split}\n",
    "& \\frac{\\epsilon}{m}+1-\\epsilon \\quad & if \\enspace a^{\\ast}=argmax_{a \\in \\mathcal{A}}Q(s, a) \\\\\n",
    "& \\frac{\\epsilon}{m} & otherwise\n",
    "\\end{split}\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\epsilon$-贪心策略改进（$\\epsilon$-Greedy Policy Improvement）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对任何$\\epsilon$-贪心的策略$\\pi$，基于$q_{\\pi}$的$\\epsilon$-贪心的策略$\\pi'$都是$\\pi$的改进，也就是说$v_{\\pi'}(s) \\geq v_{\\pi}(s)$\n",
    "\n",
    "\\begin{split}\n",
    "& q_{\\pi}(s, \\pi'(s)) & = \\sum_{a \\in \\mathcal{A}}\\pi'(a|s)q_{\\pi}(s, a) \\\\\n",
    "& & = \\frac{\\epsilon}{m}\\sum_{a \\in \\mathcal{A}}q_{\\pi}(s, a)+(1-\\epsilon)max_{a \\in \\mathcal{A}}q_{\\pi}(s, a) \\\\\n",
    "& & \\geq \\frac{\\epsilon}{m}\\sum_{a \\in \\mathcal{A}}q_{\\pi}(s, a)+(1-\\epsilon)\\sum_{a \\in \\mathcal{A}}\\frac{\\pi(a|s)-\\frac{\\epsilon}{m}}{1-\\epsilon}q_{\\pi}(s, a) \\\\\n",
    "& & = \\sum_{a \\in \\mathcal{A}}\\pi(a|s)q_{\\pi}(s, a) = v_{\\pi}(s)\n",
    "\\end{split}\n",
    "\n",
    "因此根据策略改进理论（policy improvement theorem）我们可以得出$v_{\\pi'}(s) \\geq v_{\\pi}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蒙特卡洛策略迭代（Monte-Carlo Policy Iteration）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/Monte-Carlo_policy_iteration.png\" style=\"width: 300px;\" />\n",
    "\n",
    "策略评估： 蒙特卡洛策略评估，$Q=q_{\\pi}$\n",
    "\n",
    "策略改进： $\\epsilon$-贪心策略改进"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蒙特卡洛控制（Monte-Carlo Control）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/Monte-Carlo_control.png\" style=\"width: 300px;\" />\n",
    "\n",
    "每个回合（every episode）：\n",
    "\n",
    "策略评估： 蒙特卡洛策略评估，$Q\\approx q_{\\pi}$\n",
    "\n",
    "策略改进： $\\epsilon$-贪心策略改进"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLIE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在无限探索的极限的基础上上进行贪心选择（Greedy in the Limit with Infinite Exploration, GLIE）\n",
    "\n",
    "- 所有的状态-动作组合（state-action pairs）都被探索了无限次，\n",
    "\n",
    "\\begin{equation}\n",
    "\\lim_{k \\rightarrow \\infty} N_k(s,a)=\\infty\n",
    "\\end{equation}\n",
    "\n",
    "- 策略收敛到一个贪心策略，\n",
    "\n",
    "\\begin{equation}\n",
    "\\lim_{k \\rightarrow \\infty} \\pi_k(a|s) = \\textbf{1} (a = \\operatorname*{argmax}_{a' \\in \\mathcal{A}}Q_k(s,a'))\n",
    "\\end{equation}\n",
    "\n",
    "- 比如说，如果$\\epsilon$在$\\epsilon_k=\\frac{1}{k}$时减小到零，那么$\\epsilon$-贪心就是GLIE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLIE蒙特卡洛控制（GLIE Monte-Carlo Control）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用$\\pi$采$k$th回合的样本:$\\{S_1, A_1, R_2, \\ldots , S_T\\} \\sim \\pi$\n",
    "\n",
    "- 对在回合中的每个状态$S_t$和动作$A_t$，\n",
    "\n",
    "\\begin{split}\n",
    "& N(S_t, A_t) \\leftarrow N(S_t, A_t) + 1 \\\\\n",
    "& Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\frac{1}{N(S_t, A_t)}(G_t - Q(S_t, A_t))\n",
    "\\end{split}\n",
    "\n",
    "- 在新的动作价值函数的基础上改进策略\n",
    "\n",
    "\\begin{split}\n",
    "& \\epsilon \\leftarrow \\frac{1}{k} \\\\\n",
    "& \\pi \\leftarrow \\epsilon-greedy(Q)\n",
    "\\end{split}\n",
    "\n",
    "GLIE蒙特卡洛控制会收敛到最优动作价值函数，也就是$Q(s, a) \\rightarrow q_{\\ast}(s, a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 回到21点的范例（Back to the Blackjack Example）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/back_to_the_blackjack_example.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在21点中应用蒙特卡洛控制（Monte-Carlo Control in Blackjack）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/Monte-Carlo_control_in_blackjack.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蒙特卡洛与时差法控制（MC vs TD Control）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 时差学习（temporal-difference learning, TD）与蒙特卡洛（Monte-Carlo, MC）相比有几个优势\n",
    "    - 低方差\n",
    "    - 在线\n",
    "    - 不完整序列\n",
    "- 顺理成章的思路：在我们的控制循环中使用TD而不是MC\n",
    "    - 在$Q(S, A)$上使用TD\n",
    "    - 使用$\\epsilon$-贪心的策略改进\n",
    "    - 每步更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用Sarsa来更新动作价值函数（Updating Action-Value Function with Sarsa）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/updating_action-value_functions_with_sarsa.png\" style=\"width: 100px;\" />\n",
    "\n",
    "\\begin{equation}\n",
    "Q(S,A) \\leftarrow Q(S,A)+\\alpha(R+\\gamma Q(S', A')-Q(S,A))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用Sarsa进行依附策略的控制（On-Policy Control with Sarsa）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/Monte-Carlo_control.png\" style=\"width: 300px;\" />\n",
    "\n",
    "每一步（every time-step）：\n",
    "\n",
    "策略评估： Sarsa，$Q\\approx q_{\\pi}$\n",
    "\n",
    "策略改进： $\\epsilon$-贪心策略改进"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用于依附策略控制的Sarsa算法（Sarsa Algorithm for On-Policy Control）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/sarsa_algorithm_for_on-policy_control.png\" style=\"width: 600px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarsa的收敛（Convergence of Sarsa）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在满足下列条件时，Sarsa收敛到最优动作价值函数，也就是$Q(s,a)\\rightarrow q_{\\ast}(s,a)$：\n",
    "\n",
    "- 策略$\\pi_t(a|s)$的GLIE序列\n",
    "- 步长为$\\alpha_t$的Robbins-Monro序列\n",
    "\n",
    "\\begin{split}\n",
    "& \\sum_{t=1}^{\\infty}\\alpha=\\infty \\\\\n",
    "& \\sum_{t=1}^{\\infty}\\alpha_t^2 < \\infty\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 范例-Windy Gridword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/windy_gridworld_example_1.png\" style=\"width: 500px;\" />\n",
    "\n",
    "- 在到达目标前每步的奖励$=-1$\n",
    "- 无折扣的（undiscounted）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在Windy Gridworld上使用Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/windy_gridworld_example_2.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-步 Sarsa（n-Step Sarsa）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 让我们来考虑下面这个在$n=1,2,\\ldots,\\infty$时的$n$-步回报：\n",
    "\\begin{split}\n",
    "& n=1 \\quad & (Sarsa) \\quad & q_t^{(1)}=R_{t+1}+\\gamma Q(S_{t+1}) \\\\\n",
    "& n=2 & & q_t^{(2)}=R_{t+1}+\\gamma R_{t+2}+\\gamma^2 Q(S_{t+2}) \\\\\n",
    "& \\enspace \\vdots & & \\vdots \\\\\n",
    "& n=\\infty & (MC) & q_t^{(\\infty)}=R_{t+1}+\\gamma R_{t+2}+\\ldots + \\gamma^{T-1}R_T\n",
    "\\end{split}\n",
    "\n",
    "- $n$-步的$Q$-回报被定义为\n",
    "\\begin{equation}\n",
    "q_t^{(n)}=R_{t+1}+\\gamma R_{t+2}+\\ldots+\\gamma^{n-1}R_{t+n}+\\gamma^n Q(S_{t+n})\n",
    "\\end{equation}\n",
    "\n",
    "- $n$-步Sarsa会向$n$-步$Q$-回报的方向上更新$Q(s,a)$\n",
    "\\begin{equation}\n",
    "Q(S_t,A_t)\\leftarrow Q(S_t,A_t)+\\alpha (q_t^{(n)}-Q(S_t,A_t))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前视Sarsa($\\lambda$)（Forward-View Sarsa($\\lambda$)）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/forward-view_sarsa_lambda.png\" style=\"width: 300px;\" />\n",
    "\n",
    "- $q^{\\lambda}$回报结合了所有$n$-步的$Q$-回报$q_t^{(n)}$\n",
    "- 使用了权重$(1-\\lambda)\\lambda^{n-1}$\n",
    "\\begin{equation}\n",
    "q_t^{\\lambda}=(1-\\lambda)\\sum_{n=1}^{\\infty}\\lambda^{n-1}q_t^{(n)}\n",
    "\\end{equation}\n",
    "- 前视Sarsa($\\lambda$)\n",
    "\\begin{equation}\n",
    "Q(S_t,A_t) \\leftarrow Q(S_t,A_t)+\\alpha (q_t^{\\lambda} - Q(S_t,A_t))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 后视Sarsa($\\lambda$)（Backward-View Sarsa($\\lambda$)）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 就像TD($\\lambda$)，我们在一个在线算法中使用有效性标记（eligibility traces）\n",
    "- 但是Sarsa($\\lambda$)对每个状态-动作组合（state-action pair）都有一个有效性标记\n",
    "\\begin{split}\n",
    "& E_0 (s, a) = 0 \\\\\n",
    "& E_t (s, a) = \\gamma \\lambda E_{t-1} (s, a) + \\textbf{1} (S_t=s, A_t=a)\n",
    "\\end{split}\n",
    "\n",
    "- $Q(s, a)$中的每个状态$s$和动作$a$都会被更新\n",
    "- 与TD-误差$\\delta_t$和有效性标记$E_t (s, a)$成正比\n",
    "\\begin{split}\n",
    "& \\delta_t = R_{t+1} + \\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t) \\\\\n",
    "& Q(s,a) \\leftarrow Q(s,a)+\\alpha\\delta_t E_t (s,a)\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarsa($\\lambda$)算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/sarsa_lambda_algorithm.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 范例-Sarsa($\\lambda$) Gridworld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/sarsa_lambda_gridworld_example.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 脱离策略学习（Off-Policy Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

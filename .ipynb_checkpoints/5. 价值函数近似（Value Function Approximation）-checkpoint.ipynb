{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 大规模强化学习（Large-Scale Reinforcement Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强化学习可以被用来解决大规模的问题（large problems），比如说\n",
    "\n",
    "- 西洋双陆棋（Backgammon）：$10^{20}$个状态\n",
    "- 围棋（Game of Go）：$10^{170}$个状态\n",
    "- 直升飞机（Helicopter）：连续状态空间（continuous state space）\n",
    "\n",
    "那我们怎样把无模型的预测和控制方法扩展到更大规模的问题上呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 价值函数近似（Value Function Approximation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 到现在为止我们都用表格形式来表示价值函数（represent value function by a lookup table）\n",
    "    - 每个状态$s$都有一个记录（entry）$V(s)$\n",
    "    - 或者每个状态-动作组合（state-action pair）$s, a$ 都有一个记录（entry）$Q(s, a)$\n",
    "- 大规模MDP的问题：\n",
    "    - 要存入内存中的状态和动作太多了\n",
    "    - 学习每个状态价值的速度太慢了\n",
    "- 解决大规模MDP的方案：\n",
    "    - 用函数近似（function approximation）来估计价值函数\n",
    "    \\begin{equation}\n",
    "    \\hat{v}(s,\\textbf{w})\\approx v_{\\pi}(s) \\\\\n",
    "    or \\enspace \\hat{q}(s,a,\\textbf{w})\\approx q_{\\pi}(s,a)\n",
    "    \\end{equation}\n",
    "    - 从遇到过的状态（seen states）泛化（generalise）到没有遇到过的状态（unseen states）\n",
    "    - 用MC或是TD学习来更新参数$\\textbf{w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 价值函数近似的种类（Types of Value Function Approximation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/types_of_value_function_approximation.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 选择哪种方程近似方法呢？（Which Function Approximator?）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们有许多种方程近似的方法，比如说\n",
    "\n",
    "- 特征的线性组合（linear combinations of features）\n",
    "- 神经网络（neural network）\n",
    "- 决策树（decision tree）\n",
    "- 邻近取样（nearest neighbour）\n",
    "- 傅里叶/小波基（Fourier/wavelet bases）\n",
    "- $\\ldots$\n",
    "\n",
    "我们在这里只考虑可微分的方程近似方法（differentiable function approximators）\n",
    "\n",
    "- 特征的线性组合（linear combinations of features）\n",
    "- 神经网络（neural network）\n",
    "\n",
    "此外，我们还需要适合非静态、非-iid数据（non-stationary, non-idependent and identically distributed data）的的训练方法（training method）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度下降法（Gradient Descent）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 让我们假设现在我们有一个关于参数向量（parameter vector）$\\textbf{w}$可微分的函数（differentiable function）$J(\\textbf{w})$\n",
    "- 我们把$J(\\textbf{w})$的梯度（gradient）定义为\n",
    "\\begin{equation}\n",
    "\\nabla_{\\textbf{w}}J(\\textbf{w})\n",
    "=\\left(\n",
    "\\begin{array}{lr}\n",
    "\\begin{split}\n",
    "\\frac{\\partial J(\\textbf{w})}{\\partial \\textbf{w}_1} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial J(\\textbf{w})}{\\partial \\textbf{w}_n}\n",
    "\\end{split}\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\end{equation}\n",
    "\n",
    "- 我们想要找到$J(\\textbf{w})$的局部最小值\n",
    "- 向梯度的负方向（direction of -ve gradient）调节$\\textbf{w}$\n",
    "\\begin{equation}\n",
    "\\Delta\\textbf{w}=-\\frac{1}{2}\\alpha\\nabla_{\\textbf{w}}J(\\textbf{w})\n",
    "\\end{equation}\n",
    "\n",
    "$\\quad\\alpha$在这里是步长参数（step-size parameter）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用随机梯度下降法进行价值函数近似（Value Function Approximation by Stochastic Gradient Descent）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 目标：搜寻参数向量$\\textbf{w}$来最小化近似价值函数（approximate value function）$\\hat{v}(s,\\textbf{w})$和真实价值函数（true value function）$v_{\\pi}(s)$之差的均方误差（mean-squared error）\n",
    "\\begin{equation}\n",
    "J(\\textbf{w})=\\mathbb{E}_{\\pi}[(v_{\\pi}(S)-\\hat{v}(S, \\textbf{w}))^2]\n",
    "\\end{equation}\n",
    "\n",
    "- 梯度下降可以找到局部最小值（local minimum）\n",
    "\\begin{split}\n",
    "& \\Delta \\textbf{w} & = - \\frac{1}{2}\\alpha\\nabla_{\\textbf{w}}J(\\textbf{w}) \\\\\n",
    "& & = \\alpha \\mathbb{E}_{\\pi}[(v_{\\pi}(S)-\\hat{v}(S,\\textbf{w}))\\nabla_{\\textbf{w}}\\hat{v}(S,\\textbf{w})]\n",
    "\\end{split}\n",
    "\n",
    "- 随机梯度下降在梯度上取样（stochastic gradient descent samples the gradient）\n",
    "\\begin{equation}\n",
    "\\Delta \\textbf{w} = \\alpha (v_{\\pi}(S)-\\hat{v}(S,\\textbf{w}))\\nabla_{\\textbf{w}}\\hat{v}(S,\\textbf{w})\n",
    "\\end{equation}\n",
    "\n",
    "- 期望更新与全梯度更新相等（expected update is equal to full gradient update）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征向量（Feature Vectors）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用一个特征向量（feature vector）来表示状态\n",
    "\\begin{equation}\n",
    "\\textbf{x}(S)\n",
    "=\\left(\n",
    "\\begin{split}\n",
    "& \\textbf{x}_1 (S) \\\\\n",
    "& \\enspace \\vdots \\\\\n",
    "& \\textbf{x}_n (S)\n",
    "\\end{split}\n",
    "\\right)\n",
    "\\end{equation}\n",
    "\n",
    "- 比如说\n",
    "    - 机器人到地标的距离\n",
    "    - 股市的趋势\n",
    "    - 国际象棋中棋子的位置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性价值函数近似（Linear Value Function Approximation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用特征的线性组合来表示价值函数（represent value function by a linear combination of features）\n",
    "\\begin{equation}\n",
    "\\hat{v}(S,\\textbf{w})=\\textbf{x}(S)^T \\textbf{w} = \\sum_{j=1}^{n}\\textbf{x}_j (S)\\textbf{w}_j\n",
    "\\end{equation}\n",
    "\n",
    "- 目标函数（objective function）对参数$\\textbf{w}$来说是一个二次函数（quadratic）\n",
    "\\begin{equation}\n",
    "J(\\textbf{w})=\\mathbb{E}_{\\pi}[(v_{\\pi}(S)-\\textbf{x}(S)^T\\textbf{w})^2]\n",
    "\\end{equation}\n",
    "\n",
    "- 随机梯度下降会收敛到全局最优值（global optimum）\n",
    "- 更新公式非常简单\n",
    "\\begin{align}\n",
    "\\nabla_{\\textbf{w}}\\hat{v}(S,\\textbf{w}) & =\\textbf{x}(S) \\\\\n",
    "\\Delta \\textbf{w} & = \\alpha (v_{\\pi}(S)-\\hat{v}(S,\\textbf{w}))\\textbf{x}(S)\n",
    "\\end{align}\n",
    "\n",
    "$\\quad 更新值=步长 \\times 预测误差 \\times 特征值$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查表法的特征（Table Lookup Features）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 查表法（table lookup）是线性价值函数近似的一个特例\n",
    "- 查表法所使用的特征（table lookup features）\n",
    "\\begin{equation}\n",
    "\\textbf{x}^{table}(S)\n",
    "=\\left(\n",
    "\\begin{split}\n",
    "& \\textbf{1}(S=s_1) \\\\\n",
    "& \\quad\\enspace \\vdots \\\\\n",
    "& \\textbf{1}(S=s_n)\n",
    "\\end{split}\n",
    "\\right)\n",
    "\\end{equation}\n",
    "\n",
    "- 参数向量$\\textbf{w}$对每个状态都有一个对应的值\n",
    "\\begin{equation}\n",
    "\\hat{v}(S,\\textbf{w})\n",
    "=\\left(\n",
    "\\begin{split}\n",
    "& \\textbf{1}(S=s_1) \\\\\n",
    "& \\quad\\enspace\\vdots \\\\\n",
    "& \\textbf{1}(S=s_n)\n",
    "\\end{split}\n",
    "\\right)\n",
    "\\cdot\n",
    "\\left(\n",
    "\\begin{split}\n",
    "& \\textbf{w}_1 \\\\\n",
    "& \\vdots \\\\\n",
    "& \\textbf{w}_n\n",
    "\\end{split}\n",
    "\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 渐进式预测算法（Incremental Prediction Algorithms）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 通常的假设是真实的价值函数$v_{\\pi}(s)$是由监督者（supervisor）给出的\n",
    "- 但在强化学习中我们并没有监督者（supervisor），我们只有奖励（rewards）\n",
    "- 在实际的应用中，我们用目标值（target）替代$v_{\\pi}(s)$\n",
    "    - 对于MC来说，目标就是回报$G_t$\n",
    "    \\begin{equation}\n",
    "    \\Delta \\textbf{w}=\\alpha (G_t - \\hat{v}(S_t,\\textbf{w}))\\nabla_{\\textbf{w}}\\hat{v}(S_t,\\textbf{w})\n",
    "    \\end{equation}\n",
    "    \n",
    "    - 对于TD(0)来说，目标就是TD目标$R_{t+1}+\\gamma\\hat{v}(S_{t+1},\\textbf{w})$\n",
    "    \\begin{equation}\n",
    "    \\Delta \\textbf{w}=\\alpha (R_{t+1}+\\gamma\\hat{v}(S_{t+1},\\textbf{w}) - \\hat{v}(S_t,\\textbf{w}))\\nabla_{\\textbf{w}}\\hat{v}(S_t,\\textbf{w})\n",
    "    \\end{equation}\n",
    "    \n",
    "    - 对于TD($\\lambda$)来说，目标就是$\\lambda$-回报$G_t^{\\lambda}$\n",
    "    \\begin{equation}\n",
    "    \\Delta \\textbf{w}=\\alpha (G_t^{\\lambda} - \\hat{v}(S_t,\\textbf{w}))\\nabla_{\\textbf{w}}\\hat{v}(S_t,\\textbf{w})\n",
    "    \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于价值函数近似的蒙特卡洛法（Monte-Carlo with Value Function Approximation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 回报$G_t$是一个关于真实价值$v_{\\pi}(S_t)$的无偏并且含噪音的样本（unbiased, noisy sample）\n",
    "- 因为我们可以用监督学习的方法来对“训练数据”（\"training data\"）进行处理\n",
    "\\begin{equation}\n",
    "<S_1,G_1>,<S_2,G_2>,\\ldots ,<S_T,G_T>\n",
    "\\end{equation}\n",
    "\n",
    "- 比如说，我们可以用线性蒙特卡洛策略评估（linear Monte-Carlo policy evaluation）\n",
    "\\begin{align}\n",
    "\\Delta \\textbf{w} & = \\alpha (G_t - \\hat{v}(S_t, \\textbf{w}))\\nabla_{\\textbf{w}}\\hat{v}(S_t,\\textbf{w}) \\\\\n",
    "& = \\alpha (G_t - \\hat{v}(S_t,\\textbf{w}))\\textbf{x}(S_t)\n",
    "\\end{align}\n",
    "\n",
    "- 蒙特卡洛评估收敛到局部最优值（local optimum）\n",
    "- 就算是使用非线性价值函数近似（non-linear value function approximation）时也一样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于价值函数近似的TD学习（TD Learning with Value Function Approximation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TD-目标$R_{t+1}+\\gamma\\hat{v}(S_{t+1},\\textbf{w})$是真实价值$v_{\\pi}(S_t)$的有偏样本（biased sample）\n",
    "- 我们仍然可以在“训练数据”上使用监督学习的方法：\n",
    "\\begin{equation}\n",
    "<S_1, R_2 + \\gamma\\hat{v}(S_2,\\textbf{w})>,<S_2,R_3 +\\gamma\\hat{v}(S_3,\\textbf{w})>,\\ldots ,<S_{T-1},R_T>\n",
    "\\end{equation}\n",
    "\n",
    "- 比如说，我们可以使用线性TD(0)（linear TD(0)）\n",
    "\\begin{align}\n",
    "\\Delta \\textbf{w} & = \\alpha (R+\\gamma\\hat{v}(S',\\textbf{w})-\\hat{v}(S,\\textbf{w}))\\nabla_{\\textbf{w}}\\hat{v}(S,\\textbf{w}) \\\\\n",
    "& = \\alpha\\delta\\textbf{x}(S)\n",
    "\\end{align}\n",
    "\n",
    "- 线性TD(0)收敛到（接近于）全局最优（global optimum）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于价值函数近似的TD($\\lambda$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\lambda$-回报$G_t^{\\lambda}$同样也是真实价值$v_{\\pi}(s)$的有偏样本（biased sample）\n",
    "- 我们仍然可以在“训练数据”上使用监督学习的方法：\n",
    "\\begin{equation}\n",
    "<S_1,G_1^{\\lambda}>,<S_2,G_2^{\\lambda}>,\\ldots,<S_{T-1},G_{T-1}^{\\lambda}>\n",
    "\\end{equation}\n",
    "\n",
    "- 前视线性TD($\\lambda$)（forward-view linear TD($\\lambda$)）\n",
    "\\begin{align}\n",
    "\\Delta\\textbf{w} & = \\alpha (G_t^{\\lambda}-\\hat{v}(S_t,\\textbf{w}))\\nabla_{\\textbf{w}}\\hat{v}(S_t,\\textbf{w}) \\\\\n",
    "& = \\alpha (G_t^{\\lambda}-\\hat{v}(S_t,\\textbf{w}))\\textbf{x}(S_t)\n",
    "\\end{align}\n",
    "\n",
    "- 后视线性TD($\\lambda$)（backward-view linear TD($\\lambda$)）\n",
    "\\begin{align}\n",
    "\\delta_t & = R_{t+1}+\\gamma\\hat{v}(S_{t+1},\\textbf{w})-\\hat{v}(S_t,\\textbf{w}) \\\\\n",
    "E_t & = \\gamma\\lambda E_{t-1}+\\textbf{x}(S_t) \\\\\n",
    "\\Delta\\textbf{w} & = \\alpha\\delta_t E_t\n",
    "\\end{align}\n",
    "\n",
    "前视和后视的线性TD($\\lambda$)是等价的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于价值函数近似的控制（Control with Value Function Approximation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/control_with_value_function_approximation.png\" style=\"width: 300px;\" />\n",
    "\n",
    "策略评估： 近似策略评估（approximate policy evaluation），$\\hat{q}(\\cdot,\\cdot,\\textbf{w})\\approx q_{\\pi}$\n",
    "\n",
    "策略改进： $\\epsilon$-贪心策略改进"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 动作价值函数近似（Action-Value Function Approximation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对动作价值函数（action-value function）进行方程近似\n",
    "\\begin{equation}\n",
    "\\hat{q}(S,A,\\textbf{w})\\approx q_{\\pi}(S,A)\n",
    "\\end{equation}\n",
    "\n",
    "- 对近似动作价值函数（approximate action-value function）$\\hat{q}(S,A,\\textbf{w})$和真实动作价值函数（true action-value function）$q_{\\pi}(S,A)$之差的均方误差（mean-squared error）进行最小化\n",
    "\\begin{equation}\n",
    "J(\\textbf{w})=\\mathbb{E}_{\\pi}[(q_{\\pi}(S,A)-\\hat{q}(S,A,\\textbf{w}))^2]\n",
    "\\end{equation}\n",
    "\n",
    "- 用随机梯度下降来搜寻局部最小值\n",
    "\\begin{align}\n",
    "-\\frac{1}{2}\\nabla_{\\textbf{w}}J(\\textbf{w}) & = (q_{\\pi}(S,A)-\\hat{q}(S,A,\\textbf{w}))\\nabla_{\\textbf{w}}\\hat{q}(S,A,\\textbf{w}) \\\\\n",
    "\\Delta\\textbf{w} & = \\alpha(q_{\\pi}(S,A)-\\hat{q}(S,A,\\textbf{w}))\\nabla_{\\textbf{w}}\\hat{q}(S,A,\\textbf{w})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性动作价值函数近似（Linear Action-Value Function Approximation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用一个特征向量（feature vector）来表示状态和动作\n",
    "\\begin{equation}\n",
    "\\textbf{x}(S,A)\n",
    "=\\left(\n",
    "\\begin{split}\n",
    "& \\textbf{x}_1 (S,A) \\\\\n",
    "& \\quad \\vdots \\\\\n",
    "& \\textbf{x}_n (S,A)\n",
    "\\end{split}\n",
    "\\right)\n",
    "\\end{equation}\n",
    "\n",
    "- 用特征的线性组合来表示动作价值函数\n",
    "\\begin{equation}\n",
    "\\hat{q} (S,A,\\textbf{w})=\\textbf{x} (S,A)^T\\textbf{w}=\\sum_{j=1}^{n}\\textbf{x}_j (S,A)\\textbf{w}_j\n",
    "\\end{equation}\n",
    "\n",
    "- 用随机梯度下降进行更新\n",
    "\\begin{split}\n",
    "\\nabla_{\\textbf{w}}\\hat{q}(S,A,\\textbf{w}) & = \\textbf{x}(S,A) \\\\\n",
    "\\Delta \\textbf{w} & = \\alpha (q_{\\pi}(S,A) - \\hat{q}(S,A,\\textbf{w}))\\textbf{x}(S,A)\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 渐进式控制算法（Incremental Control Algorithm）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 就像预测一样，我们必须用目标值（target）替代$q_{\\pi}(S,A)$\n",
    "    - 对于MC来说，目标值就是回报$G_t$\n",
    "    \\begin{equation}\n",
    "    \\Delta\\textbf{w} = \\alpha (G_t - \\hat{q}(S_t,A_t,\\textbf{w}))\\nabla_{\\textbf{w}}\\hat{q}(S_t,A_t,\\textbf{w})\n",
    "    \\end{equation}\n",
    "    \n",
    "    - 对于TD(0)来说，目标值就是TD目标$R_{t+1}+\\gamma Q(S_{t+1},A_{t+1})$\n",
    "    \\begin{equation}\n",
    "    \\Delta\\textbf{w} = \\alpha (R_{t+1}+\\gamma\\hat{q}(S_{t+1},A_{t+1},\\textbf{w}) - \\hat{q}(S_t,A_t,\\textbf{w}))\\nabla_{\\textbf{w}}\\hat{q}(S_t,A_t,\\textbf{w})\n",
    "    \\end{equation}\n",
    "    \n",
    "    - 对于前视TD($\\lambda$)来说，目标值就是动作价值的$\\lambda$-回报\n",
    "    \\begin{equation}\n",
    "    \\Delta\\textbf{w} = \\alpha (q_t^{\\lambda}-\\hat{q}(S_t,A_t,\\textbf{w}))\\nabla_{\\textbf{w}}\\hat{q}(S_t,A_t,\\textbf{w})\n",
    "    \\end{equation}\n",
    "    \n",
    "    - 对于后视TD($\\lambda$)来说，等价的更新是\n",
    "    \\begin{align}\n",
    "    \\delta_t & = R_{t+1}+\\gamma\\hat{q}(S_{t+1},A_{t+1},\\textbf{w})-\\hat{q}(S_t,A_t,\\textbf{w}) \\\\\n",
    "    E_t & = \\gamma\\lambda E_{t-1} + \\nabla_{\\textbf{w}}\\hat{q}(S_t,A_t,\\textbf{w}) \\\\\n",
    "    \\Delta \\textbf{w} & = \\alpha \\delta_t E_t\n",
    "    \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于粗编码的线性Sarsa在过山车范例中的应用（Linear Sarsa with Coarse Coding in Mountain Car）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/linear_sarsa_with_coarse_coding_in_mountain_car.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于径向基函数的线性Sarsa在过山车范例中的应用（Linear Sarsa with Radial Basis Functions in Mountain Car）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/linear_sarsa_with_radial_basis_functions_in_mountain_car.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 关于$\\lambda$的实验：我们应不应该使用自举？（Study of $\\lambda$: Should We Bootstrap?）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/study_of_lambda_should_we_bootstrap.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baird的反例（Baird's Counterexample）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/bairds_counterexample.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baird的反例中的的参数发散问题（Parameter Divergence in Baird's Counterexample）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/parameter_divergence_in_bairds_counterexample.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测算法的收敛性质（Convergence of Prediction Algorithm）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/convergence_of_prediction_algorithms.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度时差学习（Gradient Temporal-Difference Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TD并不根据任何目标函数的梯度来改变参数\n",
    "- 这也是为什么TD在使用脱离策略（off-policy）或使用非线性函数近似（non-linear function approximation）时会出现发散（diverge）的情况\n",
    "- 梯度TD（gradient TD）是根据预计贝尔曼误差（projected Bellman error）的真实梯度来改变参数的\n",
    "\n",
    "<img src=\"files/figures/gradient_temporal-difference_learning.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 控制算法的收敛性质（Convergence of Control Algorithms）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/convergence_of_control_algorithms.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 批处理强化学习（Batch Reinforcement Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 梯度下降比较简单也比较有吸引力\n",
    "- 但是它的样本效率不高（not sample efficient）\n",
    "- 批处理方法寻求最好的价值函数拟合（seek to find the best fitting value function）\n",
    "- 在给定智能体经验（agent's experience）的情况下，也就是“训练数据”（\"training data\"）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最小二乘法预测（Least Squares Prediction）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 给定价值函数近似（value function approximation）$\\hat{v}(s,\\textbf{w})$\\approx v_{\\pi}(s)\n",
    "- 还有由$<state, value>$组合构成的经历集（experience）$\\mathcal{D}$\n",
    "\\begin{equation}\n",
    "\\mathcal{D}=\\{<s_1,v_1^{\\pi}>,<s_2,v_2^{\\pi}>,\\ldots ,<s_T,v_T^{\\pi}>\\}\n",
    "\\end{equation}\n",
    "\n",
    "- 那么哪一组参数$\\textbf{w}$可以给出拟合最好的价值函数（the best fitting value function）$\\hat{v}(s,\\textbf{w})$？\n",
    "- 最小二乘算法（least squares algorithms）会通过搜寻参数向量（parameter vector）$\\textbf{w}$来最小化$\\hat{v}(s_t,\\textbf{w})$和目标价值$v_t^{\\pi}$之间误差的平方和，\n",
    "\\begin{split}\n",
    "LS(\\textbf{w}) & = \\sum_{t=1}^{T}(v_t^{\\pi} - \\hat{v}(s_t, \\textbf{w}))^2 \\\\\n",
    "& = \\mathbb{E}_{\\mathcal{D}}[(v^{\\pi} - \\hat{v}(s,\\textbf{w}))^2]\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于经历回放的随机梯度下降（Stochastic Gradient Descent with Experience Replay）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们有给定的由$<state,value>$组合构成的经历集（experience）\n",
    "\\begin{equation}\n",
    "\\mathcal{D}=\\{<s_1,v_1^{\\pi}>,<s_2,v_2^{\\pi}>,\\ldots,<s_T,v_T^{\\pi}>\\}\n",
    "\\end{equation}\n",
    "\n",
    "重复：\n",
    "1. 从经历集中取样状态和价值的组合\n",
    "\\begin{equation}\n",
    "<s,v^{\\pi}>\\sim\\mathcal{D}\n",
    "\\end{equation}\n",
    "2. 应用随机梯度下降更新\n",
    "\\begin{equation}\n",
    "\\Delta\\textbf{w}=\\alpha(v^{\\pi}-\\hat{v}(s,\\textbf{w}))\\nabla_{\\textbf{w}}\\hat{v}(s,\\textbf{w})\n",
    "\\end{equation}\n",
    "\n",
    "收敛到最小二乘解\n",
    "\\begin{equation}\n",
    "\\textbf{w}^{\\pi}=\\operatorname*{argmin}_{\\textbf{w}}LS(\\textbf{w})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在DQN中运用经历回放（Experience Replay in Deep Q-Networks）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN运用了经历回放（experience replay）和固定Q-目标（fixed Q-targets）\n",
    "\n",
    "- 根据$\\epsilon$-贪心策略来选择动作$a_t$\n",
    "- 在回放内存（replay memory）$\\mathcal{D}$中存储状态转移$(s_t,a_t,r_{t+1},s_{t+1})$\n",
    "- 从$\\mathcal{D}$中随机取样小批量的状态转移$(s,a,r,s')$\n",
    "- 根据旧的固定参数（old, fixed parameters）$w^{-}$来计算Q-learning的目标\n",
    "- 优化Q-network和Q-learning目标之间的MSE\n",
    "\\begin{equation}\n",
    "\\mathcal{L}_i (w_i) = \\mathbb{E}_{s,a,r,s' \\sim \\mathcal{D}_i} [(r+\\gamma \\max_{a'}Q(s',a';w_i^{-})-Q(s,a;w_i))^2]\n",
    "\\end{equation}\n",
    "\n",
    "- 使用随机梯度下降的变体（variant of stochastic gradient descent）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在Atari上使用DQN（DQN in Atari）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 端到端（end-to-end）的基于像素$s$的价值$Q(s,a)$学习\n",
    "- 输入状态$s$是上4帧的一系列原始像素（raw pixels from last 4 frames）\n",
    "- 输出是18个手柄和按键位置（18 joystick/button positions）的$Q(s,a)$\n",
    "- 在那一刻的分数变化就是奖励（reward is change in score for that step）\n",
    "\n",
    "<img src=\"files/figures/DQN_in_Atari.png\" style=\"width: 500px;\" />\n",
    "\n",
    "网络结构和超参数在所有游戏过程中都是固定的（network architecture and hyperparameters fixed across all games）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/DQN_in_Atari_2.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN的性能提升（How much does DQN help?）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/how_much_does_dqn_help.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性最小二乘预测（Linear Least Squares Prediction）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 经历回放（experience replay）寻找的是最小二乘解（least squares solution）\n",
    "- 但是这个过程可能会经历许多次的迭代\n",
    "- 使用线性的价值函数近似（linear value function approximation）$\\hat{v}(s,\\textbf{w})=\\textbf{x}(s)^{\\intercal}\\textbf{w}$\n",
    "- 我们可以直接求出最小二乘解（solve the least squares solution directly）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在$LS(\\textbf{w})$取最小值时，期望更新的值必须是零\n",
    "\\begin{split}\n",
    "\\mathbb{E}_{\\mathcal{D}}[\\Delta\\textbf{w}] & = 0 \\\\\n",
    "\\alpha \\sum_{t=1}^{T}\\textbf{x}(s_t)(v_t^{\\pi} - \\textbf{x}(s_t)^{\\intercal} \\textbf{w}) & = 0 \\\\\n",
    "\\sum_{t=1}^{T}\\textbf{x}(s_t)v_t^{\\pi} & = \\sum_{t=1}^{T}\\textbf{x}(s_t)\\textbf{x}(s_t)^{\\intercal} \\textbf{w} \\\\\n",
    "\\textbf{w} & = (\\sum_{t=1}^{T}\\textbf{x}(s_t)\\textbf{x}(s_t)^{\\intercal})^{-1}\\sum_{t=1}^T \\textbf{x}(s_t)v_t^{\\pi}\n",
    "\\end{split}\n",
    "\n",
    "- 对有$N$个特征的问题来说，直接解的计算复杂度是$O(N^3)$\n",
    "- 使用Shermann-Morrison公式的话，渐进式算法的计算复杂度是$O(N^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性最小二乘预测算法（Linear Least Squares Prediction Algorithms）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们并不知道真实的价值$v_t^{\\pi}$\n",
    "- 在实践中，我们的“训练数据”必须使用$v_t^{\\pi}$中含有噪音的或是有偏的样本（noisy or biased samples）\n",
    "\n",
    "LSMC： 最小二乘蒙特卡洛（Least Squares Monte-Carlo）使用的是回报，也就是说$v_t^{\\pi}\\approx G_t$\n",
    "\n",
    "LSTD： 最小二乘时差法（Least Squares Temporal-Difference）使用的是TD目标（TD target），也就是说$v_t^{\\pi}\\approx R_{t+1}+\\gamma\\hat{v}(S_{t+1},\\textbf{w})$\n",
    "\n",
    "LSTD($\\lambda$)： 最小二乘TD($\\lambda$)使用的是$\\lambda$-回报（$\\lambda$-return），也就是说$v_t^{\\pi}\\approx G_t^{\\lambda}$\n",
    "\n",
    "- 在每种情况下直接解MC/TD/TD($\\lambda$)的固定点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{split}\n",
    "LSMC \\quad & 0 & = \\sum_{t=1}^T \\alpha (G_t - \\hat{v}(S_t,\\textbf{w}))\\textbf{x}(S_t) \\\\\n",
    "& \\textbf{w} & = (\\sum_{t=1}^T \\textbf{x}(S_t)\\textbf{x}(S_t)^{\\intercal})^{-1} \\sum_{t=1}^{T}\\textbf{x}(S_t)G_t \\\\\n",
    "LSTD \\quad & 0 & = \\sum_{t=1}^T \\alpha (R_{t+1}+\\gamma\\hat{v}(S_{t+1},\\textbf{w}) - \\hat{v}(S_t,\\textbf{w}))\\textbf{x}(S_t) \\\\\n",
    "& \\textbf{w} & = (\\sum_{t=1}^T \\textbf{x}(S_t)(\\textbf{x}(S_t)-\\gamma\\textbf{x}(S_{t+1}))^{\\intercal})^{-1} \\sum_{t=1}^{T}\\textbf{x}(S_t)R_{t+1} \\\\\n",
    "LSTD(\\lambda) \\quad & 0 & = \\sum_{t=1}^{T}\\alpha\\delta_t E_t \\\\\n",
    "& \\textbf{w} & = (\\sum_{t=1}^T E_t (\\textbf{x}(S_t) - \\gamma\\textbf{x}(S_{t+1}))^{\\intercal})^{-1} \\sum_{t=1}^{T} E_t R_{t+1}\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性最小二乘预测算法的收敛特性（Convergence of Linear Least Squares Prediction Algorithms）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/convergence_of_linear_least_squares_prediction_algorithms.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最小二乘策略迭代（Least Squares Policy Iteration）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/least_squares_policy_iteration.png\" style=\"width: 300px;\" />\n",
    "\n",
    "策略评估： 用最小二乘Q-learning来进行策略评估（policy evaluation by least squares Q-learning）\n",
    "\n",
    "策略改进： 贪心策略改进"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最小二乘动作价值函数近似（Least Squares Action-Value Function Approximation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 近似动作价值函数$q_{\\pi}(s,a)$\n",
    "- 用特征$\\textbf{x}(s,a)$的线性组合\n",
    "\\begin{equation}\n",
    "\\hat{q}(s,a,\\textbf{w}) = \\textbf{x}(s,a)^{\\intercal}\\textbf{w} \\approx q_{\\pi}(s,a)\n",
    "\\end{equation}\n",
    "\n",
    "- 最小化$\\hat{q}(s,a,\\textbf{w})$和$q_{\\pi}(s,a)$之间的最小二乘误差\n",
    "- 使用的经历（experience）是用策略$\\pi$产生的\n",
    "- 由$<(state,action),value>$的组合构成\n",
    "\\begin{equation}\n",
    "\\mathcal{D} = \\{ <(s_1,a_1),v_1^{\\pi}>,<(s_2,a_2),v_2^{\\pi}>,\\ldots ,<(s_T,a_T),v_T^{\\pi}> \\}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最小二乘控制（Least Squares Control）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对策略评估来说，我们希望高效地使用所有经历（efficiently use all experience）\n",
    "- 对于控制来说，我们希望改进策略\n",
    "- 这些经历是在使用许多不同策略时产生的\n",
    "- 所以为了评估$q_{\\pi}(S,A)$我们必须脱离策略（off-policy）学习\n",
    "- 我们使用了和Q-learning相同的思想\n",
    "    - 我们使用的是遵循旧策略（old policy）生成的经历（experience）\n",
    "    \\begin{equation}\n",
    "    S_t,A_t,R_{t+1},S_{t+1} \\sim \\pi_{old}\n",
    "    \\end{equation}\n",
    "    \n",
    "    - 我们考虑其它后继动作（alternative successor action）$A' = \\pi_{new}(S_{t+1})$\n",
    "    - 我们向其它动作价值$R_{t+1}+\\gamma\\hat{q}(S_{t+1},A',\\textbf{w})$的方向更新$\\hat{q}(S_t,A_t,\\textbf{w})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最小二乘Q-Learning（Least Squares Q-Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 让我们来考虑接下来这个线性Q-learning更新\n",
    "\\begin{split}\n",
    "\\delta & = R_{t+1}+ \\gamma \\hat{q}(S_{t+1},\\pi(S_{t+1}),\\textbf{w})-\\hat{q}(S_t,A_t,\\textbf{w}) \\\\\n",
    "\\Delta \\textbf{w} & = \\alpha \\delta \\textbf{x}(S_t, A_t)\n",
    "\\end{split}\n",
    "\n",
    "- LSTDQ算法：求总更新为零时的解\n",
    "\\begin{split}\n",
    "0 & = \\sum_{t=1}^T \\alpha (R_{t+1}+\\gamma\\hat{q}(S_{t+1},\\pi(S_{t+1}),\\textbf{w})-\\hat{q}(S_t,A_t,\\textbf{w}))\\textbf{x}(S_t,A_t) \\\\\n",
    "\\textbf{w} & = \\Big(\\sum_{t=1}^T \\textbf{x}(S_t,A_t)(\\textbf{x}(S_t,A_t)-\\gamma\\textbf{x}(S_{t+1},\\pi(S_{t+1})))^{\\intercal}\\Big)^{-1}\\sum_{t=1}^T \\textbf{x}(S_t,A_t)R_{t+1}\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最小二乘策略迭代算法（Least Squares Policy Iteration Algorithm）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 下面的伪代码就在策略评估上使用了LSTDQ\n",
    "- 它用不同的策略反复评估经历$\\mathcal{D}$\n",
    "\n",
    "<img src=\"files/figures/least_square_policy_iteration_algorithm.png\" style=\"width: 300px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 控制算法的收敛特性（Convergence of Control Algorithms）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/convergence_of_control_algorithms_fa.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 范例-链上行走（Chain Walk Example）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/chain_walk_example.png\" style=\"width: 500px;\" />\n",
    "\n",
    "- 让我们来考虑同样问题有50个状态的版本\n",
    "- 在状态10和41上的奖励是+1，其它状态的奖励是0\n",
    "- 最优策略：R(1-9), L(10-25), R(26-41), L(42, 50)\n",
    "- 特征（features）：每个动作都有10个均匀分布的高斯核（Gaussian with $\\sigma = 4$）\n",
    "- 经历（experience）：随机行走策略（random walk policy）中的10,000步"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在链上行走中使用LSPI：动作价值函数（LSPI in Chain Walk: Action-Value Function）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/lspi_in_chain_walk_action-value_function.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在链上行走中使用LSPI：策略（LSPI in Chain Walk: Policy）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/lspi_in_chain_walk_policy.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最初编辑日期"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年4月22日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/FA.pdf\n",
    "\n",
    "[2] https://www.youtube.com/watch?v=UoPei5o4fps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

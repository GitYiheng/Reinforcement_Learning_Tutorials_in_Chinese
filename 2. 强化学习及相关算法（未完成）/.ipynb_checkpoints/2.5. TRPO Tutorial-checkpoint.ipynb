{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two main ideas behind TRPO are **MM algorithms** and the **Trust Region**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MM Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea of MM (minorization-maximization) algorithms is that, intuitively, for a maximization problem, we first find an approximated lower bound of the original objective as the surrogate objective and then maximize the approximated lower bound so as to optimize the original objective. Widely known **Expectation-Maximization (EM) algorithm** is a subclass of MM algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TRPO, Schulman el al. (2015) developed a surrogate loss based on Kakade et al. (2001) and Kakade & Langford (2002). The surrogate loss in TRPO is a lower bound of the original objective - the expected cumulative return of the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trust Region Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in Nocedal & Wright's \"Numerical Optimization\", \"trust-region methods define a region around the current iterative within which they trust the model to be an adequate representation of the objective function, and then choose the step to be the approximate minimizer of the model in this region\". Intuitively, during our optimization procedure, after we decided the gradient direction, when doing line search we want to constrain our step length ot be within a \"trust region\" so that the local estimation of the gradient/curvature remains to be \"trusted\".\n",
    "\n",
    "In TRPO, Schulman et al. (2015) used KL divergence between the old policy and updated policy as a measurement for trust region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An MDP is a tuple $(S, A, \\{P_{sa}\\}, \\gamma, R, \\rho_0)$\n",
    "\n",
    "* $S$ is a finite set of $N$ states.\n",
    "* $A=\\{a_1, \\ldots , a_k\\}$ is a set of $k$ actions\n",
    "* $P_{sa}(s')$ is the state transition probability of landing at state $s': P(s, a, s')$ upon taking the action $a$ at state $s$.\n",
    "* $\\gamma \\in [0,1)$ is the discount function.\n",
    "* $R: S\\rightarrow \\mathbb{R}$ is the reward function.\n",
    "* $\\rho_0: S \\rightarrow \\mathbb{R}$ is the state distribution of the initial state $s_0$.\n",
    "* $\\rho_\\pi: S \\rightarrow \\mathbb{R}$ is the discounted visitation frequencies,\n",
    "$$\\rho_\\pi (s) = Pr[s_0 = s] + \\gamma Pr[s_1 = s] + \\gamma^2 Pr[s_2 = s] + \\ldots$$\n",
    "* $\\eta (\\pi) = \\mathbb{E}_{s_0, a_0, \\ldots} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r(s_t) \\right]$ is the expected discounted cumulative reward of policy $\\pi$. Where\n",
    "$$s_0 \\sim \\rho_0 (s_0), a_t \\sim \\pi (a_t \\mid s_t), s_{t+1} \\sim P(s_{t+1} \\mid s_t, a_t)$$\n",
    "* $Q_\\pi (s_t, a_t)=\\mathbb{E}_{s_{t+1}, a_{t+1},\\ldots}\\left[ \\sum){l=0}^{\\infty} \\gamma^l r(s_{t+l}) \\right]$ is the action-value function\n",
    "* $V_\\pi (s_t)=\\mathbb{E}_{a_t, s_{t+1}, \\ldots} \\left[ \\sum_{l=0}^{\\infty} \\gamma^l r(s_{t+l}) \\right]$ is the value function\n",
    "* $A_\\pi (s, a) = Q_\\pi (s, a) - V_\\pi (s)$ is the advantage function. Where\n",
    "$$a_t \\sim \\pi (a_t \\mid s_t), s_{t+1} \\sim P(s_{t+1} \\mid s_t, a_t), \\text{for } t \\geq 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the important identity proved by Kakade & Langford (2002):\n",
    "\n",
    "\\begin{split}\n",
    "& \\eta (\\pi) & = \\eta (\\pi_0) + \\mathbb{E}_{s_0, a_0, \\ldots \\sim \\pi} \\left[ \\sum_{t=0}^{\\infty} A_{\\pi_0} (s_t, a_t) \\right] \\\\\n",
    "& & = \\eta(\\pi_0) + \\sum_{t=0}^{\\infty} \\sum_{s} P(s_t = s \\mid \\pi) \\sum_{a} \\pi (a \\mid s) \\gamma^t A_{\\pi_0}(s, a) \\\\\n",
    "& & = \\eta(\\pi_0) + \\sum_{s} \\sum_{t=0}^{\\infty} P(s_t = s \\mid \\pi) \\sum_{a} \\pi (a \\mid s) \\gamma^t A_{\\pi_0}(s, a) \\\\\n",
    "& & = \\eta(\\pi_0) + \\sum_s \\rho_\\pi (s) \\sum_a \\pi (a \\mid s) A_{\\pi_0} (s, a) \\\\\n",
    "& \\eta(\\pi) & = \\eta(\\pi_0) + \\mathbb{E}_{\\rho_\\pi}\\mathbb{E}_{a \\sim \\pi (s)}\\left[ A_{\\pi_0} (s, a) \\right]\n",
    "\\end{split}\n",
    "\n",
    "where $\\pi_0$ is the old policy and $\\pi$ is the new policy. Note that we have the current policy $\\pi_0$ but we don't have $\\pi$ yet, therefore, $\\rho_\\pi$ is hard to obtain. Instead, Schulman et al. (2015) used $\\rho{\\pi_0}$ as an approximation to $\\rho_\\pi$:\n",
    "\n",
    "$$\\eta (\\pi) \\approx \\eta (\\pi_0) + \\mathbb{E}_{\\rho_{\\pi_0}} \\mathbb{E}_{a \\sim \\pi (s)} \\left[ A_{\\pi_0} (s, a) \\right]$$\n",
    "\n",
    "We then define the following as the objective function,\n",
    "\n",
    "$$L_{\\pi_0}(\\pi) = \\eta (\\pi_0) + \\mathbb{E}_{\\rho_{\\pi_0}} \\mathbb{E}_{a \\sim \\pi (s)} \\left[ A_{\\pi_0} (s, a) \\right]$$\n",
    "\n",
    "Now is the time when the MM algorithm and trust region come in. Let $\\pi' = \\operatorname*{argmax}_{\\pi'} L_{\\pi_0}(\\pi')$. If we define the new policy as the following mixture:\n",
    "\n",
    "$$\\pi (s) = (1-\\alpha) \\pi_0 (s) + \\alpha \\pi' (s)$$\n",
    "\n",
    "Kakade & Langford (2002) proved that,\n",
    "\n",
    "$$\\eta (\\pi) \\geq L_{\\pi_0} (\\pi) - \\frac{2 \\epsilon \\gamma}{(1-\\gamma (1-\\alpha))(1-\\gamma)} \\alpha^2$$\n",
    "\n",
    "where,\n",
    "\n",
    "$$\\epsilon = \\max_{s} \\left| \\mathbb{E}_{a \\sim \\pi' (s)} [A_{\\pi_0} (s, a)] \\right|$$\n",
    "\n",
    "With this bound (r.h.s of the inequality), we can constraint the update to be within some trust region.\n",
    "\n",
    "Based on this bound, Schulman et al. (2015) proved the following simpler bound involving KL-divergence between the new policy and the old policy:\n",
    "\n",
    "$$\\eta (\\pi) \\geq L_{\\pi_0} (\\pi) - C \\max_{s} D_{KL} \\left( \\pi_0 (s) \\| \\pi (s) \\right)$$\n",
    "\n",
    "where $C = \\frac{2\\epsilon \\gamma}{(1-\\gamma)^2}$\n",
    "\n",
    "Unfortunately, computing the maximum-KL divergence term over the whole state space is intractable. Schulman et al. (2015) proposed to use mean-KL divergence over state space as an approximation so that we can estimate it by\n",
    "\n",
    "$$\\overline{D}_{KL}(\\pi_0 \\| \\pi) = \\mathbb{E}_{s \\sim \\rho_{\\pi_0}} \\left[ D_{KL} (\\pi_0 (s) \\| \\pi (s)) \\right]$$\n",
    "\n",
    "So the TRPO optimization problem is:\n",
    "\n",
    "$$\\operatorname*{maximize}_{\\theta} \\left[ L_{\\theta_0} (\\theta) - C \\overline{D}_{KL} (\\pi_0 \\| \\pi) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in practice, Schulman suggests that we can choose one of the following variants of the algorithm:\n",
    "\n",
    "* Directly use first order optimization methods to optimize the objective which is known as Proximal Policy Optimization.\n",
    "* At each iteration, approximate the objective by first order approximation to $L$ and second order approximation to $\\overline{D}_{KL}(\\pi_0 \\| \\pi)$ and then use second order methods like conjugate gradient to approximate the gradient direction $F^{-1}g$, where, $F$ is the second order derivative of the KL-divergence or known as the Fisher Information Matrix (FIM).\n",
    "* Place hard constraint on the KL-divergence (trust region). We can still use conjugate gradient to solve the following formulation\n",
    "\n",
    "$$\\operatorname*{maximize}_\\theta L_{\\theta_0}(\\theta)$$\n",
    "$$\\text{subject to } \\overline{D}_{KL} (\\pi_{\\theta_0} \\| \\pi_\\theta) \\leq \\delta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初次编辑日期 (Initial Edit Date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年6月1日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献 (References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] http://178.79.149.207/posts/trpo.html\n",
    "\n",
    "[2] http://andrew.gibiansky.com/blog/machine-learning/hessian-free-optimization/\n",
    "\n",
    "[3] https://github.com/stormmax/non-convex"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

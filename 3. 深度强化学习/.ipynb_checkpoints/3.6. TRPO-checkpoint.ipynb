{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 置信域策略优化 (Trust Region Policy Optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 标签 (Labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvard引用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schulman, J., Levine, S., Abbeel, P., Jordan, M. and Moritz, P., 2015, June. Trust region policy optimization. In International Conference on Machine Learning (pp. 1889-1897)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BibTex引用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@inproceedings{schulman2015trust,\n",
    "  title={Trust region policy optimization},\n",
    "  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},\n",
    "  booktitle={International Conference on Machine Learning},\n",
    "  pages={1889--1897},\n",
    "  year={2015}\n",
    "}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 摘要 (Abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimizatin (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.\n",
    "\n",
    "我们将会描述一个能保证性能单调提升的策略优化迭代流程。通过对理论成立的流程进行近似处理，我们开发了一种实用的算法，这种算法叫作信赖域策略优化（Trust Region Policy Optimization, or TRPO）。这种算法与自然策略梯度方法类似并且对优化像神经网络这样的大型非线性策略非常有效。我们的实验展示了TRPO在一系列任务上可靠的性能：让模拟的机器人学习游泳、跳跃和行走；用屏幕图像作为输入玩Atari游戏。尽管TRPO所做的近似与理论并不完全相符，但是TRPO往往能在仅仅微调超参数的情况下在性能上取得单调改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 研究问题 (Research Problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRPO is scalable and can optimize nonlinear policies with tens of thousands of parameter, which have previously posed a major challenge for model-free policy search.\n",
    "\n",
    "TRPO具有可扩展性并且可以优化大规模非线性策略，这两点都是之前的无模型策略搜索难以实现的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概要 (Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most algorithms for policy optimization can be classified into three broad categories:\n",
    "\n",
    "大多数策略优化算法都可以被分到这三类中：\n",
    "\n",
    "(1). Policy iteration methods, which alternate between estimating the value function under the current policy and improving the policy (Bertsekas, 2005).\n",
    "\n",
    "- 策略迭代方法。交替进行当前策略的价值函数的估值和策略改进。\n",
    "\n",
    "(2). Policy gradient methods, which use an estimator of the gradient of the expected return (total reward) obtained from sample trajectories (Peters and Schaal, 2008).\n",
    "\n",
    "- 策略梯度方法。用从样本轨迹中得到的期望回报（总奖励）的梯度估计值。\n",
    "\n",
    "(3). Derivative-free optimization methods, such as the cross-entropy method (CEM) and covariance matrix adaption (CMA), which treat the return as black box function to be optimized in terms of the policy parameters (Szita and Lorincz, 2006).\n",
    "\n",
    "- 无导数优化方法。比如交叉熵方法（cross-entropy method, or CEM）和协方差矩阵自适应方法（covariance matrix adaptation, CMA。将回报当作一个待优化的黑盒函数来优化策略参数。\n",
    "\n",
    "General derivative-free stochastic optimization methods such as CEM and CMA are preferred on many problems, because they achieve good results while being simple to understand and implement. For example, while Tetris is a class benchmark problem for approximate dynamic programming (ADP) methods, stochastic optimization methods are difficult to beat on this tasks (Gabillon et al., 2013). For continuous control problems, methods like CMA have been successful at learning control policies for challenging tasks like locomotion when provided with hand-engineered policy classes with low-dimensional parameterizations (Wampler and Popovic, 2009). The inability of ADP and gradient-based methods to consistently beat gradient-free random search is unsatisfying, since gradient-based optimization algorithms enjoy much better sample complexity guarantees than gradient-free methods (Nemirovski, 2005). Continuous gradient-based optimization has been very successful at learning function approximation has been very successful at learning function approximators for supervised learning tasks with huge numbers of parameters, and extending their success to reinforcement learning would allow for efficient training of complex and powerful polices.\n",
    "\n",
    "现在在许多问题上都更倾向于使用像CEM和CMA这种无导数的随机优化方法，这是因为它们不仅可以取得很好的结果，而且易于理解和实现。比如说，俄罗斯方块作为近似动态规划（approximate dynamic programming, or ADP）的一个经典问题，随机优化方法就难以胜任这个任务。对于连续控制问题，像CMA这样的方法已经可以在人工设计的低维策略的基础上成功学习用于像运动这类有挑战性的任务的控制策略了。因为基于梯度的优化算法比无梯度方法拥有更好的样本复杂性保证，所以ADP和基于梯度的方法无法一直优于无梯度随机搜索的结果让人十分不满。对于参数量很大的监督学习任务来说，连续的基于梯度的优化已经在学习函数近似器上非常成功了。如果我们可以将在监督学习任务上取得的成功扩展到强化学习中，那么在复杂策略上的有效训练也会成为可能。\n",
    "\n",
    "In this article, we first prove that minimizing a certain surrogate objective function guarantees policy improvement with non-trivial step sizes. Then we make a series of approximations to the theoretically-justified algorithm, yielding a practical algorithm, which we call trust region policy optimization (TRPO). We describe two variants of this algorithm: first, the *single-path* method, which can be applied in the model-free setting; second, the *vine* method, which requires the system to be restored to particular states, which is typically only possible in simulation. These algorithms are scalable and can optimize nonlinear policies with tens of thousands of parameters, which have previously posed a major challenge for model-free policy search (Deisenroth et al., 2013). In our experiments, we show that the same TRPO methods can learn complex policies for swimming, hopping, and walking, as well as playing Atari games directly from raw images.\n",
    "\n",
    "在这篇文章中，我们会首先证明通过最小化一个特定的代理目标函数我们可以保证在策略中取得一定的改进。接下来我们会对理论上成立的算法进行一系列的近似来得到一个实用的算法，这个实用的算法就是信赖域策略优化（trust region policy optimization, or TRPO）。我们会描述这种算法的两个变体：第一个是单路径方法（the single-path method），这种方法可以被用在无模型的情况下；第二种是藤方法（the vine method），这种方法需要将系统恢复到某些特定的状态，所以只在模拟中可以用。这些算法都是可扩展的，并且可以优化拥有上万参数的非线性策略，这也是过去无模型策略搜索方法所不能达到的。在我们的实验中，我们证明了相同的TRPO方法可以被用来学习游泳、跳跃和行走的复杂策略，还有用原始图片输入学习玩Atari游戏。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 假设 (Assumptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an infinite-horizon discounted Markov decision process (MDP), defined by the tuple $(\\mathcal{S}, \\mathcal{A}, P, r, \\rho_0, \\gamma)$, where $\\mathcal{S}$ is a finite set of states, $\\mathcal{A}$ is a finite set of actions, $P:\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S}\\rightarrow\\mathbb{R}$ is the transition probability distribution, $r: \\mathcal{S}\\rightarrow\\mathbb{R}$ is the reward function, $\\rho_0 :\\mathcal{S}\\rightarrow\\mathbb{R}$ is the distribution of the initial state $s_0$, and $\\gamma \\in (0, 1)$ is the discount factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\pi$ denote a stochastic policy $\\pi : \\mathcal{S}\\times\\mathcal{A}\\rightarrow [0, 1]$, and let $\\eta (\\pi)$ denote its expected discounted reward:\n",
    "\n",
    "$\\quad \\eta (\\pi)=\\mathbb{E}_{s_0,a_0,\\ldots}\\left[ \\sum_{t=0}^{\\infty}\\gamma^t r(s_t) \\right]$, where\n",
    "\n",
    "$\\quad s_0 \\sim \\rho_0 (s_0)$, $a_t \\sim \\pi (a_t \\mid s_t)$, $s_{t+1} \\sim P(s_{t+1}\\mid s_t , a_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following standard definitions of the state-action value function $Q_{\\pi}$, the value function $V_{\\pi}$, and the advatage function $A_{\\pi}$:\n",
    "\n",
    "$\\quad Q_{\\pi}(s_t, a_t)=\\mathbb{E}_{s_{t+1},a_{t+1},\\ldots}\\left[ \\sum_{l=0}^{\\infty} \\gamma^l r(s_{t+l}) \\right]$,\n",
    "\n",
    "$\\quad V_{\\pi}(s_t)=\\mathbb{E}_{a_t,s_{t+1},\\ldots}\\left[ \\sum_{l=0}^{\\infty} \\gamma^l r(s_{t+l}) \\right]$,\n",
    "\n",
    "$\\quad A_{\\pi}(s,a)=Q_{\\pi}(s,a)-V_{\\pi}(s)$, where\n",
    "\n",
    "$\\quad a_t \\sim \\pi(a_t\\mid s_t)$, $s_{t+1}\\sim P(s_{t+1}\\mid s_t, a_t)$ for $t \\geq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following useful identity expresses the expected return of another policy $\\tilde{\\pi}$ in terms of the advantage over $\\pi$, accumulated over timesteps:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:eq}\n",
    "\\eta (\\tilde{\\pi}) = \\eta (\\pi) + \\mathbb{E}_{s_0,a_0,\\ldots \\sim \\tilde{\\pi}} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t A_{\\pi}(s_t, a_t) \\right]\n",
    "\\end{equation}\n",
    "\n",
    "where the notation $\\mathbb{E}_{s_0,a_0,\\ldots \\sim \\tilde{\\pi}}\\left[ \\ldots \\right]$ indicates that actions are sampled $a_t \\sim \\tilde{\\pi}(\\cdot \\mid s_t)$. Let $\\rho_{\\pi}$ be the (unnormalized) discounted visitation frequencies\n",
    "\n",
    "$\\rho_{\\pi}(s)=P(s_0 =s)+\\gamma P(s_1 =s)+\\gamma^2 P(s_2 =s)+\\ldots$,\n",
    "\n",
    "where $s_0 \\sim \\rho_0$ and the actions are chosen according to $\\pi$. We can rewrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初次编辑时间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年5月9日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Schulman, J., Levine, S., Abbeel, P., Jordan, M. and Moritz, P., 2015, June. Trust region policy optimization. In International Conference on Machine Learning (pp. 1889-1897)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 什么是强化学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/what_is_reinforcement_learning.svg\" style=\"width: 300px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 安装和配置OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 在浏览器地址栏中输入：https://github.com/openai/gym 这就是OpenAI gym的GitHub库。\n",
    "* 打开命令提示符（Command Prompt），选择合适的文件夹，输入`git clone https://github.com/openai/gym`\n",
    "* 再输入`cd gym`进入gym的文件夹\n",
    "* 输入`pip install -e .`进行安装\n",
    "* 最后我们验证一下gym是否安装成功，在命令提示符处输入`python`\n",
    "* 跳转成功后输入`import gym`，如果没有报错的话gym就算是安装成功了（再输入`exit()`或按CTRL+D退出）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OpenAI gym library\n",
    "import gym\n",
    "\n",
    "# Import environments from OpenAI gym\n",
    "from gym import envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "797"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the number of environments in OpenAI gym\n",
    "len(envs.registry.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MountainCar-v0环境的可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mountain car environment\n",
    "import gym\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole-v0环境的可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cart pole environment\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample())\n",
    "env.close()\n",
    "\n",
    "# import gym\n",
    "# import time \n",
    "# env = gym.make('CartPole-v0')\n",
    "# env.reset()\n",
    "# for step in range(1000):\n",
    "#     env.render()\n",
    "#     env.step(env.action_space.sample())\n",
    "#     time.sleep(0.1)\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 观测值 (Observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们希望比在每步随机选取动作要做得更好，那么我们就需要知道动作对环境有什么作用。\n",
    "\n",
    "环境的`step`函数返回的就是我们需要的信息。实际上`step`会返还4个值：\n",
    "\n",
    "* `observation`（对象, object）：这是表示我们观测值的一个环境特定的对象。比如相机的像素数据、机器人的关节角度和角速度或者是棋类游戏的棋盘布局。\n",
    "* `reward`（浮点数, float）：这是之前动作得到的奖励。奖励数值的大小和尺度根据环境不同会有所变化，无论如何目标都是要最大化总奖励。\n",
    "* `done`（布尔值, boolean）：这代表了是否需要再次`reset`当前环境。大多数任务（但并不是全部）都被分成了明确的回合，`done`是`True`时意味着回合结束了。（比如杆子倾斜幅度太大，或者是游戏里的最后一条命没了。）\n",
    "* `info`（字典, dict）：对调试有帮助的诊断信息。这些信息有时候对学习会有帮助（比如它也许包含了环境最后一个状态改变的原始概率信息）。然而，对于智能体的官方评估是不允许用这些信息来进行学习的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 探索CartPole-v0的动作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重置环境，返回值为cart-pole的观测值$(x, \\dot{x}, \\theta, \\dot{\\theta})$\n",
    "\n",
    "* $x$：cart的位置，也就是横坐标\n",
    "* $\\dot{x}$：cart的速度\n",
    "* $\\theta$：pole的角度\n",
    "* $\\dot{\\theta}$：pole的角速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00206123, -0.03788145, -0.00525245, -0.04826139])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看可以选择的动作，`Discrete(2)`意味着我们有两个离散的动作可以选择，第1个动作的序号为`0`也就是向左推cart，第2个动作的序号为`1`也就是向右推cart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向左推cart，返回值为$\\left( (x, \\dot{x}, \\theta, \\dot{\\theta}), \\mbox{reward}, \\mbox{done}, \\mbox{info} \\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.0013036 , -0.23292769, -0.00621768,  0.24275972]), 1.0, False, {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机搜索 (Random Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "200 achieved on step 9\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger('rl')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "class Harness:\n",
    "\n",
    "    def run_episode(self, env, agent):\n",
    "        observation = env.reset()\n",
    "        total_reward = 0\n",
    "        for _ in range(1000):\n",
    "            action = agent.next_action(observation)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return total_reward\n",
    "\n",
    "\n",
    "class LinearAgent:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.parameters = np.random.rand(4) * 2 - 1\n",
    "\n",
    "    def next_action(self, observation):\n",
    "        return 0 if np.matmul(self.parameters, observation) < 0 else 1\n",
    "\n",
    "\n",
    "def random_search():\n",
    "    env = gym.make('CartPole-v0')\n",
    "    best_params = None\n",
    "    best_reward = 0\n",
    "    agent = LinearAgent()\n",
    "    harness = Harness()\n",
    "\n",
    "    for step in range(10000):\n",
    "        agent.parameters = np.random.rand(4) * 2 - 1\n",
    "        reward = harness.run_episode(env, agent)\n",
    "        if reward > best_reward:\n",
    "            best_reward = reward\n",
    "            best_params = agent.parameters\n",
    "            if reward == 200:\n",
    "                print('200 achieved on step {}'.format(step))\n",
    "\n",
    "    print(best_params)\n",
    "    env.close()\n",
    "\n",
    "def hill_climbing():\n",
    "    env = gym.make('CartPole-v0')\n",
    "    noise_scaling = 0.1\n",
    "    best_params = None\n",
    "    best_reward = 0\n",
    "    agent = LinearAgent()\n",
    "    harness = Harness()\n",
    "\n",
    "    for step in range(10000):\n",
    "        new_params = agent.parameters + (np.random.rand(4) * 2 - 1) * noise_scaling\n",
    "        reward = harness.run_episode(env, agent)\n",
    "        if reward > best_reward:\n",
    "            agent.parameters = new_params\n",
    "            best_reward = reward\n",
    "            best_params = agent.parameters\n",
    "            if reward == 200:\n",
    "                print('200 achieved on step {}'.format(step))\n",
    "                break\n",
    "    print('best reward: {}'.format(best_reward))\n",
    "    print('best parameters: {}'.format(best_params))\n",
    "    env.close()\n",
    "\n",
    "random_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬坡法 (Hill Climbing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "200 achieved on step 60\n",
      "best reward: 200.0\n",
      "best parameters: [-0.23058888  0.27271016 -0.50275668  0.34309585]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger('rl')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "class Harness:\n",
    "\n",
    "    def run_episode(self, env, agent):\n",
    "        observation = env.reset()\n",
    "        total_reward = 0\n",
    "        for _ in range(1000):\n",
    "            action = agent.next_action(observation)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return total_reward\n",
    "\n",
    "\n",
    "class LinearAgent:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.parameters = np.random.rand(4) * 2 - 1\n",
    "\n",
    "    def next_action(self, observation):\n",
    "        return 0 if np.matmul(self.parameters, observation) < 0 else 1\n",
    "\n",
    "\n",
    "def random_search():\n",
    "    env = gym.make('CartPole-v0')\n",
    "    best_params = None\n",
    "    best_reward = 0\n",
    "    agent = LinearAgent()\n",
    "    harness = Harness()\n",
    "\n",
    "    for step in range(10000):\n",
    "        agent.parameters = np.random.rand(4) * 2 - 1\n",
    "        reward = harness.run_episode(env, agent)\n",
    "        if reward > best_reward:\n",
    "            best_reward = reward\n",
    "            best_params = agent.parameters\n",
    "            if reward == 200:\n",
    "                print('200 achieved on step {}'.format(step))\n",
    "\n",
    "    print(best_params)\n",
    "    env.close()\n",
    "\n",
    "def hill_climbing():\n",
    "    env = gym.make('CartPole-v0')\n",
    "    noise_scaling = 0.1\n",
    "    best_params = None\n",
    "    best_reward = 0\n",
    "    agent = LinearAgent()\n",
    "    harness = Harness()\n",
    "\n",
    "    for step in range(10000):\n",
    "        new_params = agent.parameters + (np.random.rand(4) * 2 - 1) * noise_scaling\n",
    "        reward = harness.run_episode(env, agent)\n",
    "        if reward > best_reward:\n",
    "            agent.parameters = new_params\n",
    "            best_reward = reward\n",
    "            best_params = agent.parameters\n",
    "            if reward == 200:\n",
    "                print('200 achieved on step {}'.format(step))\n",
    "                break\n",
    "    print('best reward: {}'.format(best_reward))\n",
    "    print('best parameters: {}'.format(best_params))\n",
    "    env.close()\n",
    "\n",
    "hill_climbing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初次编辑日期"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年5月13日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] https://www.udemy.com/hands-on-reinforcement-learning-with-python\n",
    "\n",
    "[2] Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J. and Zaremba, W., 2016. Openai gym. arXiv preprint arXiv:1606.01540."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

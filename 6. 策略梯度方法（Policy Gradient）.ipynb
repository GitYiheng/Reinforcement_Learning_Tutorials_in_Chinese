{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于策略的强化学习（Policy-Based Reinforcement Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 之前我们用参数$\\theta$来近似价值或者动作价值函数\n",
    "\\begin{split}\n",
    "V_{\\theta}(s) & \\approx V^{\\pi}(s) \\\\\n",
    "Q_{\\theta}(s,a) & \\approx Q^{\\pi}(s,a)\n",
    "\\end{split}\n",
    "\n",
    "- 我们直接基于价值函数生成了一个策略\n",
    "    - 也就是用$\\epsilon$-贪心\n",
    "\n",
    "- 接下来我们将会直接对策略进行参数化（directly parameterise the policy）\n",
    "\\begin{equation}\n",
    "\\pi_{\\theta}(s,a) = \\mathbb{P}[a \\mid s, \\theta]\n",
    "\\end{equation}\n",
    "\n",
    "- 无模型强化学习（model-free reinforcement learning）同样是我们的重点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于价值的和基于策略的强化学习（Value-Based and Policy-Based RL）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/value-based_and_policy-based_RL.png\" style=\"width: 300px;\" />\n",
    "\n",
    "- 基于价值的（value-based）\n",
    "    - 学习到的价值函数（learnt value function）\n",
    "    - 隐式策略（implicit policy），比如说$\\epsilon$-贪心\n",
    "- 基于策略的（policy-based）\n",
    "    - 没有价值函数\n",
    "    - 学习到的策略（learnt policy）\n",
    "- 行动者-批评者方法（Actor-Critic）\n",
    "    - 学习到的价值函数（learnt value function）\n",
    "    - 学习到的策略（learnt policy）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于策略强化学习方法的优势（Advantages of Policy-Based RL）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优势：\n",
    "\n",
    "- 更好的收敛特性\n",
    "- 在高维（high-dimensional）或连续（continuous）动作空间中更高效\n",
    "- 可以学习随机策略（stochastic policy）\n",
    "\n",
    "劣势：\n",
    "\n",
    "- 一般收敛到局部最优值而不是全局最优值（local rather than global optimum）\n",
    "- 一般来说评估一个策略效率不高并且偏差很高（inefficient and high variance）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 范例-石头剪刀布（Example: Rock-Paper-Scissors）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/example_rock-paper-scissors.png\" style=\"width: 300px;\" />\n",
    "\n",
    "- 两名玩家参与的石头剪刀\n",
    "    - 剪刀赢布\n",
    "    - 石头赢剪刀\n",
    "    - 布赢石头\n",
    "- 让我们来考虑迭代的（iterated）石头剪刀布的策略\n",
    "    - 一个确定性的策略非常容易被利用\n",
    "    - 一个均匀随机的策略是最优的（也就是纳什均衡，Nash equilibrium）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 混淆的Gridworld（Aliased Gridworld）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/aliased_gridworld_1.png\" style=\"width: 300px;\" />\n",
    "\n",
    "- 这里的智能体不能区分灰色的状态\n",
    "- 让我们考虑下面这个形式的特征（对所有N, E, S, W，也就是北、东、南、西）\n",
    "\\begin{equation}\n",
    "\\phi(s, a) = \\textbf{1}(wall\\enspace to\\enspace N,\\enspace a=move \\enspace E)\n",
    "\\end{equation}\n",
    "\n",
    "- 我们用一个近似的价值函数（approximate value function）来比较基于价值的强化学习（value-based RL）\n",
    "\\begin{equation}\n",
    "Q_{\\theta}(s, a) = f(\\phi (s, a), \\theta)\n",
    "\\end{equation}\n",
    "\n",
    "- 对于基于策略的强化学习（policy-based RL），我们使用一个参数化的策略（a parameterised policy）\n",
    "\\begin{equation}\n",
    "\\pi_{\\theta}(s,a) = g(\\phi (s, a), \\theta)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/aliased_gridworld_2.png\" style=\"width: 300px;\" />\n",
    "\n",
    "- 在混淆的情况下，一个最优的确定性策略（an optimal deterministic policy）会在下面两种策略中选择其一\n",
    "    - 在两个灰色的状态都会向W移动（用红色箭头标出来的）\n",
    "    - 在两个灰色的状态都会向E移动\n",
    "- 无论选择了哪种确定性策略，智能体都会卡住而永远到达不了钱袋的状态\n",
    "- 基于价值的强化学习方法会学习到接近于确定性的策略\n",
    "    - 也就是贪心或者是$\\epsilon$-贪心\n",
    "- 所以智能体会在走廊中徘徊很久"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/aliased_gridworld_3.png\" style=\"width: 300px;\" />\n",
    "\n",
    "- 一个最优的随机策略（an optimal stochastic policy）会在灰色的状态上随机选择向E或是W移动\n",
    "\\begin{split}\n",
    "\\pi_{\\theta}(wall\\enspace to\\enspace N\\enspace and\\enspace S,\\enspace move\\enspace E) & = 0.5 \\\\\n",
    "\\pi_{\\theta}(wall\\enspace to\\enspace N\\enspace and\\enspace S,\\enspace move\\enspace W) & = 0.5\n",
    "\\end{split}\n",
    "\n",
    "- 智能体有很高的概率在几步后就到达目标状态（goal state）\n",
    "- 基于策略的强化学习（policy-based RL）可以学习到最优的随机策略（optimal stochastic policy）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 策略目标函数（Policy Objective Function）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 目标：给定参数是$\\theta$的策略$\\pi_{\\theta}(s, a)$，我们需要找到最优的$\\theta$\n",
    "- 但我们应该怎样衡量策略$\\pi_{\\theta}$的好坏呢？\n",
    "- 在回合制的环境中（episodic environments），我们可以使用起始价值（start value）\n",
    "\\begin{equation}\n",
    "J_1 (\\theta) = V^{\\pi_{\\theta}}(s_1) = \\mathbb{E}_{\\pi_{\\theta}}[v_1]\n",
    "\\end{equation}\n",
    "\n",
    "- 在连续的环境中（continuing environments），我们可以使用平均价值（average value）\n",
    "\n",
    "\\begin{equation}\n",
    "J_{avV}(\\theta) = \\sum_s d^{\\pi_{\\theta}}(s)V^{\\pi_{\\theta}}(s)\n",
    "\\end{equation}\n",
    "\n",
    "- 或是每步的平均奖励（average reward per time-step）\n",
    "\\begin{equation}\n",
    "J_{avR}(\\theta) = \\sum_s d^{\\pi_{\\theta}}(s) \\sum_a \\pi_{\\theta}(s,a)\\mathcal{R}_s^a\n",
    "\\end{equation}\n",
    "\n",
    "- 这里的$d^{\\pi_{\\theta}}(s)$是$\\pi_{\\theta}$的马尔科夫链的静态分布（stationary distribution）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 策略优化（Policy Optimisation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 基于策略的强化学习问题（policy-based RL）就是一个优化问题（an optimisation problem）\n",
    "- 搜寻最大化$J(\\theta)$的$\\theta$\n",
    "- 有一些方法并不需要用到梯度信息\n",
    "    - 爬坡法（hill climbing）\n",
    "    - Simplex/amoeba/Nelder-Mead法\n",
    "    - 基因算法（Genetic Algorithms）\n",
    "- 在使用梯度信息时效率可能会更高\n",
    "    - 梯度下降（gradient descent）\n",
    "    - 共轭梯度法（conjugate gradient）\n",
    "    - 拟牛顿法（Quasi-Newton）\n",
    "- 我们会主要使用梯度下降法（gradient descent），当然它也有很多延伸的方法\n",
    "- 也有利用顺序结构的方法（methods that exploit sequential structure）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 策略梯度法（Policy Gradient）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $J(\\theta)$可以是任何策略目标函数（policy objective function）\n",
    "- 策略梯度算法通过沿着策略的梯度上升方向在$J(\\theta)$中搜索一个局部最大值（a local maximum）\n",
    "\\begin{equation}\n",
    "\\Delta \\theta = \\alpha \\nabla_{\\theta} J(\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "- 这个公式中的$\\nabla_{\\theta} J(\\theta)$就是策略梯度（policy gradient）\n",
    "\\begin{equation}\n",
    "\\nabla_{\\theta}J(\\theta)\n",
    "=\\left(\n",
    "\\begin{split}\n",
    "& \\frac{\\partial J(\\theta)}{\\partial \\theta_1} \\\\\n",
    "& \\quad \\vdots \\\\\n",
    "& \\frac{\\partial J(\\theta)}{\\partial \\theta_n}\n",
    "\\end{split}\n",
    "\\right)\n",
    "\\end{equation}\n",
    "\n",
    "- 这里的$\\alpha$是步长参数（a step-size parameter）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 通过有限差分法来计算梯度（Computing Gradients by Finite Differences）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们下面来评估$\\pi_{\\theta}(s,a)$的策略梯度（policy gradient）\n",
    "- 对于每个维度 $k \\in [1, n]$\n",
    "    - 估计目标函数关于$\\theta$第$k^{th}$维的偏导数\n",
    "    - 通过在$\\theta$的第$k^{th}$维改变一个很小的量$\\epsilon$\n",
    "    \\begin{equation}\n",
    "    \\frac{\\partial J(\\theta)}{\\partial \\theta_k} \\approx \\frac{J(\\theta + \\epsilon u_k) - J(\\theta)}{\\epsilon}\n",
    "    \\end{equation}\n",
    "    - 这里的$u_k$是一个在第$k^{th}$维上长度为1，其它维度上长度为0的单位向量\n",
    "- 我们会做$n$个评估（$n$ evaluations）来计算$n$维的策略梯度\n",
    "- 简单、有噪音、效率不高，但有时有效（simple, noisy, inefficient - but sometimes effective）\n",
    "- 就算策略是不可微分的，有限差分法也可以用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用有限差分策略梯度法来训练AIBO步行（Training AIBO to Walk by Finite Difference Policy Gradient）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/aibo.png\" style=\"width: 500px;\" />\n",
    "\n",
    "- 目标：让AIBO学习快速步行（在Robocup中是有用的）\n",
    "- AIBO的步行策略是由12个数字来控制的（椭圆轨迹）\n",
    "- 用有限差分策略梯度法来调整这些参数\n",
    "- 通过给穿过场地计时来评估策略的表现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIBO的步行策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 训练前\n",
    "- 训练中\n",
    "- 训练后"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评分函数（Score Function）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们现在用分析法来计算策略梯度（compute the policy gradient analytically）\n",
    "- 让我们假设策略$\\pi_{\\theta}$在非零时是可微分的\n",
    "- 并且我们知道梯度信息$\\nabla_{\\theta}\\pi_{\\theta}(s,a)$\n",
    "- 似然比（likelihood ratios）利用了下面这个恒等式\n",
    "\\begin{split}\n",
    "\\nabla_{\\theta}\\pi_{\\theta}(s,a) & = \\pi_{\\theta}(s,a)\\frac{\\nabla_{\\theta}\\pi_{\\theta}(s, a)}{\\pi_{\\theta}(s,a)} \\\\\n",
    "& = \\pi_{\\theta}(s,a)\\nabla_{\\theta} \\log \\pi_{\\theta}(s,a)\n",
    "\\end{split}\n",
    "\n",
    "- 所以评分函数（score function）就是$\\nabla_{\\theta} \\log \\pi_{\\theta}(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax策略（Softmax Policy）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们会用softmax策略作为一个运行示例\n",
    "- 我们用特征$\\phi(s,a)^{\\intercal}\\theta$的线性组合给动作加权\n",
    "- 动作的概率和指数权重（exponentiated weight）成正比\n",
    "\\begin{equation}\n",
    "\\pi_{\\theta}(s,a)\\propto e^{\\phi(s,a)^{\\intercal}\\theta}\n",
    "\\end{equation}\n",
    "\n",
    "- 评分函数是\n",
    "\\begin{equation}\n",
    "\\nabla_{\\theta} \\log \\pi_{\\theta}(s,a)=\\phi (s, a) - \\mathbb{E}_{\\pi_{\\theta}}[\\phi(s,\\cdot)]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 高斯策略（Gaussian Policy）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在连续的动作空间中，使用高斯策略是一个自然的选择\n",
    "- 均值是状态特征的线性组合（mean is a linear combination of state features）$\\mu(s)=\\phi(s)^{\\intercal}\\theta$\n",
    "- 方差可以是固定的（fixed）$\\sigma^2$，或者也可以被参数化（parameterised）\n",
    "- 策略是高斯的（policy is Gaussian），$a \\sim \\mathcal{N}(\\mu(s),\\sigma^2)$\n",
    "- 评价函数是\n",
    "\\begin{equation}\n",
    "\\nabla_{\\theta} \\log \\pi_{\\theta}(s, a) = \\frac{(a-\\mu(s))\\phi(s)}{\\sigma^2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单步MDP（One-Step MDPs）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 让我们考虑一种简单的单步MDP\n",
    "    - 起始状态$s\\sim d(s)$\n",
    "    - 在一步后停止并伴有奖励$r=\\mathcal{R}_{s, a}$\n",
    "- 用似然比（likelihood ratios）来计算策略梯度\n",
    "\\begin{split}\n",
    "J(\\theta) & = \\mathbb{E}_{\\pi_{\\theta}}[r] \\\\\n",
    "& = \\sum_{s\\in S} d(s) \\sum_{a \\in \\mathcal{A}}\\pi_{\\theta}(s,a)\\mathcal{R}_{s,a} \\\\\n",
    "\\nabla_{\\theta} J(\\theta) & = \\sum_{s\\in \\mathcal{S}}d(s) \\sum_{a \\in \\mathcal{A}}\\pi_{\\theta}(s,a)\\nabla_{\\theta}\\log \\pi_{\\theta}(s,a)\\mathcal{R}_{s,a} \\\\\n",
    "& = \\mathbb{E}_{\\pi_{\\theta}}[\\nabla_{\\theta}\\log \\pi_{\\theta}(s,a)r]\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 策略梯度定理（Policy Gradient Theorem）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 策略梯度定理把似然性方法一般化（generalise）到了多步MDP中\n",
    "- 用长期价值（long-term value）$Q^{\\pi}(s,a)$替换瞬时奖励（instantaneous reward）$r$\n",
    "- 策略梯度定理可以用到初始状态目标、平均回报和平均价值目标上（policy gradient theorem applies to start state objective, average reward and average value objective）\n",
    "\n",
    "对任何一个可微分的策略（any differentiable policy）$\\pi_{\\theta}(s,a)$来说，对任何的策略目标函数（policy objective functions）$J=J_1$、 $J_{avR}$或是$\\frac{1}{1-\\gamma}J_{avV}$我们都有策略梯度\n",
    "\\begin{equation}\n",
    "\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\pi_{\\theta}}[\\nabla_{\\theta}\\log \\pi_{\\theta}(s,a)Q^{\\pi_{\\theta}}(s,a)]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蒙特卡洛策略梯度REINFORCE（Monte-Carlo Policy Gradient REINFORCE）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用随机梯度上升（stochastic gradient ascent）来更新参数\n",
    "- 用梯度策略定理（policy gradient theorem）\n",
    "- 用回报$v_t$作为$Q^{\\pi_{\\theta}}(s_t, a_t)$的无偏样本（unbiased sample）\n",
    "\\begin{equation}\n",
    "\\Delta\\theta_t = \\alpha\\nabla_{\\theta}\\log \\pi_{\\theta}(s_t,a_t)v_t\n",
    "\\end{equation}\n",
    "\n",
    "<img src=\"files/figures/REINFORCE.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 范例-冰球世界（Puck World Example）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/puck_world_example.png\" style=\"width: 500px;\" />\n",
    "\n",
    "- 用连续动作在冰球上施加较小的力\n",
    "- 我们会对接近目标的冰球予以奖励\n",
    "- 目标位置每30秒回重置一次\n",
    "- 我们用蒙特卡洛策略梯度法的变体来训练策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用批评者来减少方差（Reducing Variance Using a Critic）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 蒙特卡洛策略梯度仍然有很高的方差\n",
    "- 我们用一个批评者（critic）来估计动作价值函数\n",
    "\\begin{equation}\n",
    "Q_w (s,a) \\approx Q^{\\pi_{\\theta}}(s,a)\n",
    "\\end{equation}\n",
    "- 行动者-批评者（actor-critic）算法拥有两组参数\n",
    "    - 批评者（critic）： 更新动作价值函数的参数$w$\n",
    "    - 行动者（actor）：更新策略的参数$\\theta$\n",
    "- 行动者-批评者算法用的是一个近似策略梯度（an approximate policy gradient）\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_{\\theta}J(\\theta) & \\approx \\mathbb{E}_{\\pi_{\\theta}}[\\nabla_{\\theta}\\log \\pi_{\\theta}(s,a)Q_w(s,a)] \\\\\n",
    "\\Delta \\theta & = \\alpha \\nabla_{\\theta}\\log \\pi_{\\theta}(s,a)Q_w(s,a)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 估计动作价值函数（Estimating the Action-Value Function）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 批评者（critic）解决的是我们熟悉的一个问题：策略评估（policy evaluation）\n",
    "- 那到底现在的参数$\\theta$对策略$\\pi_{\\theta}$来说好不好？\n",
    "- 我们之前解决这个问题的方法是：\n",
    "    - 蒙特卡洛策略评估（Monte-Carlo policy evaluation）\n",
    "    - 时差学习（Temporal-Difference learning）\n",
    "    - TD($\\lambda$)\n",
    "- 我们也能用比如最小二乘策略评估（least-squares policy evaluation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 行动者-批评者的动作价值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 简单的行动者-批评者算法是基于用动作价值函数的批评者的（action-value critic）\n",
    "- 我们使用线性价值函数近似（linear value function approximation），$Q_w (s,a)=\\phi (s,a)^{\\intercal}w$\n",
    "    - 批评者： 用线性TD(0)中更新$w$\n",
    "    - 行动者： 用策略梯度法（policy gradient）更新$\\theta$\n",
    "\n",
    "<img src=\"files/figures/action-value_actor-critic.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 行动者-批评者算法的偏差（Bias in Actor-Critic Algorithms）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对策略梯度进行近似引入了偏差（bias）\n",
    "- 一个有偏的策略梯度（a biased policy gradient）也许不能找到正确的解\n",
    "    - 如果$Q_w (s,a)$用到了失真的特征（aliased features），在这种情况下我们能解决gridworld范例吗？\n",
    "- 好消息是，如果我们小心地选择价值函数近似\n",
    "- 这样我们就能避免引入任何偏差（bias）\n",
    "- 也就是说我们可以遵循确切的策略梯度（follow the exact policy gradient）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 兼容的函数近似（Compatible Function Approximation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在下面两个条件被满足的情况下，那么我们说策略梯度是确切的（the policy gradient is exact），\n",
    "\\begin{equation}\n",
    "    \\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\pi_{\\theta}}[\\nabla_{\\theta}\\log \\pi_{\\theta}(s,a)Q_w (s,a)]\n",
    "\\end{equation}\n",
    "\n",
    "1. 价值函数近似器和策略是兼容的（value function approximator is compatible to the policy）\n",
    "\\begin{equation}\n",
    "\\nabla_w Q_w(s,a)=\\nabla_{\\theta} \\log \\pi_{\\theta}(s,a)\n",
    "\\end{equation}\n",
    "\n",
    "2. 价值函数的参数（value function parameters）$w$最小化了均方误差（mean-squared error）\n",
    "\\begin{equation}\n",
    "\\varepsilon = \\mathbb{E}_{\\pi_{\\theta}}[(Q^{\\pi_{\\theta}}(s,a)-Q_w(s,a))^2]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 兼容函数近似理论的证明（Proof of Compatible Function Approximation Theorem）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们选择了最小化均方误差的$w$，也就是说$\\varepsilon$关于$w$的梯度必须是零，\n",
    "\\begin{split}\n",
    "\\nabla_w \\varepsilon & = 0 \\\\\n",
    "\\mathbb{E}_{\\pi_{\\theta}}[(Q^{\\theta}(s,a)-Q_w (s,a))\\nabla_w Q_w (s,a)] & = 0 \\\\\n",
    "\\mathbb{E}_{\\pi_{\\theta}}[(Q^{\\theta}(s,a)-Q_w(s,a))\\nabla_{\\theta}\\log \\pi_{\\theta}(s,a)] & = 0 \\\\\n",
    "\\mathbb{E}_{\\pi_{\\theta}}[Q^{\\theta}(s,a)\\nabla_{\\theta}\\log \\pi_{\\theta}(s,a)] & = \\mathbb{E}_{\\pi_{\\theta}}[Q_w (s,a) \\nabla_{\\theta} \\log \\pi_{\\theta} (s,a)]\n",
    "\\end{split}\n",
    "\n",
    "So $Q_w (s,a)$ can be substituted directly into the policy gradient,\n",
    "\\begin{equation}\n",
    "\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\pi_{\\theta}}[\\nabla_{\\theta}\\log \\pi_{\\theta}(s,a)Q_w (s,a)]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 通过使用基线来减少方差（Reducing Variance Using a Baseline）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们从策略梯度（policy gradient）中减去一个基线函数（a baseline function）$B(s)$\n",
    "- 在不改变期望值的同时（without changing expectation），我们可以通过这种方法减少方差（reduce variance）\n",
    "\\begin{split}\n",
    "\\mathbb{E}_{\\pi_{\\theta}}[\\nabla_{\\theta} \\log \\pi_{\\theta}(s,a)B(s)] & = \\sum_{s \\in \\mathcal{S}}d^{\\pi_{\\theta}}(s)\\sum_a \\nabla_{\\theta}\\pi_{\\theta}(s,a)B(s) \\\\\n",
    "& = \\sum_{s \\in \\mathcal{S}}d^{\\pi_{\\theta}}B(s)\\nabla_{\\theta}\\sum_{a \\in \\mathcal{A}}\\pi_{\\theta}(s,a) \\\\\n",
    "& = 0\n",
    "\\end{split}\n",
    "\n",
    "- 一个比较好的基线就是状态价值函数$B(s)=V^{\\pi_{\\theta}}(s)$\n",
    "- 我们可以用优势函数（advantage function）$A^{\\pi_{\\theta}}(s,a)$来重写策略梯度（policy gradient）\n",
    "\\begin{split}\n",
    "A^{\\pi_{\\theta}}(s,a) & = Q^{\\pi_{\\theta}}(s,a) - V^{\\pi_{\\theta}}(s) \\\\\n",
    "\\nabla_{\\theta} J(\\theta) & = \\mathbb{E}_{\\pi_{\\theta}}[\\nabla_{\\theta}\\log \\pi_{\\theta}(s,a)A^{\\pi_{\\theta}}(s,a)]\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 给优势函数估值（Estimating the Advantage Function）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 优势函数可以显著的减少策略梯度的方差\n",
    "- 所以说评论者（critic）应该比较准确地给优势函数估值（really estimate the advantage function）\n",
    "- 比如说，我们可以同时估计$V^{\\pi_{\\theta}}(s)$和$Q^{\\pi_{\\theta}}(s,a)$的价值\n",
    "- 通过使用两个函数近似器和两个参数向量（two function approximators and two parameter vectors），\n",
    "\\begin{align}\n",
    "V_v(s) & \\approx V^{\\pi_{\\theta}}(s) \\\\\n",
    "Q_w (s,a) & \\approx Q^{\\pi_{\\theta}}(s,a) \\\\\n",
    "A(s,a) & = Q_w (s,a) - V_v(s)\n",
    "\\end{align}\n",
    "\n",
    "- 我们可以通过使用比如TD学习来同时更新两个价值函数（update both value functions）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 真实价值函数$V^{\\pi_{\\theta}}(s)$的TD误差$\\delta^{\\pi_{\\theta}}$是\n",
    "\\begin{equation}\n",
    "\\delta^{\\pi_{\\theta}} = r + \\gamma V^{\\pi_{\\theta}}(s') - V^{\\pi_{\\theta}}(s)\n",
    "\\end{equation}\n",
    "\n",
    "- 这个TD误差是优势函数（advantage function）的一个无偏估计（an unbiased estimate）\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{\\pi_{\\theta}}[\\delta^{\\pi_{\\theta}} \\mid s,a] & = \\mathbb{E}_{\\pi_{\\theta}}[r+\\gamma V^{\\pi_{\\theta}}(s') \\mid s,a] - V^{\\pi_{\\theta}}(s) \\\\\n",
    "& = Q^{\\pi_{\\theta}}(s,a) - V^{\\pi_{\\theta}}(s) \\\\\n",
    "& = A^{\\pi_{\\theta}}(s,a)\n",
    "\\end{align}\n",
    "\n",
    "- 所以我们可以用TD误差来计算策略梯度\n",
    "\\begin{equation}\n",
    "\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\pi_{\\theta}}[\\nabla_{\\theta}\\log \\pi_{\\theta}(s,a) \\delta^{\\pi_{\\theta}}]\n",
    "\\end{equation}\n",
    "\n",
    "- 在实际应用中我们可以使用近似的TD误差\n",
    "\\begin{equation}\n",
    "\\delta_v = r + \\gamma V_v(s') - V_v(s)\n",
    "\\end{equation}\n",
    "\n",
    "- 这种方法只需要批评者（critic）的一组参数$v$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 不同时间尺度的批评者（Critics at Different Time-Scales）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 批评者可以基于许多时间尺度不同的目标估值价值函数（estimate value function from many targets at different time-scales）$V_{\\theta}(s)$\n",
    "    - 对于蒙特卡洛来说，目标就是回报$v_t$\n",
    "    \\begin{equation}\n",
    "    \\Delta\\theta = \\alpha (v_t - V_{\\theta}(s))\\phi(s)\n",
    "    \\end{equation}\n",
    "    \n",
    "    - 对于TD(0)来说，目标就是TD目标$r+\\gamma V(s')$\n",
    "    \\begin{equation}\n",
    "    \\Delta\\theta = \\alpha (r+\\gamma V(s')-V_{\\theta}(s))\\phi (s)\n",
    "    \\end{equation}\n",
    "    \n",
    "    - 对于前视TD($\\lambda$)来说（forward-view TD($\\lambda$)），目标就是$\\lambda$-回报$v_t^{\\lambda}$\n",
    "    \\begin{equation}\n",
    "    \\Delta\\theta = \\alpha (v_t^{\\lambda}-V_{\\theta}(s))\\phi(s)\n",
    "    \\end{equation}\n",
    "    \n",
    "    - 对于后视TD($\\lambda$)来说（backward-view TD($\\lambda$)），我们使用的是有效性标记（eligibility traces）\n",
    "    \\begin{align}\n",
    "    \\delta_t & = r_{t+1} + \\gamma V(s_{t+1}) - V(s_t) \\\\\n",
    "    e_t & = \\gamma \\lambda e_{t-1} + \\phi (s_t) \\\\\n",
    "    \\Delta\\theta & = \\alpha \\delta_t e_t\n",
    "    \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 不同时间尺度的行动者（Actors at Different Time-Scales）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们也可以在不同的时间尺度上估计策略梯度\n",
    "\\begin{equation}\n",
    "\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\pi_{\\theta}}[\\nabla_{\\theta}\\log \\pi_{\\theta}(s,a) A^{\\pi_{\\theta}}(s,a)]\n",
    "\\end{equation}\n",
    "\n",
    "- 蒙特卡洛策略梯度法使用了完整回报的误差（error from complete return）\n",
    "\\begin{equation}\n",
    "\\Delta\\theta = \\alpha (v_t - V_v (s_t))\\nabla_{\\theta}\\log \\pi_{\\theta}(s_t,a_t)\n",
    "\\end{equation}\n",
    "\n",
    "- 行动者-批评者策略梯度（actor-critic policy gradient）使用了单步TD误差（one-step TD error）\n",
    "\\begin{equation}\n",
    "\\Delta\\theta = \\alpha (r+\\gamma V_v (s_{t+1})-V_v (s_t)) \\nabla_{\\theta} \\log \\pi_{\\theta} (s_t, a_t)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于有效性标记的策略梯度法（Policy Gradient with Eligibility Traces）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 就像前视TD($\\lambda$)（forward-view TD($\\lambda$)）一样，我们可以综合不同的时间尺度（mix over time-scales）\n",
    "\\begin{equation}\n",
    "\\Delta\\theta = \\alpha (v_t^{\\lambda} - V_v (s_t))\\nabla_{\\theta}\\log \\pi_{\\theta}(s_t, a_t)\n",
    "\\end{equation}\n",
    "\n",
    "- 这里的$v_t^{\\lambda} - V_v (s_t)$是优势函数的一个有偏估计（a biased estimate of advantage function）\n",
    "- 就像后视TD($\\lambda$)（backward-view TD($\\lambda$)）一样，我们也可以使用有效性标记（eligibility traces）\n",
    "    - 根据与TD($\\lambda$)的等效性，我们可以做这样的替代$\\phi (s) = \\nabla_{\\theta}\\log \\pi_{\\theta}(s,a)$\n",
    "    \\begin{align}\n",
    "    \\delta & = r_{t+1} + \\gamma V_v (s_{t+1}) - V_v (s_t) \\\\\n",
    "    e_{t+1} & = \\lambda e_t + \\nabla_{\\theta}\\log \\pi_{\\theta}(s,a) \\\\\n",
    "    \\Delta \\theta & = \\alpha \\delta e_t\n",
    "    \\end{align}\n",
    "\n",
    "- 这个可以实时地应用在不完整序列的更新上（this update can be applied online, to incomplete sequences）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 另外的策略梯度方向（Alternative Policy Gradient Directions）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 梯度上升算法可以用任何的上升方向（gradient ascent algorithms can follow any ascent direction）\n",
    "- 一个好的上升方向可以显著地提升收敛速度\n",
    "- 同样的，通常我们也可以在不改变动作概率的前提下重新参数化一个策略（reparameterise a policy without changing action probabilities）\n",
    "- 比如说，我们可以增加在一个softmax策略中所有动作的分数（increase score of all actions in a softmax policy）\n",
    "- 基础梯度（vanilla gradient）对这些重新参数化的操作比较敏感（sensitive to these reparameterisations）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自然策略梯度（Natural Policy Gradient）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/natural_policy_gradient.png\" style=\"width: 300px;\" />\n",
    "\n",
    "- 自然策略梯度（natural policy gradient）不会受到重新参数化的影响（parameterisation independent）\n",
    "- 通过固定少量地改变策略（change policy by a small, fixed amount），我们可以找到与基础梯度接近的上升方向（ascent direction that is closest to vanilla gradient）\n",
    "\\begin{equation}\n",
    "\\nabla_{\\theta}^{nat}\\pi_{\\theta}(s,a) = G_{\\theta}^{-1}\\nabla_{\\theta} \\pi_{\\theta}(s,a)\n",
    "\\end{equation}\n",
    "\n",
    "- 这里的$G_{\\theta}$是Fisher信息矩阵（Fisher information matrix）\n",
    "\\begin{equation}\n",
    "G_{\\theta} = \\mathbb{E}_{\\pi_{\\theta}}[\\nabla_{\\theta}\\log \\pi_{\\theta}(s,a)\\nabla_{\\theta}\\log \\pi_{\\theta}(s,a)^{\\intercal}]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自然行动者-批评者（Natural Actor-Critic）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用兼容的函数近似（compatible function approximation），\n",
    "\\begin{equation}\n",
    "\\nabla_w A_w (s,a) = \\nabla_{\\theta} \\log \\pi_{\\theta}(s, a)\n",
    "\\end{equation}\n",
    "\n",
    "- 所以我们可以简化策略梯度，\n",
    "\\begin{split}\n",
    "\\nabla_{\\theta} J(\\theta) & = \\mathbb{E}_{\\pi_{\\theta}}[\\nabla_{\\theta}\\log \\pi_{\\theta}(s,a) A^{\\pi_{\\theta}}(s,a)] \\\\\n",
    "& = \\mathbb{E}_{\\pi_{\\theta}}[\\nabla_{\\theta}\\log \\pi_{\\theta}(s,a) \\nabla_{\\theta}\\log \\pi_{\\theta}(s,a)^{\\intercal}w] \\\\\n",
    "& = G_{\\theta}w \\\\\n",
    "\\nabla_{\\theta}^{nat} J(\\theta) & = w\n",
    "\\end{split}\n",
    "\n",
    "- 也就是想批评者的参数方向更新行动者的参数（update actor parameters in direction of critic parameters）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在机器蛇上使用自然行动者-批评者算法（Natural Actor-Critic in Snake Domain）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/natural_actor-critic_in_snake_domain.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/natural_actor-critic_in_snake_domain_2.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/natural_actor-critic_in_snake_domain_3.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 策略梯度算法小结（Summary of Policy Gradient Algorithms）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 策略梯度（policy gradient）有许多等价的形式\n",
    "\\begin{align}\n",
    "\\nabla_{\\theta} J(\\theta) & = \\mathbb{E}_{\\pi_{\\theta}}[\\nabla_{\\theta}\\log \\pi_{\\theta}(s,a) v_t] & REINFORCE \\\\\n",
    "& = \\mathbb{E}_{\\pi_{\\theta}}[\\nabla_{\\theta}\\log \\pi_{\\theta}(s,a) Q^w (s,a)] & Q\\enspace Actor-Critic \\\\\n",
    "& = \\mathbb{E}_{\\pi_{\\theta}}[\\nabla_{\\theta}\\log \\pi_{\\theta}(s,a) A^w (s,a)] & Advantage\\enspace Actor-Critic \\\\\n",
    "& = \\mathbb{E}_{\\pi_{\\theta}}[\\nabla_{\\theta}\\log \\pi_{\\theta}(s,a) \\delta] & TD\\enspace Actor-Critic \\\\\n",
    "& = \\mathbb{E}_{\\pi_{\\theta}}[\\nabla_{\\theta}\\log \\pi_{\\theta}(s,a) \\delta e] & TD(\\lambda)\\enspace Actor-Critic \\\\\n",
    "G_{\\theta}^{-1}\\nabla_{\\theta} J(\\theta) & = w & Natural\\enspace Actor-Critic\n",
    "\\end{align}\n",
    "\n",
    "- 以上的每种形式都可以用在随机梯度上升算法（stochastic gradient ascent algorithm）上\n",
    "- 批评者（critic）用策略评价（policy evaluation，也就是MC或是TD学习）来估值$Q^{\\pi}(s,a)$、$A^{\\pi}(s,a)$或是$V^{\\pi}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最初编辑日期"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年4月23日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf\n",
    "\n",
    "[2] https://www.youtube.com/watch?v=KHZVXao4qXs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

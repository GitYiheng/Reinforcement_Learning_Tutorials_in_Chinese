{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Limitations of \"Vanilla\" Policy Gradient Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hard to choose stepsizes\n",
    "    - Input data is nonstationary due to changing policy: observation and reward distributions change\n",
    "    - Bad step is more damaging than in supervised learning, since it affects visitation distribution\n",
    "        - Step too far $\\rightarrow$ bad policy\n",
    "        - Next batch: collected under bad policy\n",
    "        - Cannot recover - collapse in performance\n",
    "- Sample efficiency\n",
    "    - Only one gradient step per environment sample\n",
    "    - Dependent on scaling of coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing Reinforcement Learning to Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Much of modern ML: reduce learning to numerical optimization problem\n",
    "    - Supervised learning: minimize training error\n",
    "- RL: how to *use all data so far and compute the best policy*?\n",
    "    - Q-learning: can (in principle) include all transitions seen so far, however, we're optimizing the wrong objective\n",
    "    - Policy gradient methods: yes stochastic gradients, but no optimization problem*\n",
    "    - This lecture: write down an optimization problem that allows you to do a small update to policy $\\pi$ based on data sampled from $\\pi$ (*on-policy* data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Loss to Optimize?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Policy gradients\n",
    "\n",
    "$$\\hat{g}=\\hat{\\mathbb{E}}_t \\left[ \\nabla_{\\theta} \\log \\pi_{\\theta} (a_t \\mid s_t) \\hat{A}_t \\right]$$\n",
    "\n",
    "- Can differentiate the following loss\n",
    "$$L^{PG}(\\theta)=\\hat{\\mathbb{E}}_t \\left[ \\log \\pi_{\\theta} (a_t \\mid s_t) \\hat{A}_t \\right]$$\n",
    "     but don't want to optimize it too far\n",
    "\n",
    "- Equivalently differentiate\n",
    "$$L_{\\theta_{old}}^{IS}(\\theta)=\\hat{\\mathbb{E}}_t \\left[ \\frac{\\pi_\\theta (a_t \\mid s_t)}{\\pi_{\\theta_{old}}(a_t \\mid s_t)} \\hat{A}_t \\right]$$\n",
    "     at $\\theta=\\theta_{old}$, state-actions are sampled using $\\theta_{old}$. (IS = importance sampling)\n",
    "     Just the chain rule: $\\nabla_{\\theta} \\log f(\\theta)\\mid_{\\theta_{old}}=\\frac{\\nabla_{\\theta} f(\\theta)\\mid_{\\theta_{old}}}{f(\\theta_{old})}=\\nabla_{\\theta}\\left(\\frac{f(\\theta)}{f(\\theta_{old})}\\right)\\mid_{\\theta_{old}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surrogate Loss: Importance Sampling Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importance sampling interpretation\n",
    "\\begin{split}\n",
    "& \\mathbb{E}_{s_t \\sim \\pi_{\\theta_{old}},a_t \\sim \\pi_{\\theta}} \\left[ A^{\\pi} (s_t, a_t) \\right] \\\\\n",
    "= & \\mathbb{E}_{s_t \\sim \\pi_{\\theta_{old}},a_t \\sim \\pi_{\\theta_{old}}} \\left[ \\frac{\\pi_{\\theta} (a_t \\mid s_t)}{\\pi_{\\theta_{old}} (a_t \\mid s_t)} A^{\\pi_{\\theta_{old}}} (s_t, a_t) \\right] \\enspace \\text{(importance sampling)} \\\\\n",
    "= & \\mathbb{E}_{s_t \\sim \\pi_{\\theta_{old}},a_t \\sim \\pi_{\\theta_{old}}} \\left[ \\frac{\\pi_{\\theta} (a_t \\mid s_t)}{\\pi_{\\theta_{old}} (a_t \\mid s_t)} \\hat{A}_t \\right] \\enspace \\text{(replace}\\enspace A^{\\pi} \\text{ with estimator)} \\\\\n",
    "= & L_{\\theta_{old}}^{IS} (\\theta)\n",
    "\\end{split}\n",
    "\n",
    "- Kakade et al. (2002) and Schulman et al. (2015) analyze how $L^{IS}$ approximates the actual performance difference between $\\theta$ and $\\theta_{old}$\n",
    "\n",
    "- In practice, $L^{IS}$ is not much different than the logprob version $L^{PG} (\\theta)=\\hat{\\mathbb{E}}_t \\left[ \\log \\pi_{\\theta} (a_t \\mid s_t) \\hat{A}_t \\right]$, for reasonably small policy changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trust Region Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define the following trust region update:\n",
    "$$\\operatorname*{maximize}_{\\theta} \\hat{\\mathbb{E}}_t \\left[ \\frac{\\pi_\\theta (a_t \\mid s_t)}{\\pi_{\\theta_{old}}(a_t \\mid s_t)} \\hat{A}_t \\right]$$\n",
    "$$\\text{subject to}\\enspace \\hat{\\mathbb{E}}_t \\left[ \\text{KL}[\\pi_{\\theta_{old}} (\\cdot \\mid s_t), \\pi_\\theta (\\cdot \\mid s_t)] \\right] \\leq \\delta$$\n",
    "\n",
    "- Also worth considering using a penalty instead of a constraint\n",
    "$$\\operatorname*{maximize}_{\\theta} \\hat{\\mathbb{E}}_t \\left[ \\frac{\\pi_\\theta (a_t \\mid s_t)}{\\pi_{\\theta_{old}}(a_t \\mid s_t)} \\hat{A}_t \\right] - \\beta \\hat{\\mathbb{E}}\\left[ \\text{KL}[\\pi_{\\theta_{old}} (\\cdot \\mid s_t), \\pi_\\theta (\\cdot \\mid s_t)] \\right]$$\n",
    "\n",
    "- Method of Lagrange multipliers: optimality point of $\\delta$-constrained problem is also an optimality point of $\\beta$-penalized problem for some $\\beta$\n",
    "\n",
    "- In practice, $\\delta$ is easier to tune, and fixed $\\delta$ is better than fixed $\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monotonic Improvement Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Consider KL penalized objective\n",
    "$$\\operatorname*{maximize}_{\\theta} \\hat{\\mathbb{E}}\\left[ \\frac{\\pi_{\\theta} (a_t \\mid s_t)}{\\pi_{\\theta_{old}} (a_t \\mid s_t)} \\hat{A}_t \\right] - \\beta \\hat{\\mathbb{E}}_t \\left[\\text{KL}[\\pi_{\\theta_{old}}(\\cdot \\mid s_t), \\pi_{\\theta}(\\cdot \\mid s_t)] \\right]$$\n",
    "\n",
    "- Theory result: if we use max KL instead of mean KL in penalty, then we get a lower (=pessimistic) bound on policy performance\n",
    "\n",
    "<img src=\"files/figures/monotonic_improvement_result.png\" style=\"width: 300px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trust Region Policy Optimization: Pseudocode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pseudocode:\n",
    "\n",
    "\\begin{split}\n",
    "& \\textbf{for}\\text{ iteration = 1, 2, ... }\\textbf{do} \\\\\n",
    "& \\qquad \\text{Run policy for $T$ timesteps or $N$ trajectories} \\\\\n",
    "& \\qquad \\text{Estimate advantage function at all timesteps} \\\\\n",
    "& \\qquad\\qquad \\operatorname*{maximize}_{\\theta} \\sum_{n=1}^{N} \\frac{\\pi_{\\theta} (a_n \\mid s_n)}{\\pi_{\\theta_{old}} (a_n \\mid s_n)} \\hat{A}_n \\\\\n",
    "& \\qquad\\qquad \\text{subject to} \\quad \\overline{KL}_{\\pi_{\\theta_{old}}} (\\pi_{\\theta}) \\leq \\delta \\\\\n",
    "& \\textbf{end for}\n",
    "\\end{split}\n",
    "\n",
    "- Can solve constrained optimization problem efficiently by using conjugate gradient\n",
    "- Closely related to natural policy gradients (Kakade, 2002), natural actor-critic (Peters and Schaal, 2005), PEPS (Peters et al., 2010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving KL Penalized Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\operatorname*{maximize}_\\theta L_{\\pi_{\\theta_{old}}}(\\pi_\\theta) - \\beta \\cdot \\overline{KL}_{\\pi_{\\theta_{old}}}(\\pi_{\\theta})$\n",
    "- Make linear approximation to $L_{\\pi_{\\theta_{old}}}$ and quadratic approximation to KL term:\n",
    "$$\\operatorname*{maximize}_\\theta g \\cdot (\\theta - \\theta_{old}) - \\frac{\\beta}{2} (\\theta - \\theta_{old})^T F (\\theta - \\theta_{old})$$\n",
    "$$\\text{where} \\enspace g=\\frac{\\partial}{\\partial \\theta}L_{\\pi_{\\theta_{old}}}(\\pi_\\theta)\\mid_{\\theta=\\theta_{old}}, \\enspace F=\\frac{\\partial^2}{\\partial^2 \\theta}\\overline{KL}_{\\pi_{\\theta_{old}}}(\\pi_\\theta)\\mid_{\\theta=\\theta_{old}}$$\n",
    "    - Quadratic part of $L$ is negligible compared to KL term\n",
    "    - $F$ is positive semidefinite, but not if we include Hessian of $L$\n",
    "- Solution: $\\theta-\\theta_{old}=\\frac{1}{\\beta}F^{-1}g$, where $F$ is Fisher information matrix, $g$ is policy gradient. This is called the **natural policy gradient** (Kakade, 2001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Suggested optimizaing surrogate loss $L^{PG}$ or $L^{IS}$\n",
    "- Suggested using KL to constrain size of update\n",
    "- Corresponds to natural gradient step $F^{-1}g$ under linear quadratic approximation\n",
    "- Can solve for this step approximately using conjugate gradient method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Proximal\" Policy Optimization: KL Penalty Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use penalty instead of constraint\n",
    "$$\\operatorname*{maximize}_\\theta \\sum_{n=1}^{N}\\frac{\\pi_\\theta (a_n \\mid s_n)}{\\pi_{\\theta_{old}} (a_n \\mid s_n)}\\hat{A}_n - C \\cdot \\overline{KL}_{\\pi_{\\theta_{old}}}(\\pi_\\theta)$$\n",
    "\n",
    "\\begin{split}\n",
    "& \\textbf{for} \\text{ iteration = 1, 2, ... } \\textbf{do} \\\\\n",
    "& \\qquad \\text{Run policy for } T \\text{ timesteps or } N \\text{ trajectories} \\\\\n",
    "& \\qquad \\text{Estimate advantage function at all timesteps} \\\\\n",
    "& \\qquad \\text{Do SGD on above objective for some number of epochs} \\\\\n",
    "& \\qquad \\text{If KL too high, increase } \\beta \\text{. If KL too low, decrease } \\beta \\text{.} \\\\\n",
    "& \\textbf{end for}\n",
    "\\end{split}\n",
    "\n",
    "- $\\approx$ same performance as TRPO, but only first-order optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connection Between Trust Region Problem and Other Things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\operatorname*{maximize}_\\theta \\sum_{n=1}^{N} \\frac{\\pi_\\theta (a_n \\mid s_n)}{\\pi_{\\theta_{old}} (a_n \\mid s_n)} \\hat{A}_n$$\n",
    "$$\\text{subject to } \\overline{KL}_{\\pi_{\\theta_{old}}}(\\pi_\\theta) \\leq \\delta$$\n",
    "\n",
    "- Linear-quadratic approximation + penalty $\\Rightarrow$ natural gradient\n",
    "- No constraint $\\Rightarrow$ policy iteration\n",
    "- Euclidean penalty instead of KL $\\Rightarrow$ vanilla policy gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations of TRPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hard to use with architectures with multiple outputs, e.g. policy and value function (need to weight different terms in distance metric)\n",
    "- Empirically performs poorly on tasks requiring deep CNNs and RNNs, e.g. Atari benchmark\n",
    "- CG makes implementation more complicated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Natural Gradient Step with KFAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Summary: do blockwise approximation to FIM, and approximate each block using a certain factorization\n",
    "- Alternate expression for FIM as outer product (instead of second deriv. of KL)\n",
    "$$\\hat{\\mathbb{E}}_t \\left[ \\nabla_\\theta \\log \\pi_\\theta (a_t \\mid s_t)^T \\nabla_\\theta \\log \\pi_\\theta (a_t \\mid s_t) \\right]$$\n",
    "\n",
    "Grosse and Martens (2016)\n",
    "Martens and Grosse (2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初次编辑日期 (Initial Edit Date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年5月30日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献 (References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Deep RL Bootcamp Lecture 5: Natural Policy Gradients, TRPO, PPO, *YouTube*, https://www.youtube.com/watch?v=xvRrgxcpaHY"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 探索与应用困境（Exploration vs Exploitation Dilemma）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 实时决策都涉及一个根本性的选择（online decision-making involves a fundamental choice）：\n",
    "    - 应用（exploitation）：在现有信息的基础上做出最佳决策（make the best decision given current information）\n",
    "    - 探索（exploration）：收集更多的信息（gather more information）\n",
    "- 最佳的长期策略可能涉及牺牲短期利益（the best long-term strategy may involve short-term sacrifices）\n",
    "- 收集足够的信息来做出最好的全局决策（gather enough information to make the best overall decisions）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 范例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 餐厅的选择（restaurant selection）\n",
    "    - 应用：去你最喜欢的饭店\n",
    "    - 探索：尝试新的饭店\n",
    "- 实时横幅广告（online banner advertisements）\n",
    "    - 应用：展示最成功的广告\n",
    "    - 探索：展示不同的广告\n",
    "- 石油钻探（oil drilling）\n",
    "    - 应用：在最佳已知位置钻探\n",
    "    - 探索：在新的位置钻探\n",
    "- 玩游戏（game playing）\n",
    "    - 应用：用你相信最好的策略\n",
    "    - 探索：采用试验性的策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 原理（Principles）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 朴素探索策略（naive exploration）\n",
    "    - 向贪心策略中加入噪音项，比如$\\epsilon$-贪心策略（add noise to greedy policy, like $\\epsilon - greedy$）\n",
    "- 乐观初始化策略（optimistic initialisation）\n",
    "    - 用优于真实值的估计值作为初始值，直到用真实样本来更新它们（assume the best until proven otherwise）\n",
    "- 面对不确定性时的乐观策略（optimism in the face of uncertainty）\n",
    "    - 倾向选择不确定性大的动作（prefer actions with uncertain values）\n",
    "- 概率匹配策略（probability matching）\n",
    "    - 根据每个动作可能成为最优选择的概率来进行选择（select actions according to probability they are best）\n",
    "- 信息状态搜索策略（information state search）\n",
    "    - 基于信息价值的前瞻预测搜索（lookahead search incorporating value of information）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多臂老虎机问题（The Multi-Armed Bandit）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/the_multi-armed_bandit.png\" style=\"width: 300px;\" />\n",
    "\n",
    "- 一个多臂老虎机是一个元组（a tuple）$<\\mathcal{A}, \\mathcal{R}>$\n",
    "- $\\mathcal{A}$是一个含$m$个动作的已知集合（a known set）\n",
    "- $\\mathcal{R}^a (r) = \\mathbb{P}[r \\mid a]$是一个奖励的未知概率分布（an unknown probability distribution over rewards）\n",
    "- 在每一时刻$t$智能体都会选择一个动作$a_t \\in \\mathcal{A}$\n",
    "- 环境会生成一个奖励（a reward）$r_t \\sim \\mathcal{R}^{a_t}$\n",
    "- 我们的目标就是最大化累计奖励（maximise cumulative reward）$\\sum_{\\tau =1}^t r_{\\tau}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 遗憾（Regret）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 动作价值是动作$a$的平均奖励\n",
    "\\begin{equation}\n",
    "Q(a)=\\mathbb{E}[r\\mid a]\n",
    "\\end{equation}\n",
    "\n",
    "- 最优价值$V^{\\ast}$是\n",
    "\\begin{equation}\n",
    "V^{\\ast}=Q(a^{\\ast})=\\max_{a\\in\\mathcal{A}}Q(a)\n",
    "\\end{equation}\n",
    "\n",
    "- 遗憾（regret）是单步的机会损失（opportunity loss for one step）\n",
    "\\begin{equation}\n",
    "I_t = \\mathbb{E}[V^{\\ast}-Q(a_t)]\n",
    "\\end{equation}\n",
    "\n",
    "- 总遗憾（total regret）是总机会损失（total opportunity loss）\n",
    "\\begin{equation}\n",
    "L_t = \\mathbb{E}\\left[\\sum_{\\tau = 1}^t V^{\\ast} - Q(a_{\\tau})\\right]\n",
    "\\end{equation}\n",
    "\n",
    "- 最大化累计奖励（maximise cumulative reward）$\\equiv$最小化总遗憾（minimise total regret）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 给遗憾计数（Counting Regret）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 计数（count）$N_t (a)$是选择动作$a$的期望次数（expected number of selections for action $a$）\n",
    "- 间隙（gap）$\\Delta_a$是动作$a$和最优动作$a^{\\ast}$之间的价值差，$\\Delta_a = V^{\\ast} - Q(a)$\n",
    "- 遗憾是一个关于间隙和计数的函数（regret is a function of gaps and the counts）\n",
    "\\begin{align}\n",
    "L_t & = \\mathbb{E}\\left[ \\sum_{\\tau =1}^t V^{\\ast}-Q(a_{\\tau}) \\right] \\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}}\\mathbb{E}[N_t (a)](V^{\\ast}-Q(a)) \\\\\n",
    "& = \\sum_{a \\in \\mathcal{A}}\\mathbb{E}[N_t (a)]\\Delta_a\n",
    "\\end{align}\n",
    "\n",
    "- 一个好的算法会确保大间隙的计数很少（small counts for large gaps）\n",
    "- 问题：间隙是未知的！（gaps are not known!）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性或次线性的遗憾（Linear or Sublinear Regret）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/linear_or_sublinear_regret.png\" style=\"width: 500px;\" />\n",
    "\n",
    "- 如果一个算法持续地探索（forever explores），它会有一个线性的总遗憾（linear total regret）\n",
    "- 如果一个算法从不探索（never explores），它会有一个线性的总遗憾（linear total regret）\n",
    "- 那么我们可不可能取得一个次线性的总遗憾（sublinear total regret）呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 贪心算法（Greedy Algorithm）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 让我们来考虑估值$\\hat{Q}_t (a) \\approx Q(a)$的算法\n",
    "- 用蒙特卡洛评估法来估计每个动作的价值（estimate the value of each action by Monte-Carlo evaluation）\n",
    "\\begin{equation}\n",
    "\\hat{Q}_t (a) = \\frac{1}{N_t (a)}\\sum_{t=1}^T r_t \\textbf{1} (a_t = a)\n",
    "\\end{equation}\n",
    "\n",
    "- 贪心算法选择价值最高的动作（the greedy algorithm selects action with highest value）\n",
    "\\begin{equation}\n",
    "a_t^{\\ast} = \\operatorname*{argmax}_{a \\in \\mathcal{A}}\\hat{Q}_t (a)\n",
    "\\end{equation}\n",
    "\n",
    "- 贪心策略会一直停留在次优动作的选项上（greedy can lock onto a suboptimal action forever）\n",
    "- $\\Rightarrow$ 贪心策略的总遗憾是线性的（greedy has linear total regret）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\epsilon$-贪心算法（$\\epsilon$-Greedy Algorithm）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\epsilon$-贪心算法会持续探索（continues to explore forever）\n",
    "    - 选择$a=\\operatorname*{argmax}_{a\\in\\mathcal{A}}\\hat{Q}(a)$的概率为$1-\\epsilon$\n",
    "    - 选择随机动作的概率为$\\epsilon$\n",
    "- 恒定的$\\epsilon$保证了最小遗憾（constant $\\epsilon$ ensures minimum regret）\n",
    "\\begin{equation}\n",
    "I_t \\geq \\frac{\\epsilon}{\\mathcal{A}}\\sum_{a\\in\\mathcal{A}}\\Delta_a\n",
    "\\end{equation}\n",
    "\n",
    "- $\\Rightarrow$ $\\epsilon$-贪心策略的总遗憾是线性的（$\\epsilon$-greedy has linear total regret）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 乐观初始化策略（Optimistic Initialisation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这是一个简单并且实用的思想（simple and practical idea）：用高于真实值的初始值来对$Q(a)$进行初始化（initialise $Q(a)$ to high value）\n",
    "- 用渐进式蒙特卡洛评估来更新动作值（update action value by incremental Monte-Carlo evaluation）\n",
    "- 用$N(a) > 0$作为开始\n",
    "\\begin{equation}\n",
    "\\hat{Q}_t(a_t) = \\hat{Q}_{t-1}+\\frac{1}{N_t(a_t)}(r_t - \\hat{Q}_{t-1})\n",
    "\\end{equation}\n",
    "\n",
    "- 促使早期的系统性探索（encourages systematic exploration early on）\n",
    "- 但仍旧可能停留在次优动作上（lock onto suboptimal action）\n",
    "- $\\Rightarrow$ 贪心策略加上乐观初始化的总遗憾是线性的（linear total regret）\n",
    "- $\\Rightarrow$ $\\epsilon$-贪心策略加上乐观初始化的总遗憾是线性的（linear total regret）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 衰减$\\epsilon_t$-贪心算法（Decaying $\\epsilon_t$-Greedy Algorithm）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 选择衰减的进程 $\\epsilon_1, \\epsilon_2, \\ldots$（pick a decay schedule for $\\epsilon_1, \\epsilon_2, \\ldots$）\n",
    "- 让我们来考虑下面这个进程（schedule）\n",
    "\\begin{align}\n",
    "c & > 0 \\\\\n",
    "d & = \\min_{a \\mid \\Delta_a > 0} \\Delta_i \\\\\n",
    "\\epsilon_t & = \\min \\left\\{ 1, \\frac{c \\lvert \\mathcal{A} \\rvert}{d^2 t} \\right\\}\n",
    "\\end{align}\n",
    "\n",
    "- 衰减$\\epsilon_t$-贪心策略的总遗憾是对数渐进的（logarithmic asymptotic total regret）！\n",
    "- 坏消息是，进程的设置需要更多关于间隙的信息（schedule requires advance knowledge of gaps）\n",
    "- 目标：（在没有关于$\\mathcal{R}$的信息的情况下）寻找一种对任何多臂老虎机来说遗憾都是次线性的算法（find an algorithm with sublinear regret for any multi-armed bandit without knowledge of $\\mathcal{R}$）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下限（Lower Bound）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 任何算法的性能都是由最优手柄和其他手柄之间的的相似性决定的（the performance of any algorithm is determined by similarity between optimal arm and other arms）\n",
    "- 难题会有奖励均值不同但看上去相似的手柄（hard problems have similar-looking arms with different means）\n",
    "- 这可以用间隙（gap）$\\Delta_a$和分布相似性（similarity in distributions）$KL(\\mathcal{R}^a \\| \\mathcal{R}^{a^{\\ast}})$进行形式化的描述\n",
    "\n",
    "渐进总遗憾与步数的关系至少是高于其对数的（asymptotic total regret is at least logarithmic in number of steps）\n",
    "\\begin{equation}\n",
    "\\lim_{t \\rightarrow \\infty} L_t \\geq \\log t \\sum_{a \\mid \\Delta_a > 0} \\frac{\\Delta_a}{KL(\\mathcal{R}^a \\| \\mathcal{R}^{a^{\\ast}})}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 面对不确定性时的乐观策略（Optimism in the Face of Uncertainty）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/optimism_in_the_face_of_uncertainty.png\" style=\"width: 500px;\" />\n",
    "\n",
    "- 那我们应该选择哪个动作呢？\n",
    "- 一个动作价值的不确定性越高（more uncertain we are about an action-value）\n",
    "- 我们探索这个动作的重要性就越高（more important it is to explore that action）\n",
    "- 这个动作可能就是最优动作（it could turn out to be the best action）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在选择蓝色动作后\n",
    "- 我们对它的价值的不确定性就会减少\n",
    "- 也更有可能选择另一个动作\n",
    "- 直到我们锁定在最佳动作上（home in on best action）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 置信上界（Upper Confidence Bounds）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对每个动作价值我们都对它的置信上界$\\hat{U}_t (a)$进行估值\n",
    "- 从而使得$Q(a)\\leq \\hat{Q}_t (a) + \\hat{U}_t (a)$有较高的概率\n",
    "- 这取决于被选择的次数$N(a)$\n",
    "    - 当$N_t (a)$较小时 $\\Rightarrow$ $\\hat{U}_t (a)$会较大（估值的不确定性较大）\n",
    "    - 当$N_t (a)$较大时 $\\Rightarrow$ $\\hat{U}_t (a)$会较小（估值会比较准确）\n",
    "- 选择置信上界（Upper Confidence Bound, or UCB）最大的动作\n",
    "\\begin{equation}\n",
    "a_t = \\operatorname*{argmax}_{a \\in \\mathcal{A}} \\hat{Q}_t (a) + \\hat{U}_t (a)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hoeffding不等式（Hoeffding's Inequality）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们假设 $X_1, \\ldots, X_t$ 都是在$[0,1]$区间中iid的随机变量（independent and identically distributed random variables），样本均值（sample mean）为$\\overline{X}_t = \\frac{1}{\\tau}\\sum_{\\tau = 1}^t X_{\\tau}$。这样的话\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{P}\\left[ \\mathbb{E}[X] > \\overline{X}_t + u \\right] \\leq e^{-2t u^2}\n",
    "\\end{equation}\n",
    "\n",
    "- 我们会在老虎机的奖励上使用Hoeffding不等式（apply Hoeffding's Inequality to rewards of the bandit）\n",
    "- 以选择动作$a$为条件\n",
    "\\begin{equation}\n",
    "\\mathbb{P}\\left[ Q(a) > \\hat{Q}_t (a) + U_t (a) \\right] \\leq e^{-2 N_t (a) U_t (a)^2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算置信上界（Calculating Upper Confidence Bounds）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 选择真实价值超过UCB的一个概率$p$\n",
    "- 解$U_t (a)$\n",
    "\\begin{align}\n",
    "e^{-2 N_t (a) U_t (a)^2} & = p \\\\\n",
    "U_t (a) & = \\sqrt{\\frac{-\\log p}{2 N_t (a)}}\n",
    "\\end{align}\n",
    "\n",
    "- 当我们观察到更多的奖励时减少$p$，比如说 $p=t^{-4}$\n",
    "- 确保当$t \\rightarrow \\infty$时我们选择最优动作的表达式是\n",
    "\\begin{equation}\n",
    "U_t (a) = \\sqrt{\\frac{2\\log t}{N_t (a)}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCB1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这也导出了UCB1算法\n",
    "\\begin{equation}\n",
    "a_t = \\operatorname*{argmax}_{a\\in \\mathcal{A}} Q(a) + \\sqrt{\\frac{2\\log t}{N_t (a)}}\n",
    "\\end{equation}\n",
    "\n",
    "UCB算法的总遗憾是对数渐进的（the UCB algorithm achieves logarithmic asymptotic total regret）\n",
    "\n",
    "\\begin{equation}\n",
    "\\lim_{t \\rightarrow \\infty} L_t \\leq 8 \\log t \\sum_{a \\mid \\Delta_a > 0} \\Delta_a\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 范例-UCB和$\\epsilon$-贪心策略在10臂老虎机上的比较（Example: UCB vs $\\epsilon$-Greedy on 10-Armed Bandit）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/example_ucb_vs_epsilon-greedy_on_10-armed_bandit.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 贝叶斯老虎机策略（Bayesian Bandits）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 到现在为止我们都没有对奖励分布$\\mathcal{R}$做任何假设（no assumptions about the reward distribution $\\mathcal{R}$）\n",
    "    - 除了在奖励范围上的假设（except bounds on rewards）\n",
    "- 贝叶斯老虎机策略就应用了关于奖励的先验知识（prior knowledge of rewards），$p[\\mathcal{R}]$\n",
    "- 贝叶斯老虎机策略在这个基础上计算了奖励的后验分布（posterior distribution of rewards）$p[\\mathcal{R}\\mid h_t]$\n",
    "    - 这里的历史记录（history）被记作$h_t = a_1, r_1,\\ldots,a_{t-1},r_{t-1}$\n",
    "- 利用得到的后验信息来指导接下来的探索（use posterior to guide exploration）\n",
    "    - 贝叶斯置信上界策略（upper confidence bounds, or namely Bayesian UCB）\n",
    "    - 概率匹配策略，或者叫汤普森取样策略（probability matching, or namely Thompson sampling）\n",
    "- 先验信息越准确，策略的性能就越好（better performance if prior knowledge is accurate）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 贝叶斯UCB的范例：独立高斯分布（Bayesian UCB Example: Independent Gaussians）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 让我们假设奖励是遵循高斯分布的，$\\mathcal{R}_a (r) = \\mathcal{N} (r;\\mu_a \\sigma_a^2)$\n",
    "<img src=\"files/figures/bayesian_ucb_example_independent_gaussians.png\" style=\"width: 500px;\" />\n",
    "\n",
    "- 我们接下来（用贝叶斯法则）计算用$\\mu_a$和$\\sigma_a^2$来描述的高斯后验\n",
    "\\begin{equation}\n",
    "p[\\mu_a,\\sigma_a^2\\mid h_t] \\propto p[\\mu_a,\\sigma_a^2] \\prod_{t\\mid a_t=a}\\mathcal{N}(r_t;\\mu_a,\\sigma_a^2)\n",
    "\\end{equation}\n",
    "\n",
    "- 选择使$Q(a)$的标准差最大的动作\n",
    "\\begin{equation}\n",
    "a_t = \\operatorname*{argmax}_{a\\in\\mathcal{A}}\\mu_a +\\frac{c\\sigma_a}{\\sqrt{N(a)}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概率匹配策略（Probability Matching）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 概率匹配策略会根据动作$a$是最优动作的概率来对动作进行选择\n",
    "\\begin{equation}\n",
    "\\pi (a\\mid h_t) = \\mathbb{P}[Q(a) > Q(a'),\\forall a' \\neq a \\mid h_t]\n",
    "\\end{equation}\n",
    "\n",
    "- 概率匹配策略是一个在面对不确定性时的乐观策略（probability matching is optimistic in the face of uncertainty）\n",
    "    - 不确定性大的动作取值最大的概率更高（uncertain actions have higher probability of being max）\n",
    "- 用后验信息来计算出解析解的难度较大（can be difficult to compute analytically from posterior）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 汤普森取样策略（Thompson Sampling）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 汤普森取样策略实现了概率匹配\n",
    "\\begin{align}\n",
    "\\pi (a\\mid h_t) & = \\mathbb{P}[Q(a) > Q(a'),\\forall a' \\neq a \\mid h_t] \\\\\n",
    "& = \\mathbb{E}_{\\mathcal{R}\\mid h_t}\\left[ \\textbf{1}(a=\\operatorname*{argmax}_{a\\in\\mathcal{A}}Q(a)) \\right]\n",
    "\\end{align}\n",
    "\n",
    "- 用贝叶斯法则来计算后验分布$p[\\mathcal{R}\\mid h_t]$\n",
    "- 从后验信息中取样（sample）一个奖励分布$\\mathcal{R}$\n",
    "- 计算动作价值函数$Q(a)=\\mathbb{E}[\\mathcal{R}_a]$\n",
    "- 选择最大化样本价值的动作（select action maximising value on sample），$a_t = \\operatorname*{argmax}$_{a\\in \\mathcal{A}}Q(a)\n",
    "\n",
    "- 汤普森取样策略达到了Lai和Robbins的下限！（Thompson sampling achieves Lai and Robbins lower bound!）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 信息的价值（Value of Information）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 因为探索能获取更多的信息所以是非常有价值的（exploration is useful because it gains information）\n",
    "- 我们能够定量地描述信息的价值吗（quantify the value of information）？\n",
    "    - 那么决策者在决策前会用多少奖励来换取信息呢\n",
    "    - 在获取信息后的长期奖励 - 即时奖励\n",
    "- 不确定情况下的信息增益会更大（information gain is higher in uncertain situations）\n",
    "- 因此更多地探索不确定的情况是在理的\n",
    "- 如果我们知道了怎样定义信息的价值，那么我们就可以用最好的方式在探索和应用中作出权衡（if we know value of information, we can trade-off exploration and exploitation optimally）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 信息状态空间（Information State Space）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们把老虎机看作是一个单步决策问题（view bandits as one-step decision-making problems）\n",
    "- 也可以看作是一个序列决策问题（sequential decision-making problems）\n",
    "- 每一步时我们都有一个信息状态（information state）$\\tilde{s}$\n",
    "    - $\\tilde{s}$是一个关于历史信息的统计量（statistic of the history），$\\tilde{s}_t = f(h_t)$\n",
    "    - 概述了到现在为止累计的所有信息（summarise all information accumulated so far）\n",
    "- 每个动作$a$都有$\\tilde{\\mathcal{P}}_{\\tilde{s},\\tilde{s}'}^a$的概率会（通过增加信息量）导致向新的信息状态$\\tilde{s}'$的转移（each action $a$ causes a transition to a new information state $\\tilde{s}'$ by adding information, with probability $\\tilde{\\mathcal{P}}_{\\tilde{s},\\tilde{s}'}^a$）\n",
    "- 这定义了一个在增广信息状态空间（augmented information state space）上的MDP$\\tilde{M}$\n",
    "\\begin{equation}\n",
    "\\tilde{\\mathcal{M}}=<\\tilde{\\mathcal{S}},\\mathcal{A},\\tilde{\\mathcal{P}},\\mathcal{R},\\gamma>\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 范例：伯努利老虎机问题（Example: Bernoulli Bandits）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 让我们来考虑一个伯努利老虎机问题（a Bernoulli Bandits），$\\mathcal{R}^a=\\mathcal{B}(\\mu_a)$\n",
    "- 比如说赢或者输的概率是$\\mu_a$\n",
    "- 我们想要找出哪只手柄对应着最高的$\\mu_a$\n",
    "- 信息状态（information state）是$\\tilde{s}=<\\alpha,\\beta>$\n",
    "    - $\\alpha_a$记录了拉动手柄并获得奖励是0的次数\n",
    "    - $\\beta_a$记录了拉动手柄并获得奖励是1的次数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 解信息状态空间的老虎机问题（Solving Information State Space Bandits）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们现在有一个在信息状态上的无限MDP（an infinite MDP over information states）\n",
    "- 这个MDP可以用强化学习来解\n",
    "- 无模型强化学习（model-free reinforcement learning）\n",
    "    - 比如Q-learning（Duff, 1994）\n",
    "- 基于模型的贝叶斯强化学习（Bayesian model-based reinforcement learning）\n",
    "    - 比如Gittins indices（Gittins, 1979）\n",
    "    - 这种方法也被称为贝叶斯自适应强化学习（Bayes-adaptive RL）\n",
    "    - 基于先验分布我们会找到贝叶斯最优的探索与应用之间的权衡（find Bayes-optimal exploration/exploitation trade-off with respect to prior distribution）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 贝叶斯自适应伯努利老虎机策略（Bayes-Adaptive Bernoulli Bandits）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/bayes-adaptive_bernoulli_bandits.png\" style=\"width: 400px;\" />\n",
    "\n",
    "- 我们会从用$Beta(\\alpha_a,\\beta_a)$作为先验的奖励函数$\\mathcal{R}_a$开始\n",
    "- 我们会在每次动作$a$被选择后对后验$\\mathcal{R}^a$进行更新\n",
    "    - 当$r=0$时$Beta(\\alpha_a + 1,\\beta_a)$\n",
    "    - 当$r=1$时$Beta(\\alpha_a,\\beta_a + 1)$\n",
    "- 这位贝叶斯自适应MDP（Bayes-adaptive MDP）定义了转移函数$\\tilde{\\mathcal{P}}$\n",
    "- 信息状态$<\\alpha,\\beta>$对应了奖励模型$Beta(\\alpha,\\beta)$\n",
    "- 每一次的状态转移都对应着一次贝叶斯模型的更新（each state transition corresponds to a Bayesian model update）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 伯努利老虎机问题的贝叶斯自适应MDP形式（Bayes-Adaptive MDP for Bernoulli Bandits）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/bayes-adaptive_mdp_for_bernoulli_bandits.png\" style=\"width: 600px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gittins Indices在伯努利老虎机问题上的应用（Gittins Indices for Bernoulli Bandits）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们可以用动态规划来解贝叶斯自适应MDP问题\n",
    "- 这个解叫作Gittins index\n",
    "- 我们通常难以得到贝叶斯自适应MDP问题的精确解（exact solution is typically intractable）\n",
    "    - 信息状态空间过于庞大（information state space is too large）\n",
    "- 最近的新思想：应用基于模拟的搜索（Guez et al., 2012）\n",
    "    - 在信息状态空间中进行前向搜索\n",
    "    - 从当前的信息状态开始进行模拟"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于场景的老虎机问题（Contextual Bandits）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/contextual_bandits.png\" style=\"width: 200px;\" />\n",
    "\n",
    "- 基于场景的老虎机问题是一个元组（a contextual bandit is a tuple）$<\\mathcal{A},\\mathcal{S},\\mathcal{R}>$\n",
    "- $\\mathcal{A}$是一个动作（或者说是手柄）的已知集合\n",
    "- $\\mathcal{S}=\\mathbb{P}[s]$是一个状态（或者说是场景）的未知分布\n",
    "- $\\mathcal{R}_s^a(s)=\\mathbb{P}[r\\mid s,a]$是一个关于奖励的未知概率分布\n",
    "- 在每一步$t$\n",
    "    - 环境会生成状态（environment generates state）$s_t \\sim \\mathcal{S}$\n",
    "    - 智能体会选择动作（agent selects action）$a_t \\in \\mathcal{A}$\n",
    "    - 环境会生成奖励（environement generates reward）$r_t \\sim \\mathcal{R}_{s_t}^{a_t}$\n",
    "- 目标是最大化累计奖励（goal is to maximise cumulative reward）$\\sum_{\\tau =1}^t r_{\\tau}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归（Linear Regression）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 动作价值函数是状态$s$和动作$a$的期望奖励\n",
    "\\begin{equation}\n",
    "Q(s,a)=\\mathbb{E}[r\\mid s,a]\n",
    "\\end{equation}\n",
    "\n",
    "- 用一个线性函数近似器来估值价值函数（estimate value function with a linear function approximator）\n",
    "\\begin{equation}\n",
    "Q_{\\theta}(s,a)=\\phi(s,a)^{\\intercal}\\theta\\approx Q(s,a)\n",
    "\\end{equation}\n",
    "\n",
    "- 用最小二乘回归来估计参数（estimate parameters by least squares regression）\n",
    "\\begin{align}\n",
    "A_t & = \\sum_{\\tau =1}^t \\phi (s_{\\tau},a_{\\tau})\\phi (s_{\\tau}, a_{\\tau})^{\\intercal} \\\\\n",
    "b_t & = \\sum_{\\tau =1}^t \\phi (s_{\\tau}, a_{\\tau}) r_{\\tau} \\\\\n",
    "\\theta_t & = A_t^{-1} b_t\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性置信上界（Linear Upper Confidence Bounds）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 最小二乘回归估计了平均动作价值（least squares regression estimates the mean action-value）$Q_{\\theta}(s,a)$\n",
    "- 但是我们同样也可以估计动作价值的方差（but it can also estimate the variance of the action-value）$\\sigma_{\\theta}^2 (s,a)$\n",
    "- 也就是由参数估计误差导致的不确定性（the uncertainty due to parameter estimation error）\n",
    "- 我们会增加描述不确定性的额外项（add on a bonus for uncertainty），$U_{\\theta}(s,a)=c\\sigma$\n",
    "- 我们把UCB定义在平均值之上$c$乘以标准差的位置（define UCB to be $c$ standard deviations above the mean）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 几何解释（Geometric Interpretation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/geometric_interpretation.png\" style=\"width: 300px;\" />\n",
    "\n",
    "- 让我们在参数（parameters）$\\theta_t$上定义一个置信度椭球（confidence ellipsoid）$\\mathcal{E}_t$\n",
    "- 用这个椭球来估计动作价值的不确定性（use this ellipsoid to estimate the uncertainty of action values）\n",
    "- 我们在这个椭球中选择最大化动作价值的参数（pick parameters within ellipsoid that maximise action value）\n",
    "\\begin{equation}\n",
    "\\operatorname*{argmax}_{\\theta \\in \\mathcal{E}} Q_{\\theta}(s,a)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算线性置信上界（Calculating Linear Upper Confidence Bounds）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对于最小二乘回归（least squares regression）来说，参数的协方差（parameter covariance）是$A^{-1}$\n",
    "- 动作价值在特征上是线性的（action-value is linear in features），$Q_{\\theta}(s,a)=\\phi (s,a)^{\\intercal}\\theta$\n",
    "- 置信上界（upper confidence bound）是$Q_{\\theta}(s,a)+c\\sqrt{\\phi(s,a)^{\\intercal}A^{-1}\\phi (s,a)}$\n",
    "- 我么选择最大化置信上界的动作（select action maximising upper confidence bound）\n",
    "\\begin{equation}\n",
    "a_t = \\operatorname*{argmax}_{a\\in\\mathcal{A}} Q_{\\theta}(s_t,a)+c\\sqrt{\\phi(s,a)^{\\intercal}A^{-1}\\phi (s,a)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 范例：用线性UCB选择头条新闻（Example: Linear UCB for Selecting Front Page News）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/example_linear_ucb_for_selecting_front_page_news.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDP中探索与应用的原理（Exploration/Exploitation Principles to MDPs）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "探索与应用的原理同样也适用于MDP（the same principles for exploration/exploitation apply to MDPs）\n",
    "\n",
    "- 朴素探索策略（naive exploration）\n",
    "- 乐观初始化策略（optimistic initialisation）\n",
    "- 面对不确定性时的乐观策略（optimism in the face of uncertainty）\n",
    "- 概率匹配策略（probability matching）\n",
    "- 信息状态搜索策略（information state search）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 乐观初始化策略：无模型强化学习（Optimistic Initialisation: Model-Free RL）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 把动作价值函数$Q(s,a)$初始化为$\\frac{r_{max}}{1-\\gamma}$\n",
    "- 运行你最喜欢的无模型强化学习算法\n",
    "    - 蒙特卡洛控制（Monte-Carlo control）\n",
    "    - Sarsa\n",
    "    - Q-learning\n",
    "    - $\\ldots$\n",
    "- 我们鼓励对状态和动作的系统探索（encourages systematic exploration of states and actions）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 乐观初始化策略：基于模型的强化学习（Optimistic Initialisation: Model-Based RL）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 构建关于MDP的一个乐观的模型（construct an optimistic model of the MDP）\n",
    "- 把未知和估计不准的状态的初始化为一个过于乐观的值\n",
    "    - 也就是把价值初始化为转移到终止状态的奖励$r_{max}$\n",
    "- 用你最喜欢的规划算法来解这个过乐观的MDP（solve optimistic MDP by favourite planning algorithm）\n",
    "    - 策略迭代（policy iteration）\n",
    "    - 价值迭代（value iteration）\n",
    "    - 树搜索（tree search）\n",
    "    - $\\ldots$\n",
    "- 我们鼓励对状态和动作的系统探索（encourages systematic exploration of states and actions）\n",
    "- 比如RMax算法（Brafman and Tennenholtz）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 置信上界：无模型强化学习（Upper Confidence Bounds: Model-Free RL）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在动作价值函数$Q^{\\pi}(s,a)$上最大化UCB\n",
    "\\begin{equation}\n",
    "a_t = \\operatorname*{a\\in\\mathcal{A}} Q(s_t,a)+U(s_t,a)\n",
    "\\end{equation}\n",
    "\n",
    "    - 在策略评估中估计不确定性（简单的）（estimate uncertainty in policy evaluation）\n",
    "    - 忽略策略改进中的不确定性（ignores uncertainty from policy improvement）\n",
    "\n",
    "- 在最优动作价值函数$Q^{\\ast}(s,a)$上最大化UCB\n",
    "\\begin{equation}\n",
    "a_t = \\operatorname*{argmax}_{a\\in\\mathcal{A}}Q(s_t,a)+U_1 (s_t,a)+U_2(s_t,a)\n",
    "\\end{equation}\n",
    "\n",
    "    - 在策略评估中估计不确定性（简单的）（estimate uncertainty in policy evaluation）\n",
    "    - 加上在策略改进中估计不确定性（困难的）（plus uncertainty from policy improvement）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 贝叶斯基于模型的强化学习（Bayesian Model-Based RL）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 维护在MDP模型上的后验分布\n",
    "- 估计转移和奖励，$p[\\mathcal{P},\\mathcal{R}\\mid h_t]$\n",
    "    - 这里的历史记录是$h_t = s_1, a_1, r_2, \\ldots, s_t$\n",
    "- 用后验信息来引导探索（use posterior to guide exploration）\n",
    "    - 置信上界，比如贝叶斯UCB（upper confidence bounds, like Bayesian UCB）\n",
    "    - 概率匹配，比如汤普森取样（probability matching, like Thompson sampling）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 汤普森取样：基于模型的强化学习（Thompson Sampling: Model-Based RL）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 汤普森取样实现了概率匹配（Thompson sampling implements probability matching）\n",
    "\\begin{align}\n",
    "\\pi (s,a \\mid h_t) & = \\mathbb{P}[Q^{\\ast}(s,a) > Q^{\\ast} (s,a'),\\forall a' \\neq a \\mid h_t] \\\\\n",
    "& = \\mathbb{E}_{\\mathcal{P},\\mathcal{R}\\mid h_t}\\left[ \\textbf{1}(a=\\operatorname*{argmax}_{a\\in\\mathcal{A}} Q^{\\ast} (s,a)) \\right]\n",
    "\\end{align}\n",
    "\n",
    "- 用贝叶斯法则来计算后验分布$p[\\mathcal{P},\\mathcal{R}\\mid h_t]$\n",
    "- 从后验信息中取样一个MDP$\\mathcal{P},\\mathcal{R}$（sample an MDP $\\mathcal{P},\\mathcal{R}$ from posterior）\n",
    "- 用你最喜欢的规划算法来解MDP从而得到$Q^{\\ast}(s,a)$\n",
    "- 为取样的MDP选择一个最优动作（select optimal action for sample MDP），$a_t = \\operatorname*{argmax}_{a\\in\\mathcal{A}} Q^{\\ast} (s_t,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在MDP中进行信息状态搜索（Information State Search in MDPs）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们可以对MDP做增广来包含信息状态（MDPs can be augmented to include information state）\n",
    "- 现在增广后的状态（augmented state）是$<s, \\tilde{s}>$\n",
    "    - 这里的$s$是MDP中的原始状态\n",
    "    - $\\tilde{s}$是历史记录的统计信息，也就是累计信息（$\\tilde{s}$ is a statistic of the history, namely the accumulated information）\n",
    "- 每个动作$a$都会导致状态转移（each action $a$ causes a transition）\n",
    "    - 有$\\mathcal{P}_{s, s'}^a$的概率转移到新状态$s'$上\n",
    "    - 转移到一个新的信息状态$\\tilde{s}'$上\n",
    "- 在增广的信息状态空间（augmented information state space）中定义MDP$\\tilde{\\mathcal{M}}$\n",
    "\\begin{equation}\n",
    "\\tilde{\\mathcal{M}}=<\\tilde{\\mathcal{S}},\\mathcal{A},\\tilde{\\mathcal{P}},\\mathcal{R},\\gamma>\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 贝叶斯自适应MDP（Bayes-Adaptive MDPs）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在MDP模型上的后验分布是一个信息状态（posterior distribution over MDP model is an information state）\n",
    "\\begin{equation}\n",
    "\\tilde{s}_t = \\mathbb{P}[\\mathcal{P},\\mathcal{R}\\mid h_t]\n",
    "\\end{equation}\n",
    "\n",
    "- 在$<s,\\tilde{s}>$上增广的MDP被叫作贝叶斯自适应MDP（Augmented MDP over $<s,\\tilde{s}>$ is called Bayes-adaptive MDP）\n",
    "- 解这个MDP来做出（对于先验信息来说）探索与应用的最优权衡（solve this MDP to find optimal exploration/exploitation trade-off (with respect to prior)）\n",
    "- 然而，贝叶斯自适应MDP一般来说都非常大（Bayes-adaptive MDP is typically enormous）\n",
    "- 基于模拟的搜索被证明是有效的（simulation-based search has proven effective）（Guez et al.）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们讲了关于取舍探索与应用的一些原理（have covered several principles for exploration/exploitation）\n",
    "    - 朴素探索策略（naive exploration）比如$\\epsilon$-贪心策略\n",
    "    - 乐观初始化策略（optimistic initialisation）\n",
    "    - 面对不确定性时的乐观策略（optimism in the face of uncertainty）比如置信上界策略（upper confidence bounds）\n",
    "    - 概率匹配策略（probability matching）\n",
    "    - 信息状态搜索策略（information state search）\n",
    "\n",
    "- 每种原理都是在老虎机问题上被开发出来的（each principle was developed in bandit setting）\n",
    "- 但是同样的原理也可以用在MDP问题上（but same principles also apply to MDP setting）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最初编辑日期"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年4月25日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/XX.pdf\n",
    "\n",
    "[2] https://www.youtube.com/watch?v=sGuiWX07sKw"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

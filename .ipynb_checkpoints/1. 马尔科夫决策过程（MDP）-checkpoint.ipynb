{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 马尔科夫决策过程（Markov Decision Process）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "马尔科夫决策过程描述了强化学习中的环境。在这里我们认为环境是完全可观测的（fully observable），也就是说当前时刻的状态（state）可以完全定义整个过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 马尔科夫性质（Markov Property）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The future is independent of the past given the present.\"\n",
    "\n",
    "“给定现在的状态，未来的状态独立于过去的状态。”\n",
    "\n",
    "当且仅当\n",
    "\n",
    "$P[S_{t+1} | S_{t}] = P[S_{t+1} | S_{1},\\ldots,S_{t}]$\n",
    "\n",
    "时，\n",
    "\n",
    "我们说这个状态$S_t$是具备马尔科夫性质的。\n",
    "\n",
    "- 这个状态应该包含了过去的所有相关信息\n",
    "- 当这个状态是已知的时候，我们可以舍弃之前的信息\n",
    "- 这个状态为决策提供了足够的统计信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 状态转移矩阵（State Transition Matrix）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对一个马尔科夫状态$s$和下一帧的状态$s'$来说，我们把状态转移概率定义为\n",
    "\n",
    "$P_{s s'} = P[S_{t+1} = s' | S_{t} = s]$\n",
    "\n",
    "状态转移矩阵$P$定义了所有状态$s$到下一帧状态$s'$的转移概率，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "P = \n",
    "\\begin{bmatrix}\n",
    "P_{11} & \\cdots & P_{1n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "P_{n1} & \\cdots & P_{nn}\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状态转移矩阵$P$中每行概率之和为1。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 马尔科夫过程（Markov Process）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个马尔科夫过程是一个无记忆的随机过程，也就是一序列具有马尔科夫性质的随机状态$S_{1},S_{2},\\ldots$\n",
    "\n",
    "马尔科夫过程（或者叫作马尔科夫链，Markov Chain）是一个元组$<S,P>$\n",
    "\n",
    "- $S$是状态的一个（有限）集合\n",
    "\n",
    "- $P$是一个状态转移概率矩阵，其中$P_{s s'} = P[S_{t+1} = s' | S_t = s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 马尔科夫奖励过程（Markov Reward Process）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个马尔科夫奖励过程（MRP）是一个伴有价值反馈的马尔科夫链。\n",
    "\n",
    "一个马尔科夫奖励过程是一个元组$<S,P,R,\\gamma>$\n",
    "\n",
    "- $S$是状态的一个有限集合\n",
    "\n",
    "- $P$是一个状态转移概率矩阵，$P_{s s'} = P[S_{t+1} = s' | S_{t} = s]$\n",
    "\n",
    "- $R$是一个奖励函数（reward function），$R_{s} = E[R_{t+1} | S_{t} = s]$\n",
    "\n",
    "- $\\gamma$是取值范围$\\gamma \\in [0, 1]$的一个折扣系数（discount factor）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 回报（Return）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回报$G_{t}$的定义是$t$时刻之后的折扣奖励之和，\n",
    "\n",
    "$G_{t} = R_{t+1} + \\gamma R_{t+2} + \\ldots = \\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1}$\n",
    "\n",
    "- 折扣系数$\\gamma \\in [0, 1]$是未来回报的当前价值\n",
    "\n",
    "- 在$k+1$帧后接收到奖励$R$的价值是$\\gamma^{k}R$\n",
    "\n",
    "- 即时奖励的价值是高于延迟奖励的\n",
    "    - 接近于0的$\\gamma$值会使评估更加“目光短浅”\n",
    "    - 接近于1的$\\gamma$值会使评估更加“目光长远”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 为什么要使用折扣系数呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大多数的马尔科夫回报和决策过程都使用了折扣系数。那其中的原因是什么呢？\n",
    "\n",
    "- 折扣奖励在数学性质上的便利性\n",
    "\n",
    "- 可以避免在循环马尔科夫过程的无限回报\n",
    "\n",
    "- 未来的不确定性可能并没有充分体现\n",
    "\n",
    "- 在奖励是金融方面的，即时奖励较延迟奖励来说可能享有更高的利息\n",
    "\n",
    "- 人和动物的行为显示了对即时奖励的倾向性\n",
    "\n",
    "- 在确定所有过程都会终止的前提下，我们也可以用无折扣的马尔科夫奖励过程（$\\gamma=1$，undiscounted Markov reward processes）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 价值函数（Value Function）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "价值函数$v(s)$给出了状态$s$的长期价值\n",
    "\n",
    "一个MRP的状态价值函数（state value function）$v(s)$是从状态$s$开始的期望回报\n",
    "\n",
    "$v(s) = E[G_{t} | S_{t} = s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRPs的贝尔曼方程（Bellman Equation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "价值函数可以被拆分成两部分:\n",
    "\n",
    "- 即时奖励$R_{t+1}$\n",
    "\n",
    "- 后继状态的折扣价值$\\gamma v(S_{t+1})$\n",
    "\n",
    "\\begin{split}\n",
    "v(s) & = E[G_t | S_t = s] \\\\\n",
    "& = E[R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+\\ldots | S_{t} = s] \\\\\n",
    "& = E[R_{t+1}+\\gamma(R_{t+2}+\\gamma R_{t+3}+\\ldots) | S_{t} = s] \\\\\n",
    "& = E[R_{t+1}+\\gamma G_{t+1} | S_{t} = s] \\\\\n",
    "& = E[R_{t+1}+\\gamma v(S_{t+1}) | S_{t} = s]\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$v(s) = E[R_{t+1}+\\gamma v(S_{t+1}) | S_t = s]$\n",
    "\n",
    "<img src=\"files/figures/Bellman_equation_for_MRPs_1.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "$v(s) = R_{s} + \\gamma \\sum_{s' \\in S} P_{s s'}v(s')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 矩阵形式的贝尔曼方程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们有一个列向量$v$，每个对应的状态都有一个输入，那么贝尔曼方程可以用矩阵形式被简明地被表示为\n",
    "\n",
    "$v = R + \\gamma P v$\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "v(1) \\\\\n",
    "\\vdots \\\\\n",
    "v(n)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "R_1 \\\\\n",
    "\\vdots \\\\\n",
    "R_n\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\gamma\n",
    "\\begin{bmatrix}\n",
    "P_{11} & \\cdots & P_{1n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "P_{n1} & \\cdots & P_{nn}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "v(1) \\\\\n",
    "\\vdots \\\\\n",
    "v(n)\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 解贝尔曼方程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 贝尔曼方程是一个线性方程\n",
    "\n",
    "- 它可以被直接解出来：\n",
    "\n",
    "\\begin{split}\n",
    "v = R + \\gamma P v \\\\\n",
    "(I - \\gamma P) v = R \\\\\n",
    "v = (I - \\gamma P)^{-1} R\n",
    "\\end{split}\n",
    "\n",
    "- $n$个状态的计算复杂度是$O(n^3)$\n",
    "\n",
    "- 直接解只在较小规模的MRPs中可能存在\n",
    "\n",
    "- 对于规模较大的MRPs，我们有一些迭代的方法：\n",
    "    - 动态规划（Dynamic programming）\n",
    "    - 蒙特卡洛评估（Monte-Carlo evaluation）\n",
    "    - 时差学习法（Temporal-Difference learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 马尔科夫决策过程（Markov Decision Process）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个马尔科夫决策过程（MDP）是一个伴有决策的马尔科夫奖励过程。MDP是一个所有状态都具有马尔科夫性质的环境。\n",
    "\n",
    "一个马尔科夫决策过程是一个元组$<S,A,P,R,\\gamma>$\n",
    "\n",
    "- $S$是状态的一个有限集合\n",
    "\n",
    "- $A$是动作的一个有限集合\n",
    "\n",
    "- $P$是一个状态转移概率矩阵，$P_{s s'}^{a} = P[S_{t+1} = s' | S_{t} = s, A_{t} = a]$\n",
    "\n",
    "- $R$是一个奖励函数，$R_{s}^{a} = E[R_{t+1} | S_{t}=s, A_{t}=a]$\n",
    "\n",
    "- $\\gamma$是一个折扣系数，$\\gamma \\in [0, 1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 策略（Policy）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "策略$\\pi$是在给定状态情况下动作的概率分布，\n",
    "\n",
    "$\\pi (a|s) = P[A_{t}=a|S_{t}=s]$\n",
    "\n",
    "- 策略可以完全定义智能体的行为\n",
    "\n",
    "- MDP的策略只取决于此刻的状态（而不是历史）\n",
    "\n",
    "- 这也就是说策略是稳定的（与时间无关的），$A_{t} \\sim \\pi(\\cdot|S_{t}), \\forall t > 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 给定一个MDP $M=<S,A,P,R,\\gamma>$和一个策略$\\pi$\n",
    "\n",
    "- 状态序列$S_1, S_2,\\ldots$是一个马尔科夫过程$<S,P^{\\pi}>$\n",
    "\n",
    "- 状态和奖励的序列$S_1, R_2, S_2, \\ldots$是一个马尔科夫奖励过程$<S,P^{\\pi},R^{\\pi},\\gamma>$\n",
    "\n",
    "- 在这里\n",
    "\n",
    "\\begin{split}\n",
    "P_{s, s'}^{\\pi} & = \\sum_{a \\in A} \\pi (a|s) P_{s s'}^{a} \\\\\n",
    "R_{s}^{\\pi} & = \\sum_{a \\in A} \\pi (a|s) R_{s}^{a}\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 价值函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个MDP的状态价值函数（state-value function）$v_{\\pi}(s)$是从状态$s$出发并遵循策略$\\pi$的期望回报\n",
    "\n",
    "$v_{\\pi}(s) = E_{\\pi}[G_{t} | S_{t}=s]$\n",
    "\n",
    "一个MDP的动作价值函数（action-value function）$q_{\\pi}(s, a)$是从状态$s$出发，采用动作$a$，并且遵循策略$\\pi$的期望回报\n",
    "\n",
    "$q_{\\pi}(s, a) = E_{\\pi}[G_{t} | S_{t}=s, A_{t}=a]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 贝尔曼期望方程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状态价值函数可以再被分解成即时奖励加上后继状态折扣后的价值，\n",
    "\n",
    "\\begin{equation}\n",
    "v_{\\pi}(s) = E_{\\pi}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t = s]\n",
    "\\end{equation}\n",
    "\n",
    "动作价值函数简单地被分解为，\n",
    "\n",
    "\\begin{equation}\n",
    "q_{\\pi}(s, a) = E_{\\pi}[R_{t+1} + \\gamma q_{\\pi} (S_{t+1}, A_{t+1}) | S_{t}=s, A_{t}=a]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $v^{\\pi}$的贝尔曼期望方程（1）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/Bellman_expectation_equation_for_v_pi.png\" style=\"width: 300px;\"/>\n",
    "\\begin{equation}\n",
    "v_{\\pi} (s) = \\sum_{a \\in A} \\pi (a|s) q_{\\pi} (s, a)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $q^{\\pi}$的贝尔曼期望方程（1）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/Bellman_expectation_equation_for_q_pi.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "\\begin{equation}\n",
    "q_{\\pi} (s, a) = R_{s}^{a} + \\gamma \\sum_{s' \\in S} P_{s s'}^{a} v_{\\pi} (s')\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $v^{\\pi}$的贝尔曼期望方程（2）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/Bellman_expectation_equation_for_v_pi_2.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "\\begin{equation}\n",
    "v_{\\pi} (s) = \\sum_{a \\in A} \\pi (a|s) (R_s^a + \\gamma \\sum_{s' \\in S} P_{s s'}^a v_{\\pi} (s'))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $q^{\\pi}$的贝尔曼期望方程（2）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/Bellman_expectation_equation_for_q_pi_2.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "\\begin{equation}\n",
    "q_{\\pi} (s, a) = R_{s}^{a} + \\gamma \\sum_{s' \\in S} P_{s s'}^{a} \\sum_{a' \\in A} \\pi (a'|s') q_{\\pi} (s', a')\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 贝尔曼期望方程（矩阵形式）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "贝尔曼期望方程可以用推导的MRP简洁地表示成，\n",
    "\n",
    "$v_{\\pi} = R^{\\pi} + \\gamma P^{\\pi} v_{\\pi}$\n",
    "\n",
    "它有直接解\n",
    "\n",
    "$v_{\\pi} = (I - \\gamma P^{\\pi})^{-1} R^{\\pi}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最优价值函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最优状态价值函数（optimal state-value function）$v_{*}(s)$是所有策略中价值最高的函数\n",
    "\n",
    "\\begin{equation}\n",
    "v_{\\ast}(s) = \\max_{\\pi} v_{\\pi}(s)\n",
    "\\end{equation}\n",
    "\n",
    "最优动作价值函数（optimal action-value function） $q_{\\ast}$是所有策略中动作价值最高的函数\n",
    "\n",
    "\\begin{equation}\n",
    "q_{\\ast} (s, a) = \\max_{\\pi} q_{\\pi} (s, a)\n",
    "\\end{equation}\n",
    "\n",
    "- 最优价值函数给定了在MDP中最优的可能表现\n",
    "\n",
    "- 当我们知道了一个MDP的最优价值时，我们可以说这个MDP被我们解决了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最初编辑时间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年4月16日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf\n",
    "\n",
    "[2] https://www.youtube.com/watch?v=lfHX2hHRMVQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control with Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvard引用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D. and Wierstra, D., 2015. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BibTex引用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "@article{lillicrap2015continuous,\n",
    "  title={Continuous control with deep reinforcement learning},\n",
    "  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},\n",
    "  journal={arXiv preprint arXiv:1509.02971},\n",
    "  year={2015}\n",
    "}\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 摘要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We adapt the idea underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policy \"end-to-end\": directly from raw pixel inputs.\n",
    "\n",
    "基于Deep Q-Learning的基本思想，我们把它应用在了连续动作领域（continuous action domain）。我们会介绍一个基于确定性梯度策略（deterministic policy gradient）的actor-critic、无模型算法，这个算法可以被运用在连续动作空间上。在学习算法、网络结构和超参数（hyper-parameters）不改变的情况下，我们的算法可靠地求解了包括cartpole swing-up、dexterous manipulation、legged locomotion和car driving等20多个物理模拟任务。我们的算法能够找到性能与有模型规划算法相匹敌的策略。我们也进一步展示了对于许多任务来说，我们的算法能够对策略进行“端到端”的学习：也就是直接从原始像素输入中学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While DQN solves problems with high-dimensional observation spaces, it can only handle discrete and low-dimensional action spaces. Many tasks of interest, most notably physical control tasks, have continuous (real valued) and high dimensional action spaces. DQN cannot be straightforward applied to continuous domains since it relies on a finding the action that maximizes the action-value function, which in the continuous valued case requires an iterative optimization process at every step.\n",
    "\n",
    "尽管DQN可以解决高维度观测空间的问题，但它只能处理离散和低维度的动作空间。但许多比如物理控制任务都是基于连续和高维动作空间的。因为DQN是基于最大化动作价值函数来选择动作的，而在连续值的情况下每一步都需要迭代优化，所以DQN不能被直接应用在连续动作空间上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初次编辑日期"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年5月13日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D. and Wierstra, D., 2015. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

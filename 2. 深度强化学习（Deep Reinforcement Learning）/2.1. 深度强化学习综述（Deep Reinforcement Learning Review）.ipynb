{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度强化学习入门的两篇论文"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在我访问Lincoln Centre for Autonomous Systems时，Maximilian Huettenrauch给我推荐了几篇深度强化学习领域的入门论文，我在这里挑选了两篇："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Arulkumaran, K., Deisenroth, M.P., Brundage, M. and Bharath, A.A., 2017. A Brief Survey of Deep Reinforcement Learning. arXiv preprint arXiv:1708.05866.**\n",
    "\n",
    "**2. Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D. and Meger, D., 2017. Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面就是我自己翻译的\"A Brief Survey of Deep Reinforcement Learning\"的版本，对于有英文阅读能力的同学还是推荐直接阅读原文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Brief Survey of Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 摘要（Abstract）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度强化学习（deep reinforcement learning）有望给人工智能领域带来革命性的改变，并且它也是构建对视觉世界有更高层理解的自动化系统的一个里程碑。如今，深度学习促使强化学习能够应用在以前难以处理的问题上，比如说直接基于像素信息学习玩电子游戏。深度强化学习算法同样也适用于机器人领域，使得机器人的控制策略可以直接从相机输入的现实世界的信息中直接习得。在这篇综述中，我们开头会介绍强化学习的总体概述，然后会涉及到主流的基于价值（value-based）和基于策略（policy-based）的方法。我们的综述会涵盖深度强化学习的核心算法，包涵deep Q-network、trust region policy optimisation还有asynchronous advantage actor-critic。与此同时，我们也强调了深度神经网络的独特优势，专注于基于强化学习的视觉理解。在结尾部分，我们描述了几个在这个领域的研究方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 引言（Introduction）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "人工智能（AI，Artificial Intelligence）领域的主要目标之一就是创造完全自主的智能体，这种智能体可以通过在与环境的交互中试错来学习最优的行为策略。制造可以交互并且能有效地进行学习AI系统是一项长期的挑战，这包括了可以感知并且对周围世界做出反应的机器人，还有可以与自然语言和多媒体交互的完全基于软件的智能体。强化学习（RL，Reinforcement Learning）是经历驱动自主学习（experience-driven autonomous learning）的一个基础数学框架。尽管RL在过去取得了一定的成功，过去的方法缺乏可扩展性，本质上受限于维度较低的问题。存在这些限制的原因是强化学习算法和其他算法一样都面临着复杂性问题：内存复杂度、计算复杂度、在机器学习算法应用的问题上相同的复杂度。我们在最近几年见证了深度学习的崛起，深度学习的成功依仗于深度神经网络强大的函数近似（function approximation）和表示学习（representation learning）能力，这也为我们克服这些问题提供了新的工具。\n",
    "\n",
    "深度学习的出现对机器学习的许多领域都产生了显著的影响，极大地提高了在一些任务上的最高水准，比如物体识别、语音识别还有语言翻译。深度学习最重要的特点就是深度神经网络可以自动寻找高维数据（比如图片、文本和音频）的简洁低维度表示（特征）。通过人工在神经网络结构中引入归纳偏置，尤其是在层级表示上，机器学习从业者已经在克服维度灾难（curse of dimensionality）上取得了有效的进展。同样的，深度学习也加速了强化学习的发展，在强化学习的框架下使用深度学习算法就是我们所说的深度强化学习（deep reinforcement learning, or DRL）领域。这篇综述会讲到DRL的开创性文献和最近的发展，我们想要传达神经网络应用的新思路，希望能让我们离开发出自主智能体更进一步。想要阅读关于DRL近期进展更详尽综述的同学我们推荐阅读："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Li, Y., 2017. Deep reinforcement learning: An overview. arXiv preprint arXiv:1701.07274.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度学习使强化学习可以被扩展到以前难以处理的决策问题上，也就是具有高维度状态和动作空间的问题。近期DRL的工作中有两个显著的成果。第一个就是开创DRL革新的直接基于图像像素玩Atari 2600游戏并取得超越人类水平成绩的算法。这为应对函数近似技巧在强化学习中的不稳定性提供了方案，这项研究是第一个显示强化学习智能体可以被用在高维原始观测上并只基于奖励信号的有力证明。第二个巨大成功就是一个混合DRL系统的开发，这个系统就是AlphaGo。AlphaGo击败了人类围棋世界冠军，此前这方面的历史成就有IBM的Deep Blue在二十年前击败人类国际象棋冠军，还有IBM的Watson DeepQA系统击败了“Jeopardy！”游戏最强的人类玩家。与在国际象棋系统中主流的人工编写规则方法不同，AlphaGo是在传统启发式搜索算法基础上由神经网络组成并用监督学习和强化学习进行训练的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DRL算法已经被应用在各种各样的问题上了，其中就包括机器人领域。继人工设计的和从低维机器人状态特征中学习的机器人控制器后，DRL使得控制策略可以基于现实世界的相机输入被直接学习。DRL已经被用在创造可以进行元学习（meta-learn, or \"learn to learn\"）的智能体上了，使得这些智能体可以被应用在先前未见过的复杂视觉环境中，这也向实现更强大的智能体更进了一步。我们在下图中展示了DRL的一些应用领域，涉及从电子游戏到室内导航。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/a_range_of_visual_rl_domains.png\" style=\"width: 800px;\" />\n",
    "\n",
    "这是一系列基于视觉强化学习的应用\n",
    "\n",
    "(a) 图片是两个经典的Atari 2600电子游戏，分别是Arcade学习环境（Arcade Learning Environment, ALE）中的\"Freeway\"和\"Seaquest\"。由于支持各种各样种类的游戏，包括了不同的视觉输入和难度，ALE已经成为了DRL算法的标准测试平台。我们将在后面讨论，ALE是现在用来将强化学习的评估标准化的众多基准之一。\n",
    "\n",
    "(b) 图片是TORCS赛车模拟器，它已经被用来测试可以输出连续动作的DRL算法（因为ALE的游戏只支持离散的动作）。\n",
    "\n",
    "(c) 我们可以利用在机器人模拟器中取得的理论上可以是无限的训练数据，有几种方法都被设计来把知识从模拟器中迁移到现实世界。\n",
    "\n",
    "(d) 图片对应Levine et al.设计的四个机器人任务之二：拧瓶盖和把积木放到对应形状的孔中。Levine et al.以端到端的方式成功训练了视觉运动策略，证明了视觉伺服可以用神经网络直接从原始相机输入中学习。\n",
    "\n",
    "(e) 图片是一个现实中的房间，在这里一个轮式机器人被训练来在建筑物中导航。在机器人会被给定视觉输入提示，要找到对应的位置。\n",
    "\n",
    "(f) 图片是一张被用神经网络标注的自然图像，其中用强化学习来选择看哪里。通过对每个生成的单词都处理图片的一小部分，网络模型可以把它的注意力集中在最突出的点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "电子游戏也许是一个有趣的挑战，但是学习怎样玩游戏并不是DRL的最终目的。DRL背后的推力之一就是创造能够学习怎样适应现实世界系统的愿景。从节约功耗到抓取和收纳物品，DRL所主张的是把更多的体力活通过学习来自动化。然而，DRL并没有就此止步，因为强化学习是通过试错来对问题进行优化的一种方法。从设计最前沿的机器翻译模型到构建新的优化函数，DRL已经被用来尝试解决各种机器学习问题。此外，就像深度学习已经被用在机器学习的许多分支一样，DRL成为构建通用AI系统的重要部件在未来似乎是有可能的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\ldots$未完待续"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初次编辑日期"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年5月1日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Arulkumaran, K., Deisenroth, M.P., Brundage, M. and Bharath, A.A., 2017. A Brief Survey of Deep Reinforcement Learning. arXiv preprint arXiv:1708.05866.\n",
    "\n",
    "[2] Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D. and Meger, D., 2017. Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

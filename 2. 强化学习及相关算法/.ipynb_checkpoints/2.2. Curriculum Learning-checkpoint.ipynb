{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 标签 (Labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvard引用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bengio, Y., Louradour, J., Collobert, R. and Weston, J., 2009, June. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning (pp. 41-48). ACM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BibTex引用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@inproceedings{bengio2009curriculum,\n",
    "  title={Curriculum learning},\n",
    "  author={Bengio, Yoshua and Louradour, J{\\'e}r{\\^o}me and Collobert, Ronan and Weston, Jason},\n",
    "  booktitle={Proceedings of the 26th annual international conference on machine learning},\n",
    "  pages={41--48},\n",
    "  year={2009},\n",
    "  organization={ACM}\n",
    "}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 摘要 (Abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them \"curriculum learning\". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).\n",
    "\n",
    "当范例被按照概念从简到繁呈现时，人类和动物都会学得比随机呈现范例时要更快。我们在这里将这样的训练策略在机器学习的背景下形式化，并把这种训练策略叫作\"curriculum learning\"。近期，在非凸训练条件下的训练难点研究中（对于深度确定和随机的神经网络），我们在不同的情况下探索了curriculum learning。实验结果显示curriculum learning可以使得模型的泛化能力得到显著提升。我们推测curriculum learning不仅能缩短收敛的训练时间，而且能在非凸条件下找到更好的局部最优值：curriculum learning可以被看作是延续方法的一种特殊形式（一种对非凸函数全局优化的一般策略）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 研究问题 (Research Problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can machine learning algorithms benefit from a curriculum strategy?\n",
    "\n",
    "机器学习算法可以从curriculum策略中获益吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主要贡献 (Contributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Showing several cases - involving vision and language tasks - in which very simple multi-stage curriculum strategies give rise to improved generalization and faster convergence.\n",
    "\n",
    "(2) Introducing a hypothesis which may help to explain some of the advantages of a curriculum strategy. This hypothesis is essentially that a well chosen curriculum strategy can act as a continuation method (Allgower & Georg, 1980), i.e., can help to find better local minima of a non-convex training criterion.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概要 (Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curriculum Learning的历史\n",
    "\n",
    "1. Skinner (1958) in animal training: \"Shaping\"\n",
    "\n",
    "2. Allgower & Georg (1980) in optimization: The hypothesis is essentially that a well chosen curriculum strategy can act as a continuation method.\n",
    "\n",
    "3. Elman (1993) in machine learning: \"Start small\"\n",
    "\n",
    "4. Sanger (1994) in robotics: \"Learning is \"bootstrapped\" from regions of easy solvability into regions of more difficult dynamical behaviour.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By choosing which examples to present and in which order to present them to the learning system, one can *guide* training and remarkably increase the speed at which learning can occur. This idea is routinely exploited in *animal training* where it is called **shaping** (Skinner, 1958; Peterson, 2004; Krueger & Dayan, 2009)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of training a learning machine with a curriculum can be traced back at least to Elman (1993). The basic idea is to *start small*, learn easier aspects of the task or easier subtasks, and then gradually increase the difficulty level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar ideas were also explored in robotics (Sanger, 1994), by gradually making the learning task more difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were no good algorithms for training fully-connected deep architectures before 2006, when Hinton et al. (2006) introduced a learning algorithm that greedily trains one layer at a time. It exploits an unsupervised generative learning algorithm for each layer: a Restricted Boltzmann Machine (RBM) (Freund & Haussler, 1994)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaner examples may yield better generalization faster\n",
    "\n",
    "In principle one could argue that difficult examples can be more informative than easy examples. Here the difficult examples are probably not useful because they confuse the learner rather than help it establish the right location of the decision surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A curriculum as a continuation method\n",
    "\n",
    "Continuation methods (Allgower & Georg, 1980) are optimization strategies for dealing with minimizing non-convex criteria. Although these global optimization methods provide no guarantee that the global optimum will be obtained, they have been particularly useful in computational chemistry to find approximate solutions of difficult optimization problems involving the configurations of molecules (Coleman & Wu, 1994; Wu, 1997). The basic idea is to first optimize a smoothed objective and gradually consider less smoothing, with the intuition that a smoothed version of the problem reveals the global picture. One defines a single-parameter family of cost functions $C_{\\lambda}(\\theta)$ such that $C_0$ can be optimized easily (maybe convex in $\\theta$), while $C_1$ is the criterion that we actually wish to minimize. One first minimizes $C_0(\\theta)$ and then gradually increases $\\lambda$ while keeping $\\theta$ at a local minimum of $C_{\\lambda}(\\theta)$. Typically $C_0$ is a highly smoothed version of $C_1$, so that $\\theta$ gradually moves into the basin of attraction of a dominant (if not global) minimum of $C_1$. Applying a continuation method to the problem of minimizing a training criterion involves a sequence of training criteria, starting from one that is easier to optimize, and ending with the training criterion of interest.\n",
    "\n",
    "At an abstract level, a curriculum can also be seen as a sequence of training criteria. Each training criterion in the sequence is associated with a different set of weights on the training examples, or more generally, on a reweighting of the training distribution. Initially, the weights favor \"easier\" examples, or examples illustrating the simplest concepts, that can be learned most easily. The next training criterion involves a slight change in the weighting of examples that increases the probability of sampling slightly more difficult examples. At the end of the sequence, the reweighting of the examples is uniform and we train on the target training set or the target training distribution.\n",
    "\n",
    "One way to formalize this idea is the following. Let $z$ be a random variable representing an example for the learner (possibly an $(x, y)$ pair for supervised learning). Let $P(z)$ be the target training distribution from which the learner should ultimately learn a function of interest. Let $0 \\leq W_{\\lambda}(z) \\leq 1$ be the weight applied to example $z$ at step $\\lambda$ in the curriculum sequence, with $0\\leq \\lambda \\leq 1$, and $W_1 (z)=1$. The corresponding training distribution at step $\\lambda$ is\n",
    "\n",
    "$$Q_{\\lambda}(z) \\propto W_{\\lambda}(z)P(z) \\enspace \\forall z$$\n",
    "\n",
    "such that $\\int Q_{\\lambda}(z)dz = 1$. Then we have\n",
    "\n",
    "$$Q_1(z)=P(z) \\enspace \\forall z$$\n",
    "\n",
    "Consider a monotonically increasing sequence of $\\lambda$ values, starting from $\\lambda = 0$ and ending at $\\lambda = 1$.\n",
    "\n",
    "**Definition** We call the corresponding sequence of distributions $Q_{\\lambda}$ (following the above two equations) a **curriculum** if the *entropy of these distributions increases*\n",
    "\n",
    "$$H(Q_{\\lambda}) < H(Q_{\\lambda + \\epsilon}) \\enspace \\forall \\epsilon > 0$$\n",
    "\n",
    "and $W_{\\lambda}(z)$ is *monotonically increasing in* $\\lambda$, i.e.,\n",
    "\n",
    "$$W_{\\lambda + \\epsilon}(z) \\geq W_{\\lambda}(z) \\enspace \\forall z, \\forall \\epsilon > 0.$$\n",
    "\n",
    "To illustrate this definition, consider the simple setting where $Q_{\\lambda}$ is concentrated on a finite set of examples, and increasing $\\lambda$ means adding new examples to that set: the support of $Q_{\\lambda}$ increases with $\\lambda$, and the sequence of training distributions corresponds to a sequence of embedded training sets, starting with a small set of easy examples and ending with the target training set. We want the entropy to increase so as to increase the diversity of training examples to increase as they get \"added\" into the training set.\n",
    "\n",
    "In the experiments below the sequence of training sets is always discrete. In fact the curriculum strategy worked in some of our experiments with a sequence of just two steps: first a set of easy examples, and then the target training set. At the other extreme, if training proceeds in a stochastic manner by sampling training examples from a distribution, then one could imagine a continuous sequence of sampling distributions which gradually gives more weight $W_{\\lambda}(z)$ to the more difficult examples, until all examples have an equal weight of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初次编辑日期 (Initial Edit Date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年5月18日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献 (References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Bengio, Y., Louradour, J., Collobert, R. and Weston, J., 2009, June. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning (pp. 41-48). ACM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于模型的强化学习（Model-Based Reinforcement Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 策略梯度（基于策略的学习）：从经历中直接学习策略（learn policy directly from experience）\n",
    "- 基于价值函数的学习：从经历中直接学习价值函数（learn value function directly from experience）\n",
    "- 基于模型的学习：从经历中直接学习模型（learn model directly from experience）\n",
    "- 并且基于模型用规划（planning）来构造价值函数或是策略\n",
    "- 把学习和规划整合到同一个框架中（integrate learning and planning into a single architecture）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于模型和无模型的强化学习（Model-Based and Model-Free Reinforcement Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 无模型的强化学习（model-free RL）\n",
    "    - 没有模型\n",
    "    - 从经历中学习价值函数（和/或策略）\n",
    "- 基于模型的强化学习（model-based RL）\n",
    "    - 从经历中学习模型\n",
    "    - 在模型中规划价值函数（和/或策略）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 无模型的强化学习（Model-Free RL）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/model-free_rl.png\" style=\"width: 300px'\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于模型的强化学习（Model-Based RL）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/model-based_rl.png\" style=\"width: 300px'\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/model-based_rl_2.png\" style=\"width: 300px'\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于模型强化学习的优势（Advantages of Model-Based RL）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优势：\n",
    "\n",
    "- 可以通过监督学习的方法来高效地学习模型\n",
    "- 可以对模型的不确定性进行推理（reason about model uncertainty）\n",
    "\n",
    "劣势：\n",
    "\n",
    "- 先学习一个模型，再构建一个价值函数\n",
    "    - 也就是说有两个近似误差的来源（two sources of approximation error）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 什么是模型？（What is a Model?）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 模型$\\mathcal{M}$是一个用$\\eta$来参数化的MDP$<\\mathcal{S},\\mathcal{A},\\mathcal{P},\\mathcal{R}>$\n",
    "- 让我们假设状态空间$\\mathcal{S}$和动作空间$\\mathcal{A}$是已知的\n",
    "- 所以一个模型$\\mathcal{M}=<\\mathcal{P}_{\\eta},\\mathcal{R}_{\\eta}>$表示了状态转移（state transitions）$\\mathcal{P}_{\\eta}\\approx\\mathcal{P}$和奖励（rewards）$\\mathcal{R}_{\\eta}\\approx\\mathcal{R}$\n",
    "\\begin{align}\n",
    "S_{t+1} & \\sim \\mathcal{P}_{\\eta}(S_{t+1}\\mid S_t, A_t) \\\\\n",
    "R_{t+1} & = \\mathcal{R}_{\\eta}(R_{t+1}\\mid S_t, A_t)\n",
    "\\end{align}\n",
    "\n",
    "- 我们通常假设状态转移和奖励之间是条件独立的（conditional independence）\n",
    "\\begin{equation}\n",
    "    \\mathbb{P}[S_{t+1},R_{t+1}\\mid S_t,A_t]=\\mathbb{P}[S_{t+1}\\mid S_t,A_t]\\mathbb{P}[R_{t+1}\\mid S_t,A_t]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型学习（Model Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 目标：从经历$\\{ S_1,A_1,R_2,\\ldots,S_T \\}$中估计（estimate）出模型$\\mathcal{M}_{\\eta}$\n",
    "- 这是一个监督学习问题（a supervised learning problem）\n",
    "\\begin{align}\n",
    "S_1, S_1 & \\rightarrow R_2, S_2 \\\\\n",
    "S_2, S_2 & \\rightarrow R_3, S_3 \\\\\n",
    "& \\vdots \\\\\n",
    "S_{T-1}, A_{T-1} & \\rightarrow R_T, S_T\n",
    "\\end{align}\n",
    "\n",
    "- 学习$s, a \\rightarrow r$是一个回归问题（a regression problem）\n",
    "- 学习$s, a \\rightarrow s'$是一个密度估计问题（a density estimation problem）\n",
    "- 选择损失函数（loss function），比如说均方误差（mean-squared error）、KL散度（Kullback-Leibler divergence），$\\ldots$\n",
    "- 寻找最小化经验损失（empirical loss）的参数$\\eta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型的例子（Examples of Models）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 查表模型（table lookup model）\n",
    "- 线性期望模型（linear expectation model）\n",
    "- 线性高斯模型（linear Gaussian model）\n",
    "- 高斯过程模型（Gaussian process model）\n",
    "- 深度信念网络（deep belief network model）\n",
    "- $\\ldots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查表模型（Table Lookup Model）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 模型是一个明确的MDP（explicit MDP）$\\hat{\\mathcal{P}},\\hat{\\mathcal{R}}$\n",
    "- 每个状态动作组合的访问计数（count visits）$N(s,a)$\n",
    "\\begin{align}\n",
    "\\hat{\\mathcal{P}}_{s,s'}^a & = \\frac{1}{N(s,a)}\\sum_{t=1}^T \\textbf{1}(S_t, A_t, S_{t+1}=s,a,s') \\\\\n",
    "\\hat{\\mathcal{R}}_s^a & = \\frac{1}{N(s,a)}\\sum_{t=1}^T\\textbf{1}(S_t, A_t = s, a)R_t\n",
    "\\end{align}\n",
    "\n",
    "- 此外\n",
    "    - 在每一时刻（at each time-step）$t$，我们以元组的形式记录下经历（record experience tuple）\n",
    "    \\begin{equation}\n",
    "    <S_t,A_t,R_{t+1},S_{t+1}>\n",
    "    \\end{equation}\n",
    "    - 为了给模型采样，我们随机选取其中匹配的元组（to sample model, randomly pick tuple matching）\n",
    "    \\begin{equation}\n",
    "    <s,a,\\cdot,\\cdot>\n",
    "    \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 范例-AB状态（AB Example）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们有两个状态$A$, $B$；没有折扣（no discounting）；8个回合的经历（8 episodes of experience）\n",
    "\n",
    "<img src=\"files/figures/AB.png\" style=\"width: 500px'\" />\n",
    "\n",
    "我们在经历的基础上构建了一个查表模型（construct a table lookup model from the experience）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用模型进行规划（Planning with a Model）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们给定的模型$\\mathcal{M}_{\\eta}=<\\mathcal{P}_{\\eta},\\mathcal{R}_{\\eta}>$\n",
    "- 解这个MDP$<\\mathcal{S},\\mathcal{A},\\mathcal{P}_{\\eta},\\mathcal{R}_{\\eta}>$\n",
    "- 用你最喜欢的规划算法（using favourite planning algorithm）\n",
    "    - 价值迭代（value iteration）\n",
    "    - 策略迭代（policy iteration）\n",
    "    - 树搜索（tree search）\n",
    "    - $\\ldots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于样本的规划（Sample-Based Planning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 简单但是有效的规划方法（a simple but powerful approach to planning）\n",
    "- 模型只被用来生成样本（use the model only to generate samples）\n",
    "- 从模型中取样经历（sample experience from model）\n",
    "\\begin{align}\n",
    "S_{t+1} & \\sim \\mathcal{P}_{\\eta}(S_{t+1}\\mid S_t, A_t) \\\\\n",
    "R_{t+1} & = \\mathcal{R}_{\\eta}(R_{t+1}\\mid S_t, A_t)\n",
    "\\end{align}\n",
    "\n",
    "- 在样本上使用无模型强化学习（apply model-free RL to samples），比如说：\n",
    "    - 蒙特卡洛控制（Monte-Carlo control）\n",
    "    - Sarsa\n",
    "    - Q-learning\n",
    "- 基于样本的规划方法通常更高效（sample-based planning methods are often more efficient）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重回AB状态范例（Back to the AB Example）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在真实经历（real experience）的基础上构建一个查表模型（a table-lookup model）\n",
    "- 在取样的经历上使用无模型强化学习（apply model-free RL to sampled experience）\n",
    "\n",
    "<img src=\"files/figures/back_to_the_AB_example.png\" style=\"width: 500px'\" />\n",
    "\n",
    "比如蒙特卡洛学习（Monte-Carlo learning）：$V(A)=1$, $V(B)=0.75$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于不精确模型的规划（Planning with an Inaccurate Model）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 给定一个不完美的模型$<\\mathcal{P}_{\\eta},\\mathcal{R}_{\\eta}>\\neq <\\mathcal{P},\\mathcal{R}>$\n",
    "- 基于模型强化学习的性能受到了近似MDP $<\\mathcal{S},\\mathcal{A},\\mathcal{P}_{\\eta},\\mathcal{R}_{\\eta}>$最优策略的限制（performance of model-based RL is limited to optimal policy for approximate MDP $<\\mathcal{S},\\mathcal{A},\\mathcal{P}_{\\eta},\\mathcal{R}_{\\eta}>$）\n",
    "- 也就是说，基于模型的强化学习的好坏取决于估计的模型（model-based RL is only as good as the estimated model）\n",
    "- 当模型不那么精确时，规划会给出一个次优的策略（suboptimal policy）\n",
    "- 方案1：当模型错误时，我们可以使用无模型的强化学习（when model is wrong, planning process will compute a suboptimal policy）\n",
    "- 方案2：我们也可以明确地表述出模型的不确定性（reason explicitly about model uncertainty）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 真实和模拟的经历（Real and Simulated Experience）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们考虑经历的两个来源（two sources of experience）\n",
    "\n",
    "真实的经历（real experience）：从环境中采样（sampled from environment, true MDP）\n",
    "\\begin{align}\n",
    "S' & \\sim \\mathcal{P}_{s,s'}^a \\\\\n",
    "R & = \\mathcal{R}_s^a\n",
    "\\end{align}\n",
    "\n",
    "模拟的经历（simulated experience）：从模型中采样（sampled from model, approximate MDP）\n",
    "\\begin{align}\n",
    "S' & \\sim \\mathcal{P}_{\\eta}(S' \\mid S,A) \\\\\n",
    "R & = \\mathcal{R}_{\\eta}(R \\mid S,A)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学习与规划的整合（Integrating Learning and Planning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 无模型的强化学习（model-free RL）\n",
    "    - 没有模型\n",
    "    - 从真实经历中学习价值函数（和/或策略）\n",
    "- 基于模型的强化学习（model-based RL）\n",
    "    - 从真实经历中学习模型\n",
    "    - 在模拟经历的基础上规划价值函数（和/或策略）\n",
    "- Dyna\n",
    "    - 从真实经历中学习模型\n",
    "    - 在真实和模拟经历的基础上学习和规划价值函数（和/或策略）（learn and plan value function and/or policy from real and simulated experience）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dyna的框架（Dyna Architecture）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/Dyna_architecture.png\" style=\"width: 300px'\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dyna-Q算法（Dyna-Q Algorithm）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/Dyna-Q_algorithm.png\" style=\"width: 500px'\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在简单的迷宫中使用Dyna-Q（Dyna-Q on a Simple Maze）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/Dyna-Q_on_a_simple_maze.png\" style=\"width: 400px'\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于不精确模型的Dyna-Q（Dyna-Q with an Inaccurate Model）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这样的环境改变应付起来是比较困难的（the changed environment is harder）\n",
    "<img src=\"files/figures/Dyna-Q_with_an_inaccurate_model_1.png\" style=\"width: 400px'\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这样的环境改变应付起来是比较容易的（the changed environment is easier）\n",
    "<img src=\"files/figures/Dyna-Q_with_an_inaccurate_model_2.png\" style=\"width: 400px'\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前向搜索（Forward Search）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 前向搜索算法通过前瞻预测来选择最优动作（forward search algorithms select the best action by lookahead）\n",
    "- 我们基于当下的状态$s_t$来构建搜索树（build a search tree with the current state $s_t$ at the root）\n",
    "- 我们使用MDP的模型来进行前瞻预测（use a model of the MDP to look ahead）\n",
    "<img src=\"files/figures/forward_search.png\" style=\"width: 400px'\" />\n",
    "\n",
    "- 这样的话我们就不需要解整个MDP（whole MDP），而只需要解从现在开始的子MDP（sub-MDP starting from now）就可以了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于模拟的搜索（Simulation-Based Search）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 前向搜索框架使用了基于样本规划的方法（forward search paradigm using sample-based planning）\n",
    "- 用模型从当下时刻开始模拟好几回的经历（simulate episodes of experience from now with the model）\n",
    "\\begin{equation}\n",
    "\\{ s_t^k, A_t^k, R_{t+1}^k,\\ldots,S_T^k \\}_{k=1}^K \\sim \\mathcal{M}_{v}\n",
    "\\end{equation}\n",
    "\n",
    "- 在模拟的回合上使用无模型学习（apply model-free RL to simulated episodes）\n",
    "    - 蒙特卡洛控制（Monte-Carlo control） $\\rightarrow$ 蒙特卡洛搜索（Monte-Carlo search）\n",
    "    - Sarsa $\\rightarrow$ TD搜索（TD search）\n",
    "\n",
    "<img src=\"files/figures/simulation-based_search.png\" style=\"width: 400px'\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简单的蒙特卡洛搜索（Simple Monte-Carlo Search）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 给定一个模型$\\mathcal{M}_v$和一个模拟策略（a simulated policy）$\\pi$\n",
    "- 对每个动作$a \\in \\mathcal{A}$\n",
    "    - 从当下的真实状态开始模拟$K$个回合（simulate $K$ episodes from current (real) state $s_t$）\n",
    "    \\begin{equation}\n",
    "    \\{ s_t, a, R_{t+1}^k, S_{t+1}^k, A_{t+1}^k, \\ldots, S_T^k \\}_{k=1}^K \\sim \\mathcal{M}_v, \\pi\n",
    "    \\end{equation}\n",
    "    \n",
    "    - 用平均回报来评估动作，也就是蒙特卡洛评估（evaluate actions by mean return, namely Monte-Carlo evaluation）\n",
    "    \\begin{equation}\n",
    "    Q(s_t, a)=\\frac{1}{K}\\sum_{k=1}^K G_t \\xrightarrow{P} q_{\\pi}(s_t,a)\n",
    "    \\end{equation}\n",
    "\n",
    "- 选择当下价值最高的（真实）动作（select current (real) action with maximum value）\n",
    "\\begin{equation}\n",
    "a_t = \\operatorname*{argmax}_{a \\in \\mathcal{A}} Q(s_t, a)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蒙特卡洛树搜索（评价）（Monte-Carlo Tree Search (Evaluation)）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 给定一个模型$\\mathcal{M}_v$\n",
    "- 基于当下的模拟策略$\\pi$从当前状态$s_t$开始模拟$K$个回合（simulate $K$ episodes from current state $s_t$ using current simulation policy $\\pi$）\n",
    "\\begin{equation}\n",
    "\\{ s_t,A_t^k,R_{t+1}^k,S_{t+1}^k,\\ldots,S_T^k \\}_{k=1}^K \\sim \\mathcal{M}_v,\\pi\n",
    "\\end{equation}\n",
    "\n",
    "- 构建一个包含已访状态和动作的搜索树（build a search tree containing visited states and actions）\n",
    "- 用回合中$s$, $a$的平均回报来评估状态$Q(s,a)$\n",
    "\\begin{equation}\n",
    "Q(s,a)=\\frac{1}{N(s,a)}\\sum_{k=1}^K \\sum_{u=t}^T \\textbf{1}(S_u, A_u = s,a)G_u \\xrightarrow{P}q_{\\pi}(s,a)\n",
    "\\end{equation}\n",
    "\n",
    "- 在搜索结束后，选择在搜索树中当下价值最高的（真实）动作（after search is finished, select current (real) action with maximum value in search tree）\n",
    "\\begin{equation}\n",
    "a_t = \\operatorname*{argmax}_{a \\in \\mathcal{A}} Q(s_t, a)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蒙特卡洛树搜索（模拟）（Monte-Carlo Tree Search (simulation)）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在MCTS中，我们可以改进模拟策略$\\pi$（the simulation policy $\\pi$ improves）\n",
    "- 每次模拟都由两个阶段组成（基于树和脱离树）（each simulation consists of two phases (in-tree, out-of-tree)）\n",
    "    - 树策略（改进）（tree policy (improves)）：选择最大化$Q(S,A)$的动作\n",
    "    - 默认策略（固定的）（default policy (fixed)）：随机选择动作\n",
    "- （在每次模拟中）重复这个过程：\n",
    "    - 评估（evaluate）：用蒙特卡洛评估（Monte-Carlo evaluation）来评估状态$Q(S, A)$\n",
    "    - 改进（improve）：改进树策略（improve tree policy），比如用$\\epsilon-greedy(Q)$\n",
    "- 在模拟经历上使用蒙特卡洛控制（Monte-Carlo control applied to simulated experience）\n",
    "- 向最优搜索树收敛（converges on the optimal search tree），也就是$Q(S,A)\\rightarrow q_{\\ast}(S,A)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 案例分析：围棋（Case Study: the Game of Go）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/case_study_the_game_of_go.png\" style=\"width: 200px'\" />\n",
    "\n",
    "- 围棋源于古代东方，已有2500年的历史了\n",
    "- 围棋被认为是最困难的经典棋类游戏\n",
    "- 被John McCarthy认为是AI任务中的一个巨大挑战\n",
    "- 传统的游戏树搜索不能用在围棋上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 围棋的规则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/rules_of_go.png\" style=\"width: 400px'\" />\n",
    "\n",
    "- 通常在$19\\times19$的棋盘上下棋，也有$13\\times13$和$9\\times9$的棋盘\n",
    "- 围棋的规则很简单，但下棋的策略很复杂\n",
    "- 黑子和白字交替落子\n",
    "- 被围起来的子会被吃掉\n",
    "- 围地多的玩家获胜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 围棋中的位置评估（Position Evaluation in Go）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 那我们怎么评价棋型的好坏呢？（how good is a position $s$?）\n",
    "- （无折扣的）奖励函数（reward function (undiscounted)）：\n",
    "\\begin{align}\n",
    "R_t & = 0 \\enspace for\\enspace all \\enspace non-terminal\\enspace steps \\enspace t<T \\\\\n",
    "R_T & = \\left\\{\n",
    "\\begin{array}{lr}\n",
    "\\begin{split}\n",
    "& 1 \\quad & if\\enspace Black\\enspace wins \\\\\n",
    "& 0 \\quad & if\\enspace White\\enspace wins\n",
    "\\end{split}\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{align}\n",
    "\n",
    "- 策略 $\\pi = <\\pi_B , \\pi_W>$ 为两位玩家选择动作\n",
    "- 价值函数（棋型$s$到底有多好）：\n",
    "\\begin{align}\n",
    "v_{\\pi}(s) & = \\mathbb{E}_{\\pi}[R_T \\mid S=s] = \\mathbb{P}[Black\\enspace wins \\mid S=s] \\\\\n",
    "v_{\\ast}(s) & = \\max_{\\pi_B} \\min_{\\pi_W} v_{\\pi}(s)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 围棋中的蒙特卡洛评估（Monte-Carlo Evaluation in Go）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/monte-carlo_evaluation_in_go.png\" style=\"width: 400px'\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蒙特卡洛树搜索的使用（Applying Monte-Carlo Tree Search）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/applying_monte-carlo_tree_search_1.png\" style=\"width: 300px'\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/applying_monte-carlo_tree_search_2.png\" style=\"width: 300px'\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/applying_monte-carlo_tree_search_3.png\" style=\"width: 300px'\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/applying_monte-carlo_tree_search_4.png\" style=\"width: 300px'\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/applying_monte-carlo_tree_search_5.png\" style=\"width: 300px'\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蒙特卡洛树搜索的优势（Advantages of MC Tree Search）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 拥有高度选择性的最佳优先搜索（highly selective best-first search）\n",
    "- 动态地评估状态，与比如说DP不同（evaluates states dynamically, unlike e.g. DP）\n",
    "- 用采样的方法来克服维度诅咒（uses sampling to break curse of dimensionality）\n",
    "- 对“黑箱”模型也有效，只需要采样（works for \"black-box\" models, only requires samples）\n",
    "- 计算效率高，随时都能进行，可并行运算的（computationally efficient, anytime, parallelisable）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 范例：计算机围棋中的蒙特卡洛树搜索（Example: MC Tree Search in Computer Go）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/example_mc_tree_search_in_computer_go.png\" style=\"width: 500px'\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 时差法搜索（Temporal-Difference Search）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 基于模拟的搜索（simulation-based search）\n",
    "- 用TD而不是MC，也就是用到了自举（bootstrapping）\n",
    "- 蒙特卡洛树搜索基于此刻的次优MDP运用了蒙特卡洛控制（MC tree search applies MC control to sub-MDP from now）\n",
    "- 时差法搜索基于此刻的次优MDP运用了Sarsa（TD search applies Sarsa to sub-MDP from now）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蒙特卡洛和时差法搜索的比较（MC vs TD Search）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对于无模型强化学习来说，自举是有帮助的（for model-free reinforcement learning, bootstrapping is helpful）\n",
    "    - 时差法学习减少了方差但是增加了偏差（TD learning reduces variance but increases bias）\n",
    "    - 时差法学习通常比蒙特卡洛方法更高效（more efficient）\n",
    "    - TD($\\lambda$)可以比蒙特卡洛方法高效得多（much more efficient）\n",
    "- 对于基于模拟的搜索来说，自举也是有帮助的（for simulation-based search, bootstrapping is also helpful）\n",
    "    - 时差搜索法减少了方差但是增加了偏差（TD search reduces variance but increases bias）\n",
    "    - 时差法搜索通常比蒙特卡洛搜索更高效（more efficient）\n",
    "    - TD($\\lambda$)搜索可以比蒙特卡洛搜索高效得多（much more efficient）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 时差搜索法（TD Search）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 从当下的（真实）状态$s_t$开始回合的模拟（simulate episodes from the current (real) state $s_t$）\n",
    "- 给动作价值函数$Q(s,a)$估值\n",
    "- 在模拟的每一步都用Sarsa更新动作价值\n",
    "\\begin{equation}\n",
    "\\Delta Q(S,A)=\\alpha (R+\\gamma Q(S',A')-Q(S,A))\n",
    "\\end{equation}\n",
    "\n",
    "- 基于动作价值$Q(s,a)$选择动作\n",
    "    - 比如说用$\\epsilon$-贪心方法\n",
    "- 我们也可以对$Q$使用函数近似（function approximation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dyna-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在Dyna-2中，智能体存储了两套特征权重（the agent stores two sets of feature weights）\n",
    "    - 长期记忆（long-term memory）\n",
    "    - 短期记忆，也就是工作记忆（short-term memory, or working memory）\n",
    "- 长期记忆是用时差学习法基于真实经历来更新的（long-term memory is updated from real experience using TD learning）\n",
    "    - 在任何的回合中应用普适的领域信息（general domain knowledge that applies to any episode）\n",
    "- 短期记忆是用时差搜索基于模拟经历来更新的（short-term memory is updated from simulated experience using TD search）\n",
    "    - 关于当下情况的明确信息（specific local knowledge about the current situation）\n",
    "- 价值函数是长期和短期记忆之和（over value function is sum of long and short-term memories）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在围棋中时差搜索的结果（Results of TD Search in Go）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/results_of_td_search_in_go.png\" style=\"width: 500px'\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初次编辑日期"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年4月24日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/dyna.pdf\n",
    "\n",
    "[2] https://www.youtube.com/watch?v=ItMutbeOHtc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

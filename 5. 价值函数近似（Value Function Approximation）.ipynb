{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 大规模强化学习（Large-Scale Reinforcement Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强化学习可以被用来解决大规模的问题（large problems），比如说\n",
    "\n",
    "- 西洋双陆棋（Backgammon）：$10^{20}$个状态\n",
    "- 围棋（Game of Go）：$10^{170}$个状态\n",
    "- 直升飞机（Helicopter）：连续状态空间（continuous state space）\n",
    "\n",
    "那我们怎样把无模型的预测和控制方法扩展到更大规模的问题上呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 价值函数近似（Value Function Approximation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 到现在为止我们都用表格形式来表示价值函数（represent value function by a lookup table）\n",
    "    - 每个状态$s$都有一个记录（entry）$V(s)$\n",
    "    - 或者每个状态-动作组合（state-action pair）$s, a$ 都有一个记录（entry）$Q(s, a)$\n",
    "- 大规模MDP的问题：\n",
    "    - 要存入内存中的状态和动作太多了\n",
    "    - 学习每个状态价值的速度太慢了\n",
    "- 解决大规模MDP的方案：\n",
    "    - 用函数近似（function approximation）来估计价值函数\n",
    "    \\begin{equation}\n",
    "    \\hat{v}(s,\\textbf{w})\\approx v_{\\pi}(s) \\\\\n",
    "    or \\enspace \\hat{q}(s,a,\\textbf{w})\\approx q_{\\pi}(s,a)\n",
    "    \\end{equation}\n",
    "    - 从遇到过的状态（seen states）泛化（generalise）到没有遇到过的状态（unseen states）\n",
    "    - 用MC或是TD学习来更新参数$\\textbf{w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 价值函数近似的种类（Types of Value Function Approximation）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/types_of_value_function_approximation.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 选择哪种方程近似方法呢？（Which Function Approximator?）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们有许多种方程近似的方法，比如说\n",
    "\n",
    "- 特征的线性组合（linear combinations of features）\n",
    "- 神经网络（neural network）\n",
    "- 决策树（decision tree）\n",
    "- 邻近取样（nearest neighbour）\n",
    "- 傅里叶/小波基（Fourier/wavelet bases）\n",
    "- $\\ldots$\n",
    "\n",
    "我们在这里只考虑可微分的方程近似方法（differentiable function approximators）\n",
    "\n",
    "- 特征的线性组合（linear combinations of features）\n",
    "- 神经网络（neural network）\n",
    "\n",
    "此外，我们还需要适合非静态、非-iid数据（non-stationary, non-idependent and identically distributed data）的的训练方法（training method）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度下降法（Gradient Descent）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 让我们假设现在我们有一个关于参数向量（parameter vector）$\\textbf{w}$可微分的函数（differentiable function）$J(\\textbf{w})$\n",
    "- 我们把$J(\\textbf{w})$的梯度（gradient）定义为\n",
    "\\begin{equation}\n",
    "\\nabla_{\\textbf{w}}J(\\textbf{w})\n",
    "=\\left(\n",
    "\\begin{array}{lr}\n",
    "\\begin{split}\n",
    "\\frac{\\partial J(\\textbf{w})}{\\partial \\textbf{w}_1} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial J(\\textbf{w})}{\\partial \\textbf{w}_n}\n",
    "\\end{split}\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\end{equation}\n",
    "\n",
    "- 我们想要找到$J(\\textbf{w})$的局部最小值\n",
    "- 向梯度的负方向（direction of -ve gradient）调节$\\textbf{w}$\n",
    "\\begin{equation}\n",
    "\\Delta\\textbf{w}=-\\frac{1}{2}\\alpha\\nabla_{\\textbf{w}}J(\\textbf{w})\n",
    "\\end{equation}\n",
    "\n",
    "$\\quad\\alpha$在这里是步长参数（step-size parameter）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用随机梯度下降法进行价值函数近似（Value Function Approximation by Stochastic Gradient Descent）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 目标：搜寻参数向量$\\textbf{w}$来最小化近似价值函数（approximate value function）$\\hat{v}(s,\\textbf{w})$和真实价值函数（true value function）$v_{\\pi}(s)$之差的均方误差（mean-squared error）\n",
    "\\begin{equation}\n",
    "J(\\textbf{w})=\\mathbb{E}_{\\pi}[(v_{\\pi}(S)-\\hat{v}(S, \\textbf{w}))^2]\n",
    "\\end{equation}\n",
    "\n",
    "- 梯度下降可以找到局部最小值（local minimum）\n",
    "\\begin{split}\n",
    "& \\Delta \\textbf{w} & = - \\frac{1}{2}\\alpha\\nabla_{\\textbf{w}}J(\\textbf{w}) \\\\\n",
    "& & = \\alpha \\mathbb{E}_{\\pi}[(v_{\\pi}(S)-\\hat{v}(S,\\textbf{w}))\\nabla_{\\textbf{w}}\\hat{v}(S,\\textbf{w})]\n",
    "\\end{split}\n",
    "\n",
    "- 随机梯度下降在梯度上取样（stochastic gradient descent samples the gradient）\n",
    "\\begin{equation}\n",
    "\\Delta \\textbf{w} = \\alpha (v_{\\pi}(S)-\\hat{v}(S,\\textbf{w}))\\nabla_{\\textbf{w}}\\hat{v}(S,\\textbf{w})\n",
    "\\end{equation}\n",
    "\n",
    "- 期望更新与全梯度更新相等（expected update is equal to full gradient update）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征向量（Feature Vectors）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用一个特征向量（feature vector）来表示状态\n",
    "\\begin{equation}\n",
    "\\textbf{x}(S)\n",
    "=\\left(\n",
    "\\begin{split}\n",
    "& \\textbf{x}_1 (S) \\\\\n",
    "& \\enspace \\vdots \\\\\n",
    "& \\textbf{x}_n (S)\n",
    "\\end{split}\n",
    "\\right)\n",
    "\\end{equation}\n",
    "\n",
    "- 比如说\n",
    "    - 机器人到地标的距离\n",
    "    - 股市的趋势\n",
    "    - 国际象棋中棋子的位置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性价值函数近似（Linear Value Function Approximation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用特征的线性组合来表示价值函数（represent value function by a linear combination of features）\n",
    "\\begin{equation}\n",
    "\\hat{v}(S,\\textbf{w})=\\textbf{x}(S)^T \\textbf{w} = \\sum_{j=1}^{n}\\textbf{x}_j (S)\\textbf{w}_j\n",
    "\\end{equation}\n",
    "\n",
    "- 目标函数（objective function）对参数$\\textbf{w}$来说是一个二次函数（quadratic）\n",
    "\\begin{equation}\n",
    "J(\\textbf{w})=\\mathbb{E}_{\\pi}[(v_{\\pi}(S)-\\textbf{x}(S)^T\\textbf{w})^2]\n",
    "\\end{equation}\n",
    "\n",
    "- 随机梯度下降会收敛到全局最优值（global optimum）\n",
    "- 更新公式非常简单\n",
    "\\begin{align}\n",
    "\\nabla_{\\textbf{w}}\\hat{v}(S,\\textbf{w}) & =\\textbf{x}(S) \\\\\n",
    "\\Delta \\textbf{w} & = \\alpha (v_{\\pi}(S)-\\hat{v}(S,\\textbf{w}))\\textbf{x}(S)\n",
    "\\end{align}\n",
    "\n",
    "$\\quad 更新值=步长 \\times 预测误差 \\times 特征值$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查表法的特征（Table Lookup Features）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 查表法（table lookup）是线性价值函数近似的一个特例\n",
    "- 查表法所使用的特征（table lookup features）\n",
    "\\begin{equation}\n",
    "\\textbf{x}^{table}(S)\n",
    "=\\left(\n",
    "\\begin{split}\n",
    "& \\textbf{1}(S=s_1) \\\\\n",
    "& \\quad\\enspace \\vdots \\\\\n",
    "& \\textbf{1}(S=s_n)\n",
    "\\end{split}\n",
    "\\right)\n",
    "\\end{equation}\n",
    "\n",
    "- 参数向量$\\textbf{w}$对每个状态都有一个对应的值\n",
    "\\begin{equation}\n",
    "\\hat{v}(S,\\textbf{w})\n",
    "=\\left(\n",
    "\\begin{split}\n",
    "& \\textbf{1}(S=s_1) \\\\\n",
    "& \\quad\\enspace\\vdots \\\\\n",
    "& \\textbf{1}(S=s_n)\n",
    "\\end{split}\n",
    "\\right)\n",
    "\\cdot\n",
    "\\left(\n",
    "\\begin{split}\n",
    "& \\textbf{w}_1 \\\\\n",
    "& \\vdots \\\\\n",
    "& \\textbf{w}_n\n",
    "\\end{split}\n",
    "\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 渐进式预测算法（Incremental Prediction Algorithms）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 通常的假设是真实的价值函数$v_{\\pi}(s)$是由监督者（supervisor）给出的\n",
    "- 但在强化学习中我们并没有监督者（supervisor），我们只有奖励（rewards）\n",
    "- 在实际的应用中，我们用目标值（target）替代$v_{\\pi}(s)$\n",
    "    - 对于MC来说，目标就是回报$G_t$\n",
    "    \\begin{equation}\n",
    "    \\Delta \\textbf{w}=\\alpha (G_t - \\hat{v}(S_t,\\textbf{w}))\\nabla_{\\textbf{w}}\\hat{v}(S_t,\\textbf{w})\n",
    "    \\end{equation}\n",
    "    \n",
    "    - 对于TD(0)来说，目标就是TD目标$R_{t+1}+\\gamma\\hat{v}(S_{t+1},\\textbf{w})$\n",
    "    \\begin{equation}\n",
    "    \\Delta \\textbf{w}=\\alpha (R_{t+1}+\\gamma\\hat{v}(S_{t+1},\\textbf{w}) - \\hat{v}(S_t,\\textbf{w}))\\nabla_{\\textbf{w}}\\hat{v}(S_t,\\textbf{w})\n",
    "    \\end{equation}\n",
    "    \n",
    "    - 对于TD($\\lambda$)来说，目标就是$\\lambda$-回报$G_t^{\\lambda}$\n",
    "    \\begin{equation}\n",
    "    \\Delta \\textbf{w}=\\alpha (G_t^{\\lambda} - \\hat{v}(S_t,\\textbf{w}))\\nabla_{\\textbf{w}}\\hat{v}(S_t,\\textbf{w})\n",
    "    \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于价值函数近似的蒙特卡洛法（Monte-Carlo with Value Function Approximation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 回报$G_t$是一个关于真实价值$v_{\\pi}(S_t)$的无偏并且含噪音的样本（unbiased, noisy sample）\n",
    "- 因为我们可以用监督学习的方法来对“训练数据”（\"training data\"）进行处理\n",
    "\\begin{equation}\n",
    "<S_1,G_1>,<S_2,G_2>,\\ldots ,<S_T,G_T>\n",
    "\\end{equation}\n",
    "\n",
    "- 比如说，我们可以用线性蒙特卡洛策略评估（linear Monte-Carlo policy evaluation）\n",
    "\\begin{align}\n",
    "\\Delta \\textbf{w} & = \\alpha (G_t - \\hat{v}(S_t, \\textbf{w}))\\nabla_{\\textbf{w}}\\hat{v}(S_t,\\textbf{w}) \\\\\n",
    "& = \\alpha (G_t - \\hat{v}(S_t,\\textbf{w}))\\textbf{x}(S_t)\n",
    "\\end{align}\n",
    "\n",
    "- 蒙特卡洛评估收敛到局部最优值（local optimum）\n",
    "- 就算是使用非线性价值函数近似（non-linear value function approximation）时也一样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于价值函数近似的TD学习（TD Learning with Value Function Approximation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TD-目标$R_{t+1}+\\gamma\\hat{v}(S_{t+1},\\textbf{w})$是真实价值$v_{\\pi}(S_t)$的有偏样本（biased sample）\n",
    "- 我们仍然可以在“训练数据”上使用监督学习的方法：\n",
    "\\begin{equation}\n",
    "<S_1, R_2 + \\gamma\\hat{v}(S_2,\\textbf{w})>,<S_2,R_3 +\\gamma\\hat{v}(S_3,\\textbf{w})>,\\ldots ,<S_{T-1},R_T>\n",
    "\\end{equation}\n",
    "\n",
    "- 比如说，我们可以使用线性TD(0)（linear TD(0)）\n",
    "\\begin{align}\n",
    "\\Delta \\textbf{w} & = \\alpha (R+\\gamma\\hat{v}(S',\\textbf{w})-\\hat{v}(S,\\textbf{w}))\\nabla_{\\textbf{w}}\\hat{v}(S,\\textbf{w}) \\\\\n",
    "& = \\alpha\\delta\\textbf{x}(S)\n",
    "\\end{align}\n",
    "\n",
    "- 线性TD(0)收敛到（接近于）全局最优（global optimum）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于价值函数近似的TD($\\lambda$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\lambda$-回报$G_t^{\\lambda}$同样也是真实价值$v_{\\pi}(s)$的有偏样本（biased sample）\n",
    "- 我们仍然可以在“训练数据”上使用监督学习的方法：\n",
    "\\begin{equation}\n",
    "<S_1,G_1^{\\lambda}>,<S_2,G_2^{\\lambda}>,\\ldots,<S_{T-1},G_{T-1}^{\\lambda}>\n",
    "\\end{equation}\n",
    "\n",
    "- 前视线性TD($\\lambda$)（forward-view linear TD($\\lambda$)）\n",
    "\\begin{align}\n",
    "\\Delta\\textbf{w} & = \\alpha (G_t^{\\lambda}-\\hat{v}(S_t,\\textbf{w}))\\nabla_{\\textbf{w}}\\hat{v}(S_t,\\textbf{w}) \\\\\n",
    "& = \\alpha (G_t^{\\lambda}-\\hat{v}(S_t,\\textbf{w}))\\textbf{x}(S_t)\n",
    "\\end{align}\n",
    "\n",
    "- 后视线性TD($\\lambda$)（backward-view linear TD($\\lambda$)）\n",
    "\\begin{align}\n",
    "\\delta_t & = R_{t+1}+\\gamma\\hat{v}(S_{t+1},\\textbf{w})-\\hat{v}(S_t,\\textbf{w}) \\\\\n",
    "E_t & = \\gamma\\lambda E_{t-1}+\\textbf{x}(S_t) \\\\\n",
    "\\Delta\\textbf{w} & = \\alpha\\delta_t E_t\n",
    "\\end{align}\n",
    "\n",
    "前视和后视的线性TD($\\lambda$)是等价的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于价值函数近似的控制（Control with Value Function Approximation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/control_with_value_function_approximation.png\" style=\"width: 300px;\" />\n",
    "\n",
    "策略评估： 近似策略评估（approximate policy evaluation），$\\hat{q}(\\cdot,\\cdot,\\textbf{w})\\approx q_{\\pi}$\n",
    "\n",
    "策略改进： $\\epsilon$-贪心策略改进"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 动作价值函数近似（Action-Value Function Approximation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对动作价值函数（action-value function）进行方程近似\n",
    "\\begin{equation}\n",
    "\\hat{q}(S,A,\\textbf{w})\\approx q_{\\pi}(S,A)\n",
    "\\end{equation}\n",
    "\n",
    "- 对近似动作价值函数（approximate action-value function）$\\hat{q}(S,A,\\textbf{w})$和真实动作价值函数（true action-value function）$q_{\\pi}(S,A)$之差的均方误差（mean-squared error）进行最小化\n",
    "\\begin{equation}\n",
    "J(\\textbf{w})=\\mathbb{E}_{\\pi}[(q_{\\pi}(S,A)-\\hat{q}(S,A,\\textbf{w}))^2]\n",
    "\\end{equation}\n",
    "\n",
    "- 用随机梯度下降来搜寻局部最小值\n",
    "\\begin{align}\n",
    "-\\frac{1}{2}\\nabla_{\\textbf{w}}J(\\textbf{w}) & = (q_{\\pi}(S,A)-\\hat{q}(S,A,\\textbf{w}))\\nabla_{\\textbf{w}}\\hat{q}(S,A,\\textbf{w}) \\\\\n",
    "\\Delta\\textbf{w} & = \\alpha(q_{\\pi}(S,A)-\\hat{q}(S,A,\\textbf{w}))\\nabla_{\\textbf{w}}\\hat{q}(S,A,\\textbf{w})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性动作价值函数近似（Linear Action-Value Function Approximation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用一个特征向量（feature vector）来表示状态和动作\n",
    "\\begin{equation}\n",
    "\\textbf{x}(S,A)\n",
    "=\\left(\n",
    "\\begin{split}\n",
    "& \\textbf{x}_1 (S,A) \\\\\n",
    "& \\quad \\vdots \\\\\n",
    "& \\textbf{x}_n (S,A)\n",
    "\\end{split}\n",
    "\\right)\n",
    "\\end{equation}\n",
    "\n",
    "- 用特征的线性组合来表示动作价值函数\n",
    "\\begin{equation}\n",
    "\\hat{q} (S,A,\\textbf{w})=\\textbf{x} (S,A)^T\\textbf{w}=\\sum_{j=1}^{n}\\textbf{x}_j (S,A)\\textbf{w}_j\n",
    "\\end{equation}\n",
    "\n",
    "- 用随机梯度下降进行更新\n",
    "\\begin{split}\n",
    "\\nabla_{\\textbf{w}}\\hat{q}(S,A,\\textbf{w}) & = \\textbf{x}(S,A) \\\\\n",
    "\\Delta \\textbf{w} & = \\alpha (q_{\\pi}(S,A) - \\hat{q}(S,A,\\textbf{w}))\\textbf{x}(S,A)\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 渐进式控制算法（Incremental Control Algorithm）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 就像预测一样，我们必须用目标值（target）替代$q_{\\pi}(S,A)$\n",
    "    - 对于MC来说，目标值就是回报$G_t$\n",
    "    \\begin{equation}\n",
    "    \\Delta\\textbf{w} = \\alpha (G_t - \\hat{q}(S_t,A_t,\\textbf{w}))\\nabla_{\\textbf{w}}\\hat{q}(S_t,A_t,\\textbf{w})\n",
    "    \\end{equation}\n",
    "    \n",
    "    - 对于TD(0)来说，目标值就是TD目标$R_{t+1}+\\gamma Q(S_{t+1},A_{t+1})$\n",
    "    \\begin{equation}\n",
    "    \\Delta\\textbf{w} = \\alpha (R_{t+1}+\\gamma\\hat{q}(S_{t+1},A_{t+1},\\textbf{w}) - \\hat{q}(S_t,A_t,\\textbf{w}))\\nabla_{\\textbf{w}}\\hat{q}(S_t,A_t,\\textbf{w})\n",
    "    \\end{equation}\n",
    "    \n",
    "    - 对于前视TD($\\lambda$)来说，目标值就是动作价值的$\\lambda$-回报\n",
    "    \\begin{equation}\n",
    "    \\Delta\\textbf{w} = \\alpha (q_t^{\\lambda}-\\hat{q}(S_t,A_t,\\textbf{w}))\\nabla_{\\textbf{w}}\\hat{q}(S_t,A_t,\\textbf{w})\n",
    "    \\end{equation}\n",
    "    \n",
    "    - 对于后视TD($\\lambda$)来说，等价的更新是\n",
    "    \\begin{align}\n",
    "    \\delta_t & = R_{t+1}+\\gamma\\hat{q}(S_{t+1},A_{t+1},\\textbf{w})-\\hat{q}(S_t,A_t,\\textbf{w}) \\\\\n",
    "    E_t & = \\gamma\\lambda E_{t-1} + \\nabla_{\\textbf{w}}\\hat{q}(S_t,A_t,\\textbf{w}) \\\\\n",
    "    \\Delta \\textbf{w} & = \\alpha \\delta_t E_t\n",
    "    \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于粗编码的线性Sarsa在过山车范例中的应用（Linear Sarsa with Coarse Coding in Mountain Car）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/linear_sarsa_with_coarse_coding_in_mountain_car.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于径向基函数的线性Sarsa在过山车范例中的应用（Linear Sarsa with Radial Basis Functions in Mountain Car）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/linear_sarsa_with_radial_basis_functions_in_mountain_car.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

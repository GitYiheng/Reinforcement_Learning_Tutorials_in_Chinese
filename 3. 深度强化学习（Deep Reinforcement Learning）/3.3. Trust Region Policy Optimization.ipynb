{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an infinite-horizon discounted Markov decision process (MDP), defined by the tuple $(\\mathcal{S}, \\mathcal{A}, P, r, \\rho_0, \\gamma)$, where $\\mathcal{S}$ is a finite set of states, $\\mathcal{A}$ is a finite set of actions, $P:\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S}\\rightarrow\\mathbb{R}$ is the transition probability distribution, $r: \\mathcal{S}\\rightarrow\\mathbb{R}$ is the reward function, $\\rho_0 :\\mathcal{S}\\rightarrow\\mathbb{R}$ is the distribution of the initial state $s_0$, and $\\gamma \\in (0, 1)$ is the discount factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\pi$ denote a stochastic policy $\\pi : \\mathcal{S}\\times\\mathcal{A}\\rightarrow [0, 1]$, and let $\\eta (\\pi)$ denote its expected discounted reward:\n",
    "\n",
    "$\\quad \\eta (\\pi)=\\mathbb{E}_{s_0,a_0,\\ldots}\\left[ \\sum_{t=0}^{\\infty}\\gamma^t r(s_t) \\right]$, where\n",
    "\n",
    "$\\quad s_0 \\sim \\rho_0 (s_0)$, $a_t \\sim \\pi (a_t \\mid s_t)$, $s_{t+1} \\sim P(s_{t+1}\\mid s_t , a_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following standard definitions of the state-action value function $Q_{\\pi}$, the value function $V_{\\pi}$, and the advatage function $A_{\\pi}$:\n",
    "\n",
    "$\\quad Q_{\\pi}(s_t, a_t)=\\mathbb{E}_{s_{t+1},a_{t+1},\\ldots}\\left[ \\sum_{l=0}^{\\infty} \\gamma^l r(s_{t+l}) \\right]$,\n",
    "\n",
    "$\\quad V_{\\pi}(s_t)=\\mathbb{E}_{a_t,s_{t+1},\\ldots}\\left[ \\sum_{l=0}^{\\infty} \\gamma^l r(s_{t+l}) \\right]$,\n",
    "\n",
    "$\\quad A_{\\pi}(s,a)=Q_{\\pi}(s,a)-V_{\\pi}(s)$, where\n",
    "\n",
    "$\\quad a_t \\sim \\pi(a_t\\mid s_t)$, $s_{t+1}\\sim P(s_{t+1}\\mid s_t, a_t)$ for $t \\geq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following useful identity expresses the expected return of another policy $\\tilde{\\pi}$ in terms of the advantage over $\\pi$, accumulated over timesteps:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:eq}\n",
    "\\eta (\\tilde{\\pi}) = \\eta (\\pi) + \\mathbb{E}_{s_0,a_0,\\ldots \\sim \\tilde{\\pi}} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t A_{\\pi}(s_t, a_t) \\right]\n",
    "\\end{equation}\n",
    "\n",
    "where the notation $\\mathbb{E}_{s_0,a_0,\\ldots \\sim \\tilde{\\pi}}\\left[ \\ldots \\right]$ indicates that actions are sampled $a_t \\sim \\tilde{\\pi}(\\cdot \\mid s_t)$. Let $\\rho_{\\pi}$ be the (unnormalized) discounted visitation frequencies\n",
    "\n",
    "$\\rho_{\\pi}(s)=P(s_0 =s)+\\gamma P(s_1 =s)+\\gamma^2 P(s_2 =s)+\\ldots$,\n",
    "\n",
    "where $s_0 \\sim \\rho_0$ and the actions are chosen according to $\\pi$. We can rewrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

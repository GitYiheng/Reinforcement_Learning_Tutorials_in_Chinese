{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "策略梯度方法（policy gradient methods）是最近把深度神经网络应用在控制方面取得突破的基础，应用包括电子游戏、3D运动和围棋。但是因为策略梯度方法对步长（stepsize）的选择非常敏感，所以通过使用策略梯度方法得到好的结果是非常具有挑战性的。如果步长太小，进展就会很慢；如果步长太大信号就会被噪音盖过，或者性能就会经历灾难性的下降。它们的样本效率也常常很低，学习一个简单的任务需要百万帧的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "研究人员尝试了通过约束或者优化策略更新的步长，使用比如像TRPO和ACER这些方法消除这些缺陷。这些方法都有各自的取舍。ACER（actor-critic with experience replay）的性能在Atari benchmark上只比PPO好一些，但算法比PPO复杂得多，需要额外的代码来做离线更正（off-policy corrections）和一个重放缓存（a replay buffer）。TRPO（trust region policy optimization）尽管在连续控制任务中非常有用，但与共享参数的算法的兼容性并不好，这里是指在那些视觉输入非常重要的任务中策略和价值函数或是辅助损失（auxiliary losses）的参数共享，比如Atari。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以用监督学习轻松地实现成本函数并在它上面使用梯度下降，而且我们很有信心在较少调参的前提下会得到很好的结果。强化学习的成功之路并没有那么明显，算法有许多难以调试的动态部分，取得较好的结果这往往需要大量调参方面的努力。PPO在实现的简易程度、样本复杂度和调参简易度上取得了相对的平衡，确保了计算单步更新时在最小化成本函数的同时保证了与之前策略的偏差也较小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初次编辑日期"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年4月30日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] https://blog.openai.com/openai-baselines-ppo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

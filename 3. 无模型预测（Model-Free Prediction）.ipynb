{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 无模型强化学习（Model-Free Reinforcement Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用动态规划来进行规划\n",
    "    - 解一个已知的MDP\n",
    "\n",
    "- 无模型预测（model-free prediction）\n",
    "    - 估计一个未知MDP的价值函数\n",
    "\n",
    "- 无模型控制（model-free control）\n",
    "    - 优化一个未知MDP的价值函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蒙特卡洛强化学习（Monte-Carlo Reinforcement Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MC方法直接从经验集（episodes of experience）上学习\n",
    "\n",
    "- MC是无模型的（model-free）：没有MDP状态转移和奖励的知识\n",
    "\n",
    "- MC从完整的回合（complete episodes）中学习：没有用到自举（bootstrapping）\n",
    "\n",
    "- MC用了最简单的思想：价值等于平均回报（value=mean return）\n",
    "\n",
    "- 警告：MC只能用在回合制的MDP上（episodic MDPs）\n",
    "    - 所有的回合必须终止"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蒙特卡洛策略评估（Monte-Carlo Policy Evaluation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 目标：从使用策略$\\pi$的回合经验中学习$v_{\\pi}$\n",
    "\n",
    "\\begin{equation}\n",
    "S_1, A_1, R_2, \\ldots, S_k \\sim \\pi\n",
    "\\end{equation}\n",
    "\n",
    "- 回报（return）是总折扣奖励（total discounted reward）：\n",
    "\n",
    "\\begin{equation}\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\ldots + \\gamma^{T-1} R_T\n",
    "\\end{equation}\n",
    "\n",
    "- 价值函数（value function）就是期望回报（expected return）：\n",
    "\n",
    "\\begin{equation}\n",
    "v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t | S_t = s]\n",
    "\\end{equation}\n",
    "\n",
    "- 蒙特卡洛策略评估使用了经验平均回报（empirical mean return）而不是期望回报（expected return）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 首访蒙特卡洛策略评估（First-Visit Monte-Carlo Policy Evaluation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这是用来评估状态$s$\n",
    "\n",
    "- 状态$s$在一个回合中被访问的首个时间步长（first time-step）$t$\n",
    "\n",
    "- 增量计数器（increment counter） $N(s) \\leftarrow N(s)+1$\n",
    "\n",
    "- 增量总回报（increment total return） $S(s) \\leftarrow S(s) + G_t$\n",
    "\n",
    "- 用平均回报来估计价值 $V(s) = S(s)/N(s)$\n",
    "\n",
    "- 根据大数定理（law of large numbers），在$N(s) \\rightarrow \\infty$时$V(s) \\rightarrow v_{\\pi}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 每访蒙特卡洛策略评估（Every-Visit Monte-Carlo Policy Evaluation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这是用来评估状态$s$\n",
    "\n",
    "- 状态$s$在一个回合中每个被访问的步长（every time-step）$t$\n",
    "\n",
    "- 增量计数器（increment counter） $N(s) \\leftarrow N(s)+1$\n",
    "\n",
    "- 增量总回报（increment total return） $S(s) \\leftarrow S(s) + G_t$\n",
    "\n",
    "- 用平均回报来估计价值 $V(s) = S(s)/N(s)$\n",
    "\n",
    "- 同样的，在$N(s) \\rightarrow \\infty$时$V(s) \\rightarrow v_{\\pi}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 范例-二十一点（Blackjack Example）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 状态（200个）\n",
    "    - 当前总和（12-21）\n",
    "    - 庄家的明牌（A-10）\n",
    "    - 我有没有一张“可用的”A（yes-no）\n",
    "- 停牌（stick）：停止要牌（结束）\n",
    "- 要牌（twist）：再加一张牌（不换牌）\n",
    "- 停牌的奖励：\n",
    "    - 如果玩家的牌面点数之和$>$庄家的牌面点数之和，奖励为+1\n",
    "    - 如果玩家的牌面点数之和$=$庄家的牌面点数之和，奖励为0\n",
    "    - 如果玩家的牌面点数之和$<$庄家的牌面点数之和，奖励为-1\n",
    "- 要牌的奖励：\n",
    "    - 如果玩家的牌面点数之和$>$21（结束），奖励为-1\n",
    "    - 其它情况下，奖励为0\n",
    "- 状态转移：如果牌面点数之和$<$12，自动要牌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21点在蒙特卡洛学习后的价值函数（Blackjack Value Function after Monte-Carlo Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "策略：如果牌面点数之和$\\geq$20的话选择停牌，否则选择要牌\n",
    "\n",
    "<img src=\"files/figures/blackjack.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 平均增量（Incremental Mean）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "序列$x_1, x_2, \\ldots$的平均值$\\mu_1, \\mu_2, \\ldots$可以被逐步计算，\n",
    "\n",
    "\\begin{align}\n",
    "\\mu_k & = \\frac{1}{k}\\sum_{j=1}^k x_j \\\\\n",
    "& = \\frac{1}{k}(x_k + \\sum_{j=1}^{k-1} x_j) \\\\\n",
    "& = \\frac{1}{k}(x_k + (k-1)\\mu_{k-1}) \\\\\n",
    "& = \\mu_{k-1} + \\frac{1}{k}(x_k - \\mu_{k-1})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蒙特卡洛逐步更新（Incremental Monte-Carlo Updates）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在每个回合$S_1, A_1, R_2, \\ldots, S_T$后逐步更新$V(s)$\n",
    "\n",
    "- 对每个回报为$G_t$的状态$S_t$\n",
    "\n",
    "\\begin{align}\n",
    "& N(S_t) \\leftarrow N(S_t)+1 \\\\\n",
    "& V(S_t) \\leftarrow V(S_t) + \\frac{1}{N(S_t)}(G_t - V(S_t))\n",
    "\\end{align}\n",
    "\n",
    "- 在非静态问题上（non-stationary problems），跟踪移动平均值（running mean）是非常有用的，也就是忘掉旧的回合（old episodes）\n",
    "\n",
    "\\begin{equation}\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha (G_t - V(S_t))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 时差学习（Temporal-Difference Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TD方法直接从经验集（episodes of experience）上学习\n",
    "- TD是无模型的（model-free）：没有MDP状态转移和奖励的知识\n",
    "- TD用自举（bootstrapping）从不完整的回合（incomplete episodes）中学习\n",
    "- TD从一个估计值向一个估计值的方向上更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蒙特卡洛和时差法（MC and TD）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 目标：在用策略$\\pi$的前提下从经验中在线（online）学习$v_{\\pi}$\n",
    "\n",
    "- 增量每访蒙特卡洛（incremental every-visit Monte-Carlo）\n",
    "    - 在价值$V(S_t)$的基础上朝实际回报（actual return）$G_t$的方向更新\n",
    "\n",
    "\\begin{equation}\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha (G_t - V(S_t))\n",
    "\\end{equation}\n",
    "\n",
    "- 最简单的时差学习算法：TD(0)\n",
    "    - 在价值$V(S_t)$的基础上朝回报估计值（estimated return）的方向更新$R_{t+1} + \\gamma V(S_{t+1})$\n",
    "\n",
    "    \\begin{equation}\n",
    "    V(S_t) \\leftarrow V(S_t) + \\alpha (R_{t+1} + \\gamma V(S_{t+1}) - V(S_t))\n",
    "    \\end{equation}\n",
    "    \n",
    "    - $R_{t+1} + \\gamma V(S_{t+1})$被叫作TD目标（TD target）\n",
    "    - $\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$被叫作TD误差（TD error）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 范例-开车回家（Driving Home Example）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/driving_home_1.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/driving_home_2.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蒙特卡洛和时差法的优劣（1）（Advantages and Disadvantages of MC vs TD）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TD可以在知道最终结果前就学习\n",
    "    - TD可以在每一步后在线学习\n",
    "    - MC必须等到回合结束也就是回报是已知的时候\n",
    "- TD可以在没有最终结果的时候进行学习\n",
    "    - TD可以从未完成的序列中学习\n",
    "    - MC只能从完成的序列中学习\n",
    "    - TD在连续的（无终止的）环境中也可以使用\n",
    "    - MC只能在回合制的（会终止的）环境中使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在偏差和方差中的权衡（Bias/Variance Trade-Off）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 回报$G_t = R_{t+1} + \\gamma R_{t+2} + \\ldots + \\gamma^{T-1} R_T$是$v_{\\pi}(S_t)$的无偏估计（unbiased estimate）\n",
    "- 真实的TD目标$R_{t+1}+\\gamma v_{\\pi}(S_{t+1})$是$v_{\\pi}(S_t)$的无偏估计（unbiased estimate）\n",
    "- TD目标$R_{t+1}+\\gamma V(S_{t+1})$是$v_{\\pi}(S_t)$的有偏估计（biased estimate）\n",
    "- 与回报相比，TD目标的方差要小得多：\n",
    "    - 回报取决于许多随机的动作、状态转移和奖励\n",
    "    - TD目标取决于一个随机的动作、状态转移和奖励"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蒙特卡洛和时差法的优劣（2）（Advantages and Disadvantages of MC vs TD）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MC的特点是高方差和零偏差（high variance, zero bias）\n",
    "    - 有非常好的收敛特性\n",
    "    - （就算用函数逼近也有非常好的收敛特性）\n",
    "    - 对初始价值不是非常敏感\n",
    "    - 理解和使用起来非常简单\n",
    "- TD的特点是低方差和有一些偏差（low variance, some bias）\n",
    "    - 通常比MC要高效许多\n",
    "    - TD(0)收敛到$v_{\\pi}(s)$\n",
    "    - （但在使用函数逼近时不总是收敛）\n",
    "    - 对初始值更加敏感"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 范例-随机行走（Random Walk Example）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/random_walk_1.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/random_walk_2.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 批量蒙特卡洛和时差法（Batch MC and TD）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MC和TD的收敛情况：当经验$\\rightarrow \\infty$时$V(s) \\rightarrow v_{\\pi}(s)$\n",
    "- 但有限经验的批量解的情况是怎么样的呢？\n",
    "\\begin{equation}\n",
    "s_1^1, a_1^1, r_2^1, \\ldots, s_{T_1}^1 \\\\\n",
    "\\vdots \\\\\n",
    "s_1^K, a_1^K, r_2^K, \\ldots, s_{T_K}^K\n",
    "\\end{equation}\n",
    "    - 也就是重复样本回合$k \\in [1, K]$\n",
    "    - 在回合$k$上用MC或者TD(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 范例-AB状态（AB Example）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有两个状态A和B；没有折扣（no discounting）；8个回合的经验\n",
    "\n",
    "<img src=\"files/figures/AB.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "那么$V(A)$和$V(B)$是多少呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初次编辑时间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年4月20日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MC-TD.pdf\n",
    "\n",
    "[2] https://www.youtube.com/watch?v=PnHCvfgC_ZA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

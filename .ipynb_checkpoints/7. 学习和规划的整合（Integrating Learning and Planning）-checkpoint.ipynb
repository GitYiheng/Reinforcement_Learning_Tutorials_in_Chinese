{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于模型的强化学习（Model-Based Reinforcement Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 策略梯度（基于策略的学习）：从经历中直接学习策略（learn policy directly from experience）\n",
    "- 基于价值函数的学习：从经历中直接学习价值函数（learn value function directly from experience）\n",
    "- 基于模型的学习：从经历中直接学习模型（learn model directly from experience）\n",
    "- 并且基于模型用规划（planning）来构造价值函数或是策略\n",
    "- 把学习和规划整合到同一个框架中（integrate learning and planning into a single architecture）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于模型和脱离模型的强化学习（Model-Based and Model-Free Reinforcement Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 脱离模型的强化学习（model-free RL）\n",
    "    - 没有模型\n",
    "    - 从经历中学习价值函数（和/或策略）\n",
    "- 基于模型的强化学习（model-based RL）\n",
    "    - 从经历中学习模型\n",
    "    - 在模型中规划价值函数（和/或策略）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 脱离模型的强化学习（Model-Free RL）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/model-free_rl.png\" style=\"width: 300px'\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于模型的强化学习（Model-Based RL）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/model-based_rl.png\" style=\"width: 300px'\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/model-based_rl_2.png\" style=\"width: 300px'\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于模型强化学习的优势（Advantages of Model-Based RL）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优势：\n",
    "\n",
    "- 可以通过监督学习的方法来高效地学习模型\n",
    "- 可以对模型的不确定性进行推理（reason about model uncertainty）\n",
    "\n",
    "劣势：\n",
    "\n",
    "- 先学习一个模型，再构建一个价值函数\n",
    "    - 也就是说有两个近似误差的来源（two sources of approximation error）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 什么是模型？（What is a Model?）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 模型$\\mathcal{M}$是一个用$\\eta$来参数化的MDP$<\\mathcal{S},\\mathcal{A},\\mathcal{P},\\mathcal{R}>$\n",
    "- 让我们假设状态空间$\\mathcal{S}$和动作空间$\\mathcal{A}$是已知的\n",
    "- 所以一个模型$\\mathcal{M}=<\\mathcal{P}_{\\eta},\\mathcal{R}_{\\eta}>$表示了状态转移（state transitions）$\\mathcal{P}_{\\eta}\\approx\\mathcal{P}$和奖励（rewards）$\\mathcal{R}_{\\eta}\\approx\\mathcal{R}$\n",
    "\\begin{align}\n",
    "S_{t+1} & \\sim \\mathcal{P}_{\\eta}(S_{t+1}\\mid S_t, A_t) \\\\\n",
    "R_{t+1} & = \\mathcal{R}_{\\eta}(R_{t+1}\\mid S_t, A_t)\n",
    "\\end{align}\n",
    "\n",
    "- 我们通常假设状态转移和奖励之间是条件独立的（conditional independence）\n",
    "\\begin{equation}\n",
    "    \\mathbb{P}[S_{t+1},R_{t+1}\\mid S_t,A_t]=\\mathbb{P}[S_{t+1}\\mid S_t,A_t]\\mathbb{P}[R_{t+1}\\mid S_t,A_t]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型学习（Model Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 目标：从经历$\\{ S_1,A_1,R_2,\\ldots,S_T \\}$中估计（estimate）出模型$\\mathcal{M}_{\\eta}$\n",
    "- 这是一个监督学习问题（a supervised learning problem）\n",
    "\\begin{align}\n",
    "S_1, S_1 & \\rightarrow R_2, S_2 \\\\\n",
    "S_2, S_2 & \\rightarrow R_3, S_3 \\\\\n",
    "& \\vdots \\\\\n",
    "S_{T-1}, A_{T-1} & \\rightarrow R_T, S_T\n",
    "\\end{align}\n",
    "\n",
    "- 学习$s, a \\rightarrow r$是一个回归问题（a regression problem）\n",
    "- 学习$s, a \\rightarrow s'$是一个密度估计问题（a density estimation problem）\n",
    "- 选择损失函数（loss function），比如说均方误差（mean-squared error）、KL散度（Kullback-Leibler divergence），$\\ldots$\n",
    "- 寻找最小化经验损失（empirical loss）的参数$\\eta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初次编辑日期"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年4月24日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/dyna.pdf\n",
    "\n",
    "[2] https://www.youtube.com/watch?v=ItMutbeOHtc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

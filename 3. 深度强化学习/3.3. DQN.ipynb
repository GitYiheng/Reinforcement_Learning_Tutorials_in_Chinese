{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用深度强化学习玩Atari游戏 (Playing Atari with Deep Reinforcement Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 标签 (Labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvard引用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D. and Riedmiller, M., 2013. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BibTex引用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@article{mnih2013playing,\n",
    "  title={Playing atari with deep reinforcement learning},\n",
    "  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},\n",
    "  journal={arXiv preprint arXiv:1312.5602},\n",
    "  year={2013}\n",
    "}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 摘要 (Abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.\n",
    "\n",
    "我们要介绍第一个用强化学习直接从高维传感器输入成功学习了控制策略的深度学习模型。这个模型是用Q-learning变体训练的一个卷积神经网络，这个模型的输入是原始像素，这个模型的输出的一个估计未来奖励的价值函数。我们在7个基于Arcade学习环境的Atari 2600游戏上运用了我们的方法，并且我们没有调整学习算法的结构。我们发现我们的算法在6个游戏中超过了所有之前的方法，并且在3个游戏中超过了人类专家。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 研究问题 (Research Problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to learn to control agents directly from high-dimensional visual inputs.\n",
    "\n",
    "怎样直接从高维视觉输入中学习智能体的控制策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主要贡献 (Contributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) DQN is the \"very first\" well-known work of applying deep learning techniques to reinforcement learning problems.\n",
    "\n",
    "(2) DQN adopts TD-Gammon's architecture of estimating the value function with neural networks (Tesauro, 1995).\n",
    "\n",
    "(3) DQN adopts an experience replay mechanism (Lin, 1993) to overcome the correlated data issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概要 (Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difficulty of combining reinforcement learning and deep learning\n",
    "### 结合强化学习和深度学习的难点\n",
    "\n",
    "Reinforcement learning presents several challenges from a deep learning perspective.\n",
    "\n",
    "Firstly, most successful deep learning applications to date have acquired large amounts of hand-labelled training data. Reinforcement learning algorithms, on the other hand, must be able to learn from a scalar reward signal that is frequently sparse, noisy and delayed. The delay between actions and resulting rewards, which can be thousands of timesteps long, seems particularly daunting when compared to the direct association between inputs and targets found in supervised learning.\n",
    "\n",
    "Another issue is that most deep learning algorithms assume the data samples to be independent, while in reinforcement learning one typically encounters sequences of highly correlated states.\n",
    "\n",
    "Furthermore, in reinforcement learning, the data distribution changes as the algorithm learns new behaviours, which can be problematic for deep learning methods that assume a fixed underlying distribution.\n",
    "\n",
    "从深度学习的角度看，强化学习面临了许多的挑战。\n",
    "\n",
    "首先，现在最成功的深度学习应用需要大量手工标注的训练数据。但强化学习算法必须能够从稀疏的、有噪音的和有延迟的标量奖励信号中学习。强化学习中的动作和奖励间的延迟有时可以有上千帧那么多，和监督学习中输入和目标值间的关联比起来，这样的延迟显得更加艰巨了。\n",
    "\n",
    "其次，大多数深度学习算法都假设了数据样本间是互相独立的，但在强化学习中高度相关的状态序列是非常常见的。\n",
    "\n",
    "此外，在强化学习中，算法在学习新的行为时数据的分布也会随之改变，这对于假设数据分布是静态的深度学习方法来说是有问题的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movitation \n",
    "### 主要动机\n",
    "\n",
    "Tesauro's TD-Gammon architecture provides a starting point for DQN. TD-Gammon architecture updates the parameters of a network that estimates the value function, directly from on-policy samples of experience, $s_t, a_t, r_t, s_{t+1}, a_{t+1}$, drawn from the algorithm's interactions with the environment (or by self-play, in the case of backgammon). Since this approach was able to outperform the best human backgammon player 20 years ago, it is natural to wonder whether two decades of hardware improvements, coupled with modern deep neural network architectures and scalable reinforcement learning algorithms might produce significant progress.\n",
    "\n",
    "Tesauro的TD-Gammon框架给DQN提供了一个起点。在TD-Gammon框架中，算法首先通过与环境交互（在西洋双陆棋中自我博弈）来生成经历，接下来算法直接从依附策略的样本经历中取样，$s_t, a_t, r_t, s_{t+1}, a_{t+1}$，并用这些样本来更新价值函数估值网络中的参数。因为这种方法能在20年前击败最强的人类西洋双陆棋选手，所以把这20年在硬件上的进步、深度神经网络结构和可扩展的强化学习算法结合起来并期待获得显著的进展就是一件很自然的事。\n",
    "\n",
    "In contrast to TD-Gammon and similar online approaches, authors utilize a technique known as *experience replay* where they store the agent's experiences at each time-step, $e_t = (s_t, a_t, r_t, s_{t+1})$ in a data-set $\\mathcal{D}=e_1,\\ldots , e_N$, pooled over many episodes into a *replay memory*. During the inner loop of the algorithm, authors apply Q-learning updates, or minibatch updates, to samples of experience, $e \\sim \\mathcal{D}$, drawn at random from the pool of stored samples. After performing experience replay, the agent selects and executes and action according to an $\\epsilon$-greedy policy. Since using histories of arbitrary length as inputs to a neural network can be difficult, authors' Q-function instead works on fixed length representation of histories produced by a function $\\phi$. The full algorithm, which authors call *deep Q-learning*, is presented in Algorithm 1.\n",
    "\n",
    "和TD-Gammon形成对比的是，作者使用了和在线方法类似的叫作“经历重放”的技巧，“经历重放”就是把智能体在多个回合中每一帧的经历$e_t = (s_t, a_t, r_t, s_{t+1})$储存在一个数据集$\\mathcal{D}=e_1,\\ldots , e_N$中，这个数据集就被叫作“回放内存”。在算法的内循环中，作者对从“回放内存”中随机选取的经历样本$e \\sim \\mathcal{D}$进行了Q-learning更新，或者叫作小批量更新。在经历回放后，智能体会根据$\\epsilon$-贪心策略来选取动作并执行。因为用随机长度的经历序列来训练神经网络难度比较大，作者用一个$\\phi$函数来生成固定长度经历序列。这整个算法叫作deep Q-learning，具体的算法如Algorithm 1所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/deep_q-learning_with_experience_replay_algorithm.png\" style=\"width: 700px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages of DQN over standard online Q-learning\n",
    "### DQN与标准在线Q-learning相比的优势\n",
    "\n",
    "First, each step of experience is potentially used in many weight updates, which allows for greater data efficiency.\n",
    "\n",
    "首先，每一步经历都可以被多次在权重更新时使用，这使得数据效率得以提高。\n",
    "\n",
    "Second, learning directly from consecutive samples is inefficient, due to the strong correlations between the samples; randomizing the samples breaks these correlations and therefore reduces the variance of the updates.\n",
    "\n",
    "其次，因为连续样本间存在很强的关联性，所以直接从连续样本中学习的效率并不高。通过将样本打乱可以将样本间的关联性削弱，也借此降低了更新的方差。\n",
    "\n",
    "Third, when learning on-policy the current parameters determine the next data sample that the parameters are trained on. It is easy to see how unwanted feedback loops may arise and the parameters could get stuck in a poor local minimum, or even diverge catastrophically (Tsitsiklis and Roy, 1997). By using experience replay the behaviour distribution is averaged over many of its previous states, smoothing out learning and avoiding oscillations or divergence in the parameters.\n",
    "\n",
    "第三，在进行依附策略的学习时，当前的参数会决定下一个用来训练参数的数据样本。显而易见，这样的话可能就会有我们不希望看到的反馈循环出现，使我们困在较差的局部最小值，或者会导致偏离得很离谱。通过使用经历回放，行为分布是用许多之前的状态求平均值得到的，这样就使得学习变得更加平滑，并且避免了参数的震荡和发散。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation metrics\n",
    "### 评估指标\n",
    "\n",
    "1. Average total reward\n",
    "\n",
    "<img src=\"files/figures/DQN_averaged_reward.png\" style=\"width: 600px;\" />\n",
    "\n",
    "2. Policy's estimated action-value function Q\n",
    "\n",
    "<img src=\"files/figures/DQN_averaged_Q.png\" style=\"width: 600px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 之前的研究 (Previous Work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value function approximation ($V(s)$, not $Q(s,a)$) using neural networks (namely multi-layer perceptron, or MLP):\n",
    "\n",
    "- Tesauro, G., 1995. Temporal difference learning and TD-Gammon. Communications of the ACM, 38(3), pp.58-68.\n",
    "- Pollack, J.B. and Blair, A.D., 1997. Why did TD-gammon work?. In Advances in Neural Information Processing Systems (pp. 10-16).\n",
    "- Tsitsiklis, J.N. and Van Roy, B., 1996. An analysis of temporal-difference learning with function approximationTechnical. Report LIDS-P-2322). Laboratory for Information and Decision Systems, Massachusetts Institute of Technology.\n",
    "- Baird, L., 1995. Residual algorithms: Reinforcement learning with function approximation. In Machine Learning Proceedings 1995 (pp. 30-37).\n",
    "\n",
    "#### Estimating value function or policy with restricted Boltzmann machines (RBM)\n",
    "\n",
    "- Sallans, B. and Hinton, G.E., 2004. Reinforcement learning with factored states and actions. Journal of Machine Learning Research, 5(Aug), pp.1063-1088.\n",
    "- Heess, N., Silver, D. and Teh, Y.W., 2012, December. Actor-Critic Reinforcement Learning with Energy-Based Policies. In EWRL (pp. 43-58).\n",
    "\n",
    "#### Linear control by Q-learning with gradient temporal-difference methods\n",
    "\n",
    "- Bhatnagar, S., Precup, D., Silver, D., Sutton, R.S., Maei, H.R. and Szepesvári, C., 2009. Convergent temporal-difference learning with arbitrary smooth function approximation. In Advances in Neural Information Processing Systems (pp. 1204-1212).\n",
    "- Maei, H.R., Szepesvári, C., Bhatnagar, S. and Sutton, R.S., 2010, June. Toward off-policy learning control with function approximation. In ICML (pp. 719-726).\n",
    "\n",
    "#### DQN with batch update (earlier than this paper!)\n",
    "\n",
    "- Riedmiller, M., 2005, October. Neural fitted Q iteration–first experiences with a data efficient neural reinforcement learning method. In European Conference on Machine Learning (pp. 317-328). Springer, Berlin, Heidelberg.\n",
    "\n",
    "#### Learn low dimensional representation by deep autoencoders\n",
    "\n",
    "- Lange, S. and Riedmiller, M., 2010, July. Deep auto-encoder neural networks in reinforcement learning. In Neural Networks (IJCNN), The 2010 International Joint Conference on (pp. 1-8). IEEE.\n",
    "\n",
    "#### Q-learning + experience replay\n",
    "\n",
    "- Lin, L.J., 1993. Reinforcement learning for robots using neural networks (No. CMU-CS-93-103). Carnegie-Mellon Univ Pittsburgh PA School of Computer Science.\n",
    "\n",
    "#### Playing Atari with reinforcement learning\n",
    "\n",
    "- Bellemare, M.G., Naddaf, Y., Veness, J. and Bowling, M., 2013. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47, pp.253-279.\n",
    "- Bellemare, M., Veness, J. and Bowling, M., 2012. Sketch-based linear value function approximation. In Advances in Neural Information Processing Systems (pp. 2213-2221).\n",
    "\n",
    "#### Playing Atari with evolutionary architecture\n",
    "\n",
    "- Hausknecht, M., Lehman, J., Miikkulainen, R. and Stone, P., 2014. A neuroevolution approach to general atari game playing. IEEE Transactions on Computational Intelligence and AI in Games, 6(4), pp.355-366."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 假设 (Assumptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors consider tasks in which an agent interacts with an environment $\\mathcal{E}$, in this case the Atari emulator, in a sequence of actions, observations and rewards. At each time-step the agent selects an action $a_t$ from the set of legal game action, $\\mathcal{A}={1,\\ldots,K}$. The action is passed to the emulator and modifies its internal state and the game score. In general $\\mathcal{E}$ may be **stochastic**. The emulator's internal state is not observed by the agent; instead it observes an image $x_t \\in \\mathbb{R}^d$ from the emulator, which is a vector of raw pixel values representing the current screen. In addition it receives a reward $r_t$ representing the change in game score. Note that in general the game score may depend on the whole prior sequence of actions and observations; feedback about an action may only be received after many thousands of time-steps have elapsed.\n",
    "\n",
    "Since the agent only observes images of the current screen, the task is **partially observed** and many emulator states are perceptually aliased, i.e. it is impossible to fully understand the current situation from only the current screen $x_t$. Authors therefore consider sequences of actions and observations, $s_t=x_1, a_1, x_2,\\ldots , a_{t-1}, x_t$, and learn game strategies that depend upon these sequences. All sequences in the emulator are assumed to terminate in a finite number of time-steps. This formalism gives rise to a large but finite Markov decision process (MDP) in which **each sequence is a distinct state**. As a result, we can apply standard reinforcement learning methods for MDPs, simply by using the complete sequence $s_t$ as the state representation at time $t$.\n",
    "\n",
    "The goal of the agent is to interact with the emulator by selecting actions in a way that maximises future rewards. Authors make the standard assumption that future rewards are discounted by a factor of $\\gamma$ per time-step, and define the future discounted *return* at time $t$ as $R_t = \\sum_{t'=t}^T \\gamma^{t'-t}r_{t'}$, where $T$ is the time-step at which the game terminates. We define the optimal action-value function $Q^{\\ast}(s,a)$ as the maximum expected return achievable by following any strategy, after seeing some sequence $s$ and then taking some action $a$, $Q^{\\ast}(s,a)=\\max_{\\pi} \\mathbb{E}[R_t \\mid s_t = s, a_t = a, \\pi]$, where $\\pi$ is a policy mapping sequences to actions (or distribution over actions).\n",
    "\n",
    "The optimal action-value function obeys an important identity known as the *Bellman equation*. This is based on the following intuition: if the optimal value $Q^{\\ast}(s' , a')$ of the sequence $s'$ at the next time-step was known for all possible action $a'$, then the optimal strategy is to select the action $a'$ maximising the expected value of $r+\\gamma Q^{\\ast}(s', a')$,\n",
    "\n",
    "$$Q^{\\ast}(s,a)=\\mathbb{E}_{s'\\sim \\mathcal{E}}\\left[ r+\\gamma \\max_{a'} Q^{\\ast}(s',a') \\mid s, a \\right]$$\n",
    "\n",
    "The basic idea behind many reinforcement learning algorithms is to estimate the action-value function, by using the Bellman equation as an iterative update, $Q_{i+1}(s,a)=\\mathbb{E}[r+\\gamma \\max_{a'}Q_i (s', a') \\mid s, a]$. Such *value iteration* algorithms converge to the optimal action-value function, $Q_i \\rightarrow Q^{\\ast}$ as $i \\rightarrow \\infty$. **In practice, this basic approach is totally impractical, because the action-value function is estimated separately for each sequence, without any generalisation.** Instead, it is common to use a function approximator to estimate the action-value function, $Q(s,a;\\theta)\\approx Q^{\\ast}(s,a)$. In the reinforcement learning community this is typically a linear function approximator, but sometimes a non-linear function approximator is used instead, such as a neural network. Authors refer to a neural network function approximator with weights $\\theta$ as a Q-network. A Q-network can be trained by minimising a sequence of loss functions $L_i (\\theta_i)$ that changes at each iteration $i$,\n",
    "\n",
    "$$L_i (\\theta_i) = \\mathbb{E}_{s,a \\sim \\rho (\\cdot)}\\left[ (y_i - Q(s,a;\\theta_i))^2 \\right]$$\n",
    "\n",
    "where $y_i = \\mathbb{E}_{s'\\sim \\mathcal{E}}[r+\\gamma \\max_{a'} Q(s',a';\\theta_{i-1})\\mid s, a]$ is the target for iteration $i$ and $\\rho (s,a)$ is a probability distribution over sequences $s$ and actions $a$ that we refer to as the *behaviour distribution*. The parameters from the previous iteration $\\theta_{i-1}$ are held fixed when optimising the loss function $L_i (\\theta_i)$. Note that the targets depend on the network weights; this is in contrast with the targets used for supervised learning, which are fixed before learning begins. Differentiating the loss function with respect to the weights we arrive at the following gradients,\n",
    "\n",
    "$$\\nabla_{\\theta_i} L_i (\\theta_i) = \\mathbb{E}_{s,a\\sim \\rho (\\cdot);s'\\sim \\mathcal{E}}\\left[ \\left( r+\\gamma \\max_{a'} Q(s', a';\\theta_{i-1})-Q(s,a;\\theta_i) \\right) \\nabla_{\\theta_i} Q(s,a;\\theta_i) \\right]$$\n",
    "\n",
    "Rather than computing the full expectations in the above gradient, it is often computationally expedient to optimise the loss function by stochastic gradient descent. If the weights are updated after every time-step, and the expectations are replaced by single samples from the behaviour distribution $\\rho$ and the emulator $\\mathcal{E}$ respectively, then we arrive at the familiar *Q-learning* algorithm.\n",
    "\n",
    "Note that this algorithm is *model-free*: it solves the reinforcement learning task directly using samples from the emulator $\\mathcal{E}$, without explicitly constructing an estimate of $\\mathcal{E}$. It is also *off-policy*: it learns about the greedy strategy $a=\\max_{a} Q(s,a;\\theta)$, while following a behaviour distribution that ensures adequate exploration of the state space. In practice, the behaviour distribution is often selected by an $\\epsilon$-greedy strategy that follows the greedy strategy with probability $1-\\epsilon$ and selects a random action with probability $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 局限性 (Limitations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The authors' algorithm only stores the last $N$ experience tuples in the replay memory, and samples uniformly at random from $\\mathcal{D}$ when performing updates. This approach is in some respects limited since the memory buffer does not differentiate important transitions and always overwrites with recent transitions due to the finite memory size $N$. Similarly, the uniform sampling gives equal importance to all transitions in the replay memory. A more sophisticated sampling strategy might emphasize transitions from which authors can learn the most, similar to prioritized sweeping (Moore and Atkeson, 1993).\n",
    "\n",
    "- DQN is computationally hungry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初次编辑日期 (Initial Edit Date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年5月14日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献 (References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D. and Riedmiller, M., 2013. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 无模型强化学习（Model-Free Reinforcement Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用动态规划来进行规划\n",
    "    - 解一个已知的MDP\n",
    "\n",
    "- 无模型预测（model-free prediction）\n",
    "    - 估计一个未知MDP的价值函数\n",
    "\n",
    "- 无模型控制（model-free control）\n",
    "    - 优化一个未知MDP的价值函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蒙特卡洛强化学习（Monte-Carlo Reinforcement Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MC方法直接从经验集（episodes of experience）上学习\n",
    "\n",
    "- MC是无模型的（model-free）：没有MDP状态转移和奖励的知识\n",
    "\n",
    "- MC从完整的回合（complete episodes）中学习：没有用到自举（bootstrapping）\n",
    "\n",
    "- MC用了最简单的思想：价值等于平均回报（value=mean return）\n",
    "\n",
    "- 警告：MC只能用在回合制的MDP上（episodic MDPs）\n",
    "    - 所有的回合必须终止"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蒙特卡洛策略评估（Monte-Carlo Policy Evaluation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 目标：从使用策略$\\pi$的回合经验中学习$v_{\\pi}$\n",
    "\n",
    "\\begin{equation}\n",
    "S_1, A_1, R_2, \\ldots, S_k \\sim \\pi\n",
    "\\end{equation}\n",
    "\n",
    "- 回报（return）是总折扣奖励（total discounted reward）：\n",
    "\n",
    "\\begin{equation}\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\ldots + \\gamma^{T-1} R_T\n",
    "\\end{equation}\n",
    "\n",
    "- 价值函数（value function）就是期望回报（expected return）：\n",
    "\n",
    "\\begin{equation}\n",
    "v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t | S_t = s]\n",
    "\\end{equation}\n",
    "\n",
    "- 蒙特卡洛策略评估使用了经验平均回报（empirical mean return）而不是期望回报（expected return）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 首访蒙特卡洛策略评估（First-Visit Monte-Carlo Policy Evaluation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这是用来评估状态$s$\n",
    "\n",
    "- 状态$s$在一个回合中被访问的首个时间步长（first time-step）$t$\n",
    "\n",
    "- 增量计数器（increment counter） $N(s) \\leftarrow N(s)+1$\n",
    "\n",
    "- 增量总回报（increment total return） $S(s) \\leftarrow S(s) + G_t$\n",
    "\n",
    "- 用平均回报来估计价值 $V(s) = S(s)/N(s)$\n",
    "\n",
    "- 根据大数定理（law of large numbers），在$N(s) \\rightarrow \\infty$时$V(s) \\rightarrow v_{\\pi}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 每访蒙特卡洛策略评估（Every-Visit Monte-Carlo Policy Evaluation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这是用来评估状态$s$\n",
    "\n",
    "- 状态$s$在一个回合中每个被访问的步长（every time-step）$t$\n",
    "\n",
    "- 增量计数器（increment counter） $N(s) \\leftarrow N(s)+1$\n",
    "\n",
    "- 增量总回报（increment total return） $S(s) \\leftarrow S(s) + G_t$\n",
    "\n",
    "- 用平均回报来估计价值 $V(s) = S(s)/N(s)$\n",
    "\n",
    "- 同样的，在$N(s) \\rightarrow \\infty$时$V(s) \\rightarrow v_{\\pi}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二十一点范例（Blackjack Example）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 状态（200个）\n",
    "    - 当前总和（12-21）\n",
    "    - 庄家的明牌（A-10）\n",
    "    - 我有没有一张“可用的”A（yes-no）\n",
    "- 停牌（stick）：停止要牌（结束）\n",
    "- 要牌（twist）：再加一张牌（不换牌）\n",
    "- 停牌的奖励：\n",
    "    - 如果玩家的牌面点数之和$>$庄家的牌面点数之和，奖励为+1\n",
    "    - 如果玩家的牌面点数之和$=$庄家的牌面点数之和，奖励为0\n",
    "    - 如果玩家的牌面点数之和$<$庄家的牌面点数之和，奖励为-1\n",
    "- 要牌的奖励：\n",
    "    - 如果玩家的牌面点数之和$>$21（结束），奖励为-1\n",
    "    - 其它情况下，奖励为0\n",
    "- 状态转移：如果牌面点数之和$<$12，自动要牌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21点在蒙特卡洛学习后的价值函数（Blackjack Value Function after Monte-Carlo Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "策略：如果牌面点数之和$\\geq$20的话选择停牌，否则选择要牌\n",
    "\n",
    "<img src=\"files/figures/blackjack.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 平均增量（Incremental Mean）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "序列$x_1, x_2, \\ldots$的平均值$\\mu_1, \\mu_2, \\ldots$可以被逐步计算，\n",
    "\n",
    "\\begin{align}\n",
    "\\mu_k & = \\frac{1}{k}\\sum_{j=1}^k x_j \\\\\n",
    "& = \\frac{1}{k}(x_k + \\sum_{j=1}^{k-1} x_j) \\\\\n",
    "& = \\frac{1}{k}(x_k + (k-1)\\mu_{k-1}) \\\\\n",
    "& = \\mu_{k-1} + \\frac{1}{k}(x_k - \\mu_{k-1})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初次编辑时间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年4月20日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MC-TD.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

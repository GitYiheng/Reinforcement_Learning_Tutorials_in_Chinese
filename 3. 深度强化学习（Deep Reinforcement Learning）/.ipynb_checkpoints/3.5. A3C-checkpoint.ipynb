{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic suceeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep neural networks provide rich representations that can enable reinforcement learning (RL) algorithms to perform effectively. However, it was previously thought that the combination of simple online RL algorithms with deep neural networks was fundamentally unstable. Instead, a variety of solutions have been proposed to stabilize the algorithm. These approaches share a common idea: the sequence of observed data encountered by an online RL agent is non-stationary, and online RL updates are strongly correlated. By storing the agent's data in an experience replay memory, the data can be batched or randomly sampled from different time-steps. Aggregating over memory in this way reduces non-stationary and decorrelates updates, but at the same time limits the methods to off-policy reinforcement learning algorithms.\n",
    "\n",
    "Deep RL algorithms based on experience replay have achieved unprecedented success in challenging domain such as Atari 2600. However, experience replay has several drawbacks: it uses more memory and computation per real interaction; and it requires off-policy learning algorithms that can update from data generated by an older policy.\n",
    "\n",
    "In this paper we provide a very different paradigm for deep reinforcement learning. Instead of experience replay, we asynchronously execute multiple agents in parallel, on multiple instances of the environment. This parallelism also decorrelates the agent's data into a more stationary process, since at any given time-step the parallel agents will be experiencing a variety of different states. This simple idea enables a much larger spectrum of fundamental on-policy RL algorithms, such as Sarsa, n-step methods, and actor-critic methods, as well as off-policy RL algorithms such as Q-learning, to be applied robustly and effectively using deep neural networks.\n",
    "\n",
    "Our parallel reinforcement learning paradigm also offers practical benefits. Whereas previous approaches to deep reinforcement learning rely heavily on specialized hardware such as GPUs or massively distributed architectures, our experiments run on a single machine with a standard multi-core CPU. When applied to a variety of Atari 2600 domains, on many games asynchronous reinforcement learning achieves better results, in far less time than previous GPU-based algorithms, using far less resource than massively distributed approaches. The best of the proposed methods, asynchronous advantage actor-critic (A3C), also mastered a variety of continuous motor control tasks as well as learned general strategies for exploring 3D mazes purely from visual inputs. We believe that the success of A3C on both 2D and 3D games, discrete and continuous action spaces, as well as its ability to train feedforward and recurrent agents makes it the most general and successful reinforcement learning agent to date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The General Reinforcement Learning Architecture (Gorila) of (Nair et al., 2015) performs asynchronous training of reinforcement learning agents in a distributed setting. In Gorila, each process contains an actor that acts in its own copy of environment, a separate replay memory, and a learner that samples data from the replay memory and computes gradients of the DQN loss (Mnih et al., 2015) with respect to the policy parameters. The gradients are asynchronously sent to a central parameter server which updates a central copy of the model. The updated policy parameters are sent to the actor-learner processes and 30 parameter server instances, a total of 130 machines, Gorila was able to significantly outperform DQN over 49 Atari games. On many games Gorila reached the score achieved by DQN over 20 times faster than DQN. We also note that a similar way of parallelizing DQN was proposed by (Chavez et al., 2015).\n",
    "\n",
    "In earlier work, (Li & Schuurmans, 2011) applied the Map Reduce framework to parallelizing batch reinforcement learning methods with linear function approximation. Parallelism was used to speed up large matrix operations but not to parallelize the collection of experience or stabilize learning. (Grounds & Kudenko, 2008) proposed a parallel version of the Sarsa algorithm that uses multiple separate actor-learners to accelerate training. Each actor-learner learns separately and periodically sends updates to weights that have changed significantly to the other learner using peer-to-peer communication.\n",
    "\n",
    "(Tsitsiklis, 1994) studied convergence properties of Q-learning in the asynchronous optimization setting. These results show that Q-learning is still guaranteed to converge when some of the information is outdated as long as outdated information is always eventually discarded and several other technical assumptions are satisfied. Even earlier, (Bertsekas, 1982) studied the related problem of distributed dynamic programming.\n",
    "\n",
    "Another related area of work is in evolutionary methods, which are often straightforward to parallelize by distributing fitness evaluations over multiple machines or threads (Tomassini, 1999). Such parallel evolutionary approaches have recently been applied to some visual reinforcement learning tasks. In one example, (Koutnik et al., 2014) evolved convolutional neural network controllers for the TORCS driving simulator by performing fitness evaluations on 8 CPU cores in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Reinforcement Learning Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the standard reinforcement learning setting where an agent interacts with an environment $\\mathcal{E}$ over a number of discrete time steps. At each time step $t$, the agent receives a state $s_t$ and selects an action $a_t$ from some set of possible actions $\\mathcal{A}$ according to its policy $\\pi$, where $\\pi$ is a mapping from state $s_t$ to actions $a_t$. In return, the agent receives the next state $s_{t+1}$ and receives a scalar reward $r_t$. The process continues until the agent reaches a terminal state after which the process restarts. The return $R_t = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+k}$ is the total accumulated return from time step $t$ with discount factor $\\gamma \\in (0, 1]$. The goal of the agent is to maximize the expected return from each state $s_t$.\n",
    "\n",
    "The action value $Q^{\\pi}(s,a)=\\mathbb{E}[R_t\\mid s_t = s,a]$ is the expected return for selecting action $a$ in state $s$ and following policy $\\pi$. The optimal value function $Q^{\\ast}(s,a)=\\max_{\\pi}Q^{\\ast}(s,a)$ gives the maximum action value for state $s$ and action $a$ achievable by any policy. Similarly, the value of state $s$ under policy $\\pi$ is defined as $V^{\\pi}(s)=\\mathbb{E}[R_t\\mid s_t = s]$ and is simply the expected return for following policy $\\pi$ from state $s$.\n",
    "\n",
    "In value-based model-free reinforcement learning methods, the action value function is represented using a function approximator, such as a neural network. Let $Q(s,a;\\theta)$ be an approximate action-value function with parameters $\\theta$. The updates to $\\theta$ can be derived from a variety of reinforcement learning algorithms. One example of such an algorithm is Q-learning, which aims to directly approximate the optimal action value function: $Q^{\\ast}(s,a)\\approx Q(s,a;\\theta)$. In one-step Q-learning, the parameters $\\theta$ of the action value function $Q(s,a;\\theta)$ are learned by iteratively minimizing a sequence of loss functions, where the $i$th loss function defined as\n",
    "\n",
    "$L_i (\\theta_i)=\\mathbb{E}\\left( r + \\gamma \\max_{a'} Q(s',a';\\theta_{i-1})-Q(s,a;\\theta_i) \\right)^2$\n",
    "\n",
    "where $s'$ is the state encountered after state $s$.\n",
    "\n",
    "We refer to the above method as one-step Q-learning because it updates the action value $Q(s,a)$ toward the one-step return $r+\\gamma \\max_{a'}Q(s',a';\\theta)$. One drawback of using one-step methods is that obtaining a reward $r$ only directly affects the value of the state action pairs are affected only indirectly through the updated value $Q(s,a)$. This can make the learning process slow since many updates are required the propagate a reward to the relevant preceding states and actions.\n",
    "\n",
    "One way of propagating rewards faster is by using $n$-step returns (Watkins, 1989; Peng & Williams, 1996). In $n$-step Q-learning, $Q(s,a)$ is updated toward the $n$-step return defined as $r_t + \\gamma r_{t+1}+\\ldots + \\gamma^{n-1}r_{t+n-1}+\\max_{a}\\gamma^n Q(s_{t+n},a)$. This results in a single reward $r$ directly affecting the values of $n$ preceding state action pairs. This makes the process of propagating rewards to relevant state-action pairs potentially much more efficient.\n",
    "\n",
    "In contrast to value-based methods, policy-based model-free methods directly parameterize the policy $\\pi (a\\mid s;\\theta)$ and update the parameters $\\theta$ by performing, typically approximate, gradient ascent on $\\mathbb{E}[R_t]$. One example of such a method is the REINFORCE family of algorithms due to Williams (1992). Standard REINFORCE updates the policy parameters $\\theta$ in the direction $\\nabla_\\theta \\log \\pi (a_t \\mid s_t;\\theta) R_t$, which is an unbiased estimate while keeping it unbiased by subtracting a learned function of the state $b_t (s_t)$, known as a baseline (Williams, 1992), from the return. The resulting gradient is $\\nabla_\\theta \\log \\pi (a_t \\mid s_t ;\\theta)(R_t - b_t (s_t))$.\n",
    "\n",
    "A learned estimate of the value function is commonly used as the baseline $b_t (s_t) \\approx V^{\\pi}(s_t)$ leading to a much lower variance estimate of the policy gradient. When an approximate value function is used as the baseline, the quality $R_t - b_t$ used to scale the policy gradient can be seen as an estimate of the *advantage* of action $a_t$ in state $s_t$, or $A(a_t, s_t)=Q(a_t, s_t)-V(s_t)$, because $R_t$ is an estimate of $Q^{\\pi}(a_t, s_t)$ and $b_t$ is an estimate of $V^{\\pi}(s_t)$. This approach can be viewed as an actor-critic architecture where the policy $\\pi$ is the actor and the baseline $b_t$ is the critic (Sutton & Barto, 1998; Degris et al., 2012)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Asynchronous RL Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now present multi-threaded asynchronous variants of one-step Sarsa, one-step Q-learning, n-step Q-learning, and advantage actor-critic. The aim in designing these methods was to find RL algortihms that can train deep neural network policies reliably and without large resource requirements. While the underlying RL methods are quite different, with actor-critic being an on-policy policy search method and Q-learning being an off-policy value-based method, we use two main ideas to make all four algorithms practical given our design goal.\n",
    "\n",
    "First, we use asynchronous actor-learners, similarly to the Gorila framework (Nair et al., 2015), but instead of using separate machines and a parameter server, we use multiple CPU threads on a single machine. Keeping the learners on a single machine removes the communication costs of sending gradients and parameters and enables us to use Hogwild! (Recht et al., 2011) style updates for training.\n",
    "\n",
    "Second, we make the observation that multiple actors-learners running in parallel are likely to be exploring different parts of the environment. Moreover, one can explicitly use different exploration policies in each actor-learner to maximize this diversity. By running different exploration policies in different threads, the overall changes being made to the parameters by multiple actor-learners applying online updates in parallel are likely to be less correlated in time than a single agent applying online updates. Hence, we do not use a replay memory and rely on parallel actors employing different exploration policies to perform the stabilizing role undertaken by experience replay in the DQN training algorithm.\n",
    "\n",
    "In addition to stabilizing learning, using multiple parallel actor-learners has multiple practical benefits. First, we obtain a reduction in training time that is roughly linear in the number of parallel actor-learners. Second, since we no longer rely on experience replay for stabilizing learning we are able to use on-policy reinforcement learning methods such as Sarsa and actor-critic to train neural networks in a stable way. We now describe our variants of one-step Q-learning, one-step Sarsa, n-step Q-learning and advantage actor-critic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/asynchronous_one-step_q-learning.png\" style=\"width: 400px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

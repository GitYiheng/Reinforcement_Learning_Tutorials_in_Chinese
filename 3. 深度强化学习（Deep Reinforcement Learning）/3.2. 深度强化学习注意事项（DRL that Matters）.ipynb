{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度强化学习注意事项 (Deep Reinforcement Learning that Matters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 标签 (Labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvard引用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D. and Meger, D., 2017. Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BibTex引用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@article{henderson2017deep,\n",
    "  title={Deep reinforcement learning that matters},\n",
    "  author={Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},\n",
    "  journal={arXiv preprint arXiv:1709.06560},\n",
    "  year={2017}\n",
    "}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 摘要 (Abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.\n",
    "\n",
    "近年来，深度强化学习在解决不同领域的有挑战性的问题方面取得了显著的进展。为了维持这样的进展，再现现有算法和有效地评估新方法就显得非常重要了。但想要再现深度强化学习新算法中的结果往往异常艰难。有两点因素导致了现有算法难以再现和分析的这种情况，第一点就是标准基准环境中存在的不确定性，此外的第二点就是算法中可能变化的部分。所以，如果实验报告没有有效的度量标准和严格的规范，那么判断对现有算法的改进是否有意义就会变得非常困难。在这篇论文中，我们探讨了再现现有算法所面临的挑战、规范的实验方法和实验报告结构。我们会展示常见基线算法在指标和结果上的变化幅度，并且提出了再现性更好的深度强化学习基线。我们的目标是推动大家在这方面的探讨，减少在无法再现和容易产生歧义的实验结果上做无用功，借此确保深度强化学习的研究能够得到持续的进展。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 研究问题 (Research Problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproducibility and good practice in deep reinforcement learning.\n",
    "\n",
    "深度强化学习算法实验的可再现性（可重复性）和规范操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主要贡献 (Contributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) The first work to address the question of reproducibility and good experimental practice in the context of deep reinforcement learning.\n",
    "\n",
    "- 这是第一篇讨论深度强化学习可再现性（可重复性）和良好的规范操作的论文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初次编辑日期 (Initial Edit Date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018年5月17日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献 (References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D. and Meger, D., 2017. Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
